Meta-Learning Dynamics Forecasting Using Task Inference
RuiWang*1 RobinWalters*2 RoseYu1
Abstract
𝑿𝟏:𝒕 𝑿𝟏:𝒕
Currentdeeplearningmodelsfordynamicsfore-
1202 casting struggle with generalization. They can
onlyforecastinaspecificdomainandfailwhen
appliedtosystemswithdifferentparameters,ex-
Encoder Prediction Encoder Prediction
ternal forces, or boundary conditions. We pro- Network Network
beF poseamodel-basedmeta-learningmethodcalled
DyAdwhichcangeneralizeacrossheterogeneous 𝒛 a ed xa tp et nfo alr fs om rca ell 𝒛 a ed xa tep rt f ao lr fl oa rr cg ee
r n
… …
domainsbypartitioningthemintoseparatesub-
02 domains, each with a different task. DyAd has
twoparts: apredictionnetworkwhichlearnsthe 𝒄=𝟓 𝒄=𝟐𝟓
𝑿𝒕+𝟏 𝑿𝒕+𝟏
shared dynamics of the entire domain, and an
]GL.sc[
encoderwhichinferstheparametersofthetask.
The encoder adapts the prediction network dur-
ing inference time using adaptive instance nor-
Figure1.ComparisonofDyAdappliedtotwoinputsoffluidturbu-
malizationandanewlayer,AdaPad,specifically
lence,onewithsmallexternalforcingandonewithlargerexternal
designedforboundaryconditions. Theencoder
forces. Theencoderinfersthetime-shiftinvariantcharacteristic
canalsouseanyweaksupervisionsignalsthatcan
1v17201.2012:viXra variablezwhichisusedtoadaptthepredictionnetwork.
helpdistinguishdifferenttasks,allowingthein-
corporationofadditionaldomainknowledge. Our
model outperforms a variety of state-of-the-art Meta-learning (Thrun & Pratt, 1998; Baxter, 1998; Finn
approachesonbothturbulentflowandreal-world etal.,2017),orlearningtolearn,improvesgeneralization
oceandataforecastingtasks. bylearningmultipletasksfromtheenvironment. Therecent
developmentinmeta-learninghasbeensuccessfullyapplied
tofew-shotclassification(Munkhdalai&Yu,2017),active
1.Introduction learning (Yoon et al., 2018), and reinforcement learning
(Guptaetal.,2018). However,meta-learninginthecontext
Learningdynamicalsystemswithdeepneuralnetworkshas
offorecastinghigh-dimensionalphysicaldynamicshasnot
showngreatsuccessinawiderangeofsystemsfromfluid
beenstudiedbefore. Thechallengeswithmeta-learningdy-
mechanicstoneuraldynamics(Tompsonetal.,2017;Chen
namicalsystemsareuniqueinthat(1)weneedtoefficiently
etal.,2018;Kolter&Manek,2019;Zoltowskietal.,2020;
infer thelatent parameters ofthe dynamical systemfrom
Lietal.,2021). However,themainlimitationofprevious
observedtimeseriesdata,and(2)weneedtoaccountfor
workmodelingdynamicswithneuralnetworksisverylim-
changesinunknowninitialandboundaryconditions.
itedgeneralizability. Mostapproachestreatdynamicsdata
asatimeseriesandtrainonpastdatainordertopredictfu- Ourapproachisinspiredbythefactthatsimilardynamical
turedata. Thusanewmodelmustbetrainedtopredicteach systemsmaysharetime-invariantcharacteristicvariables.
specificsystem. Itisimperativetodevelopgeneralizable Even the slightest change in these variables may lead to
deeplearningmodelsfordynamicalsystemsthatcanlearn vastlydifferentphenomena.Forexample,inclimatescience,
andgeneralizewelloveralargeheterogeneousdomain. fluidsaregovernedbyasetofdifferentialequationscalled
Navier-Stokesequations. Somevariablessuchaskinematic
*Equalcontribution 1ComputerScienceandEngineering,Uni- viscosity, which measures a fluid’s internal resistance to
versityofCaliforniaSanDiego,USA2KhouryCollegeofCom-
deformation,andexternalforces,suchasgravity,determine
puterScience,NortheasternUniversity,Boston,USA.Correspon-
the flow characteristics. By inferring these variables, we
denceto:RoseYu<roseyu@eng.ucsd.edu>.
canmodeldiversesystembehaviorfromsmoothlyflowing
watertoatmosphericturbulence.Meta-LearningDynamicsForecastingUsingTaskInference
We propose a model-based meta-learning method, called wanttolearnamapf suchthat:
DyAd,whichcanrapidlyadapttodynamicssystemswith
differentparameters. DyAdhastwoparts,anencodergand f :(x t−l+1,...,x t)−→(x t+1,...x t+h) (2)
apredictionnetworkf. Theencodermapsdifferentdynam-
Herelisthelengthoftheinputseriesandhistheforecasting
icalsystemstotime-invariantcharacteristicvariablessuch
horizonintheoutput.
asconstantsofmotion,boundaryconditions,andexternal
forceswhichvaryfromsystemtosystem. Theprediction Existingapproachesfordynamicsforecastingonlypredict
network f then takes the characteristic variables and the future data for a specific system as a single task. The re-
pastsystemstatestoforecastthefuturesystemstate. Due sultingmodeloftengeneralizespoorlytodifferentsystem
totheinformationinthecharacteristicvariable,thepredic- dynamics. Thusanewmodelmustbetrainedtopredictfor
tionnetworkhastheflexibilitytoadapttoawiderangeof systemswithdifferentdynamics.
systemswithheterogeneousdynamics.
To perform meta-learning, we learn multiple forecasting
Unlike gradient-based meta-learning techniques such as taskssimultaneouslywhereeachtaskisidentifiedbysome
MAML (Finn et al., 2017), DyAd automatically adapts parametersc⊂ψ,representingtime-invariantcharacteris-
during inference using an encoder and does not require ticssuchasconstantsofmotion,externalforces,andbound-
anyretraining. Similartoothermodel-basedmeta-learning ary conditions. Here we use c for a subset of parameters
methodssuchasMetaNets(Munkhdalai&Yu,2017),we becauseweusuallydonothavethefullknowledgeofthe
employatwo-partdesignwithanadaptablelearnerwhich systemdynamics. Theparametersconlypartiallydescribe
receives task-specific weights. However, for time series thecharacteristicsinthedata.
forecasting,sinceinputandoutputcomefromthesamedo-
Consider,forexample,thedomainoffluiddynamics. Here,
main,asupportsetofadditionallabeleddataisunnecessary
x isthevelocityfieldofthefluidflowattimet. Different
todefinethetask. Theencodercaninferthetaskdirectly t
typesoffluidflowsmaybeclassifiedandlabeledaccording
fromqueryinput.
to their degree of turbulence by taking c to be Reynolds
Ourcontributionsinclude: number,averagevorticity,averagemagnitude,oravectorof
• Anewmodel-basedmeta-learningmethod(DyAd)for allthree. FluidflowswithdifferentReynoldsnumbersmay
haveverydifferentdynamicsandcharacteristics.
forecastingindynamicalsystemswhichadaptstoeach
inputusingtaskinference. Formally,letµbethedistributionoverX ×Y representing
• An encoder capable of extracting the time-invariant thefunctionf: X →Y whereX =Rd×l andY =Rd×h.
partofadynamicalsystemusingtime-shiftinvariant Our main assumption is that the domain X may be parti-
modelstructureandloss. tionedintoseparatetasksX =∪ X labeledbydifferent
c∈C c
• AnewlayerAdaPaddesignedforboundaryconditions. taskparameterc∈C. NotethatthespaceC maybeeither
• Improvedaccuracyonheterogeneousdomainssuchas discreet or continuous. Denote the task map g: X → C
fluid flow and sea temperature prediction relative to takingx∈X toc. Letµ betheconditionaldistributionof
c c
modelstrainedseparatelyonhomogeneousdomainsor (x,y)∼µsuchthatg(x)=c.
ontheentiredomainbutwithouttaskinference.
During training, the model is presented with data from a
• Goodgeneralizationtonewtaskswithparametersout-
subset of tasks {c } ∼ C. Our goal is to learn the func-
sidethetrainingdistribution. k
tionf: X →Y overthewholedomainX whichcanthus
generalizeacrossalltasksc∈C.
2.Methods
2.1.Meta-learningindynamicsforecasting 2.2.DyAd: DynamicAdaptationNetwork
A dynamical system is governed by a set of differential Weproposeamodel-basedmeta-learningapproachfordy-
equations: namicsforecasting. Ourapproachinferstheforecastingtask
asalatentvariableandusesittoadapttheprediction.
(cid:8) ξi(x,x˙,x¨,...;ψ)=0(cid:9)
(1)
TaskInference. Givenmultipleforecastingtasks,twoap-
proachestomodelingf areeithertolearnalltasksatonce
wherex∈Rdisad-dimensionalstateofthesystemandψ
oronetaskatatime. IfthetrainingsetS = {(x(i),y(i))}
aretheparameters. Oftentimes,thedynamicsparameterψ
hasgoodanduniformcoverageofthedifferentclassesin
canrepresentdifferentsystemcoefficients,externalforces
X,thatis,ifSisasampledfromµi.i.dandislargeenough,
orboundaryconditions.
then a single high capacity neural network may model f
Theproblemofdynamicsforecastingisthatgivenasetof well. However,ifdistributionµishighlyheterogeneousfor
seriesfromthesystemin(1),{(x ,x ,...,x )(i)}n ,we differentcandifthetrainingsetisnoti.i.d.,thenasingle
1 2 t i=1Meta-LearningDynamicsForecastingUsingTaskInference
Encoder
𝑿 allowsgeneralizationtotasksnotinthetrainingset.
𝟏:𝒕
Ourcharacteristiclatentvariablez bearsaffinitywiththe
Conv3D 3x3x3
“style”vectorinstyletransfertechniques. Ratherthanaes-
BatchNorm
LeakyReLU thetic style in images, our latent variable represents the
MaxPool 64x64x128 characteristicsofthedynamicsthataretime-invariant. In-
Conv3D 3x3x3 terestingly,findingcharacteristicnumberssuchasReynolds
BatchNorm number is also a central topic in fluid mechanics. In the
LeakyReLU style-transferliterature,agenerativenetworkisguidedby
MaxPool 32x32x256 theuseofanexternalstylethoughadaptiveinstancenormal-
Conv3D 3x3x3 izationbetweenconvolutionallayers(Karrasetal.,2019;
BatchNorm Huang&Belongie,2017).Weuseadaptiveinstancenormal-
LeakyReLU izationtoincorporatethelatentvariablez intoResNetto
c
MaxPool 16x16x512 createanadaptablepredictionnetworkf =f(·,z ).
c c
Conv3D 3x3x3
PartialDisentanglement.Thetime-invariantcharacteristic
BatchNorm
latentvariablemaybeviewedaspartialdisentanglementof
LeakyReLU 8x8x1024
thesystemstate. Assuggestedby(Locatelloetal.,2019;
Global MeanPool
FC Nie et al., 2020), our disentanglement method is guided
byinductivebiasandweaksupervision. Unlikecomplete
z To forecaster
disentanglement,asine.g.(Massagueetal.,2020),inwhich
FC
the latent system state is factored into time-invariant and
𝒄 time-varyingcomponents(z¯,z˜),ourlatentvariableisonly
z¯. Nonetheless,z¯providesastrongsignaltotheprediction
Figure2.Detail of the DyAd encoder. The conv3D layers are networkwhichisusefulforgeneralization.
shiftequivariantandglobalmeanpoolingisshiftinvariant. The
networkisapproximatelyinvarianttospatialandtemporalshifts. 2.3.Time-shiftInvariantEncoder
modelmaystrugglewithgeneralization. Weassumethatcareparametersofthesystemwhichare
time-invariant,suchasconstantsofmotionorfixedparame-
Weproposetolearnthefunctionf intwostages,thatis,by
tersofthedynamicalsystem. Inordertoenforcethisinduc-
firstinferringthetaskcfromtheinputdataxandthenlearn-
tivebias,weencodetime-invariancebothinthearchitecture
ingdifferentspecializedpredictionfunctionsf : X →Y
c c andtrainingobjectiveoftheencoder.
foreachtaskc∈C. Weintroducealatentvariablez asa
c
high-dimensionalrepresentationforthetaskcandinferthis Theencoderisimplementedusing4layersof3Dconvolu-
latentvariablefromtheinputdatax. Assystemdynamics tionasseeninFigure2. Weconvolveacrosstwospatialand
areencodedinx,wecanusethesamesingleinputxtoinfer onetemporaldimension. Afterthisthereisaglobalmean-
thetaskcandthenpredicttheoutputlabelyˆ=f (x). poolinglayerandfullyconnectedlayerwhichoutputsthe
c
Prediction Network Block
Model-BasedAdaptation. Caremustbetakenwithhow
the functions f are implemented. A naive approach in
c
whicheachf chasitsownweightsandistrainedseparately z 𝒉𝒊
would be highly impractical if the number of tasks |C| is
large. Eachf wouldhaveverylittletrainingdataandthe
c
FC AdaPad
combinedmodelwouldhavealargenumberofweights. In
Conv2D 3x3
essence,treatingeachccompletelyseparatelyfailstotake
LeakyReLU
advantageofthesimilaritybetweentasksf .
c
AdaPad
Ourproposalstrikesabalancebetweenthesetwoextremes. Conv2D 3x3
AsshowninFigure1,ourmodelconsistsoftwoparts: an LeakyReLU
encoder g and prediction network f. The encoder maps
+
the input x to a high-dimensional latent variable z that
c
characterizes the dynamics of the system, hence the task FC AdaIN
c. Wethenusez toadaptthepredictionnetworkf tothe
c
specificdynamicsofthedomainc, i.e., modely = f (x) 𝒉𝒊+𝟏
c
asy = f(x,z ). Inthisway,weefficientlytraindifferent
c
specific mappings f for each c. Moreover, the encoder Figure3. Detailofoneblockofthepredictionnetwork.
cMeta-LearningDynamicsForecastingUsingTaskInference
estimateofthelatentvariablezˆ . Onelastfullyconnected AdaPadlayer,whichisspecializedtoencodingthebound-
c
layercomputestheadditionaloutputcˆ=Wzˆ . aryconditionsofeachspecificdynamicalsystem. Generally
c
when predicting dynamical systems, error is introduced
Since convolutions are equivariant to shift (up to bound-
alongtheboundariessinceitisunknownhowthedynam-
ary frames) and mean pooling is invariant to shift, the
ics interact with the boundary of the domain, and there
encoder is shift-invariant. That is, enc(x ,...,x ,0) =
1 t maybeunknowninflowsoroutflows. Inourmethod,the
enc(0,x ,...,x ). Inpractice,shiftingthetimesequence
1 t boundaryconditionsmaybeinferredbytheencoderinz
forwardoneframewilladdonenewframeatthebeginning
and introduced during prediction by AdaPad as padding
anddroponeframeattheend. Thiscreatessomechangein
immediatelyoutsidethespatialdomainineachlayer.
outputvalueoftheencoder. Thus,practically,theencoder
isonlyapproximatelyshift-invariant. Denotethespatialindexofinputxbyx for1≤i≤W
i,j
and1≤j ≤H. LettingAandbbetrainableweights,the
Following 3D convolutions, we apply BatchNorm,
paddingiscomputed
LeakyReLU, and max-pooling. Max-pooling further re-
ducesthetheoreticalshift-invarianceofthenetworksince
Az+b=(x ,...,x ,x ,...,x ,
0,0 0,H+1 W+1,0 W+1,H+1
2x2x2max-poolingisperfectlyequivarianttoshiftsofsize
x ,...,x ,x ,...,x ).
2andonlyapproximatelyinvarianttoshiftsofsize1. 0,1 W,0 W+1,1 W,H+1
Then
2.4.PredictionNetwork
y =AdaPad(x,z)=(x ) .
AsshowninFigure4,ourpredictionnetworkissimilarto i,j 0≤i≤W+1,0≤j≤H+1
ResNetbutincludeslayerswhichincorporatethecharac-
Thusy istheinputxpaddedwithadditionalvaluescom-
teristiclatentvariable. Weusetwospecializedlayers,adap- putedfromthelatentvariablez.
tive instance normalization AdaIN and adaptive padding
AdaPadtospecializethepredictionnetworktospecificdy-
2.5.DyAdTraining
namical systems. AdaIN has been used in style transfer
networkstoguidegenerativenetworks. Here,AdaINmay We use a two-stage approach for training. The encoder
adaptforspecificcoefficientsandexternalforces. Wealso network g φ is trained first separately from the prediction
introduceanewlayerAdaPad(x,z)whichiswell-suited network. To combat the loss of shift invariance from the
forencodingtheboundaryconditionsofdynamicalsystems. change from the boundary frames, we train the encoder
usingatime-shift-invarianceloss. Giventwotrainingsam-
AdaIN.WeemployadaptiveinstancenormalizationAdaIN,
ples (x(i),y(i),c(i)) and (x(j),y(j),c(j)) where x(i) and
which has proven effective in the style transfer literature
x(j) come from the same task and thus c(i) = c(j), we
as way to use a style vector to guide a convolutional net-
haveloss
work (Dumoulin et al., 2016; Ghiasi et al., 2017; Huang
& Belongie, 2017; Dumoulin et al., 2018; Karras et al., L =L (cˆ(i),c(i))+αL (zˆ(i),zˆ(j))+β|(cid:107)zˆ(i)(cid:107)−m| (3)
enc 1 2
2019). Denotethechannelsofinputxbyx andletµ(x )
i i
andσ(x )bethemeanandstandarddeviationofchannel whereestimateszˆ(i) = g (x(i))andzˆ(j) = g (x(j))and
i φ φ
i. For each AdaIN layer, a particular style is computed cˆ(i) =Wzˆ(i)+bisanaffinetransformationofz.
s = (µ ,σ ) = Az+b,wherethelinearmapAandbias AdaPad
i i i
barelearnedweights. Adaptiveinstancenormalizationis
𝒙 z
thendefined
x −µ(x )
y =σ i i +µ .
i i σ(x i) i FC
Inessence,thechannelsarerenormalizedtothestyles.
Fordynamicsprediction,thecharacteristiclatentvariable
z encodes data analogous to the various coefficients of a
differentialequationandexternalforcesonthesystem. In
numericalsimulationofadifferentialequationthesecoeffi-
cientsenterasscalingsofdifferenttermsintheequationand
theexternalforcesareaddedtothecombinedforceequation.
ThusinourcontextAdaIN,whichscaleschannelsandadds
aglobalvector,iswell-suitedtoinjectingthisinformation. 𝒚
AdaPad. To compliment AdaIN, we introduce the
Figure4. IllustrationoftheAdaPadoperation.Meta-LearningDynamicsForecastingUsingTaskInference
ThefirsttermofthelossL (cˆ,c(i))allowsweaksupervision The following inequality compares the performance for
1
ofthelatentvariablez withinputc(i). Whilenotalltime- multi-task learning to learning the individual tasks. Let
invariantcharacteristicsofthedynamicalsystemareknown, R (F)betheRademachercomplexityforF overµ .
k k
domainknowledgeofcharacteristicparameterswhichiden-
Lemma 3.2. The Rademacher complexity for multi-task
tify the dynamics may be incorporated in the datum c(i). learningisboundedR(F)≤(1/K)(cid:80)K
R (F).
k=1 k
Forexample,theReynoldsnumberofthefluid.
WecannowcomparetheboundfromTheorem3.1withthe
ThesecondtermL (zˆ(i),zˆ(j))isthetime-shiftinvariance
2
boundobtainedbyconsideringeachtaskindividually.
loss,whichpenalizesthechangesinlatentvariablesbetween
samples. Sincethetime-shiftinvarianceofconvolutionis Proposition3.3. Assumethelossisboundedl≤1/2,then
onlyapproximate,thislosstermdrivesthetime-shifterror the generalization bound given by considering each task
evenlower.Thethirdterm|(cid:107)zˆ(i)(cid:107)−m|preventstheencoder individuallyis
fromgeneratingsmallzˆ(i) duetotime-shiftinvarianceloss.
(cid:32) K (cid:33) (cid:114)
ForbothL andL ,weusemeansquarederror. 1 (cid:88) log1/δ
1 2 (cid:15)(f)≤(cid:15)ˆ(f)+2 R (F) + . (4)
K k 2n
Thepredictionnetworkistrainedafterwards. Thekernels k=1
oftheconvolutionsandthefullyconnectedmappingsofthe
whichisstrictlylooserthantheboundfromTheorem3.1.
AdaINandAdaPadlayersarealltrainedsimultaneouslyas
thepredictionnetworkistrained. Thelossfortheprediction Thishelpsexplainwhyourmultitasklearningframework
networkistheforecastinglossforthetruelabely, hasbettergeneralizationthanlearningeachtaskindepen-
dently. Theshareddatatightensthegeneralizationbound.
L =L (yˆ,y).
pred 3
DomainAdaptationError.Sincewetestonc∼Coutside
WeuseMSEpertimestepandgeneratemulti-stepforecast-
thetrainingset{c },weincurerrorduetodomainadapta-
k
inginanautoregressivefashion.
tionfromthesourcedomainsµ ,...,µ totargetdomain
c1 cK
µ withµbeingthetruedistribution.Denotethecorrespond-
c
3.TheoreticalAnalysis ingempiricaldistributionsofnsamplespertaskbyµˆ . For
c
similar domains, the Wasserstein distance W (µ ,µ ) is
1 c c(cid:48)
Thehigh-levelideaofourmethodistolearnagoodrepre-
small. Theboundfrom(Redkoetal.,2017)applieswellto
sentationofthedynamicsthatgeneralizeswellandadapt
oursettingassuch:
thisrepresentationtonewtasks. Ourmodelachievesthis
Theorem3.4((Redkoetal.,2017),Theorem2). Letλ =
bylearningonaheterogeneousdomainwhereitlearnsto
(cid:16) (cid:17)
c(f)+1/K(cid:80)K
inferthetasks. Weprovideanalysisforthisprocedure. See min f∈F (cid:15) k=1(cid:15) ck(f) . There is N =
AppendixBforalongertreatmentwithproofs. N(ζ,dim(X)) such that for n > N, for any hypothesis
f,withprobabilityatleast1−δ,
SupposewehaveK tasks,eachofwhichissampledfroma
continuousspace{c k}K k=1 ∼C. Foreachtaskc k,wehave 1 (cid:88)K (cid:32) 1 (cid:88)K (cid:33)
a collection of series as realizations from the dynamical (cid:15) (f)≤ (cid:15) (f)+W µˆ , µˆ
systemX ={(x ,...,x ;c )(i)}nk wherec represents c K ck 1 c K ck
k t 1 k i=1 k k=1 k=1
thesystembehaviorinaspecificdomainLetX =∪ c∈CX c +(cid:112) 2log(1/δ)(cid:16)(cid:112) 1/n+(cid:112) 1/(nK)(cid:17) +λ.
betheunionofsamplesoveralltasks.
Multi-task Learning Error. Our model resembles the
Encoder versus Prediction Network Error. Error from
multi-task representation learning setting (Maurer et al.,
DyAd may result from either the encoder g or the pre-
2016)withjointrisk(cid:15)=(1/K)(cid:80) (cid:15) themeanofrisks(cid:15) φ
k k k diction network f . Our hypothesis space has the form
θ
ofeachtaskdefinedseparately. Denotethecorresponding
{x (cid:55)→ f (x,g (x))}whereφandθaretheweightsofthe
empirical risks (cid:15)ˆand (cid:15)ˆ . We bound the true risk (cid:15) using θ φ
k encoder and prediction network respectively. Let (cid:15) be
theempiricalrisk(cid:15)ˆandRademachercomplexityR(F)of X
the error over the entire domain X, that is, for all c. Let
thehypothesisclassF. Thefollowingtheoremrestatesthe
(cid:15) (g )=E (L (g(x),g (x))betheencodererror.
mainresultin(Andoetal.,2005)withsimplifiednotations. enc φ x∼X 1 φ
Proposition 3.5. Assume c (cid:55)→ sup (f (·,c)) is Lipschitz
Theorem3.1. (Andoetal.,2005)Assumelossisbounded θ θ
continuouswithLipschitzconstantγ. Thenwebound
l ≤1/2. GivennsampleseachfromK differentforecast-
ing tasks µ 1,··· ,µ k with probability at least 1−δ, the (cid:15) X(f θ(·,g φ(·)))≤γ(cid:15) enc(g φ)+E c∼C[(cid:15) c(f θ(x,c))] (5)
followinginequalityholdsforeachf ∈F:
wherethefirsttermistheerrorduetotheencoderincor-
(cid:114)
1 (cid:88) 1 (cid:88) log1/δ rectlyidentifyingthetaskandthesecondtermistheerror
(cid:15) (f)≤ (cid:15)ˆ (f)+2R(F)+
K k K k 2nK duethepredictionnetworkalone.
k kMeta-LearningDynamicsForecastingUsingTaskInference
4.RelatedWork transfertechniques. Styletransferinitiallyappearinnon-
photorealisticrendering(Kyprianidisetal.,2012). Recently,
LearningDynamicalSystems. Deeplearningmodelsare
neural style transfer (Jing et al., 2019) has been applied
gainingpopularityforlearningdynamicalsystems(Shietal.,
toimagesynthesis(Gatysetal.,2016),videosgeneration
2017;Chenetal.,2018;Kolter&Manek,2019;Azencot
(Ruderetal.,2016),andlanguagetranslation(Prabhumoye
etal.,2020a). Anemergingtopicisphysics-informeddeep
et al., 2018). For dynamical systems, Sato et al. (2018)
learningRaissietal.(2017);MaziarRaissi(2019);Lutter
adaptstexturesynthesistotransferthestyleofturbulence
etal.(2018);Azencotetal.(2020b);Wangetal.(2020a)
foranimation. Kim&Lee(2019)studiesunsupervisedgen-
whichintegratesinductivebiasesfromphysicalsystemsto
erativemodelingofturbulentflowsbutforsuper-resolution
improvelearning. Forexample,(MaziarRaissi,2019)use
reconstructionratherthanforecasting.
deepneuralnetworkstosolvePDEsautomaticallybutre-
quireexplicitinputofboundaryconditionsduringinference. 5.Experiments
(Lutteretal.,2018)encodeEuler-Lagrangeequationinto
We compare our model with a series of baselines on the
thedeepneuralnetsbutfocusonlearninglow-dimensional
multi-stepforecastingwithdifferentdynamics. Weconsider
trajectories. For high-dimensional fluid flow, (Xie et al.,
two testing scenarios: (1) dynamics with different initial
2018)and(Tompsonetal.,2017)developeddeeplearning
conditions(test-future)and(2)dynamicswithdifferentpa-
modelsinthecontextoffluidflowanimation,wherethepre-
rameters such as external force (test-domain). The first
dictionandphysicalconsistencyislesscritical. Thereare
scenarioevaluatesthemodels’abilitytoextrapolateintothe
alsophysics-informeddeeplearningmodels(deBezenac
futureforthesametask. Thesecondscenarioestimatesthe
et al., 2018; Wang et al., 2020b; Ayed et al., 2019a;b; Li
capabilityofthemodelstogeneralizeacrossdifferenttasks.
etal.,2021). Forinstance,Andersonetal.(2019)designed
rotationallycovariantneuralnetworkforlearningmolecular Weexperimentonsyntheticturbulentflows,real-worldsea
systems. Morton et al. (2018); Azencot et al. (2020b) in- surfacetemperatureandoceancurrentsdata. Theyarediffi-
corporatedKoopmantheoryintothearchitecture. However, culttoforecastusingnumericalmethodsduetounknown
theseapproachesoftenfocusonaspecifictypeofsystem externalforcesandcomplexdynamicsnotfullycapturedby
dynamicsinsteadofmeta-learninginthiswork. simplifiedmathematicalmodels. Wedeferthedetailsofthe
datasetsandexperimentstoAppendixA.2.
Meta-learningandDomainAdaptation.Theaimofmeta
learning,orlearningtolearn(Thrun&Pratt,1998),istoac-
5.1.Datasets
quiregenericknowledgeofdifferenttasksinorderforrapid
learningonnewtasks. Basedonhowthemeta-levelknowl- TurbulentFlowwithVaryingBuoyancy. Wegeneratea
edge is extracted and used, meta-learning methods have syntheticdatasetofturbulentflowswithanumericalsimula-
beenclassifiedintomodel-based(Munkhdalai&Yu,2017; tor,PhiFlow1. Itcontains64×64velocityfieldsofturbulent
Duanetal.,2017;Santoroetal.,2016;Aletetal.,2018;Ore- flowsinwhichwevarythebuoyantforceactingonthefluid
shkinetal.,2019;Seoetal.,2020),metric-based(Vinyals from1to25. Eachbuoyantforcecorrespondstoaforecast-
et al., 2016; Snell et al., 2017) and gradient-based (Finn ingtaskandthereare25tasksintotal. Weusethemean
etal.,2017;Andrychowiczetal.,2016;Rusuetal.,2019; vorticityofeachtaskaspartialsupervisioncaswecandi-
Grant et al., 2018; Yaoet al., 2019). Most meta-learning rectlycalculateitfromthedata. Vorticitycancharacterize
approachesareoutsideoftheforecastingdomainwithafew formationandcircularmotionofturbulentflows.
exceptions.Oreshkinetal.(2019)designaresidualarchitec-
Sea Surface Temperature. We evaluate on a real-world
turefortimeseriesforecastingwithameta-learningparallel.
seasurfacetemperaturedatageneratedbytheNEMOocean
Aletetal.(2018)proposeamodularmeta-learningapproach
engine(Madecetal.,2015)2. WeselectanareafromPacific
tocombineneuralnetworkmodulesforcontinuescontrol.
ocean range from 01/01/2018 to 12/31/2020. The corre-
Butforecastingphysicaldynamicsposesuniquechallenges
sponding latitude and longitude are (-174∼-153, 5∼26).
tometa-learningasweseekwaystoencodephysicalknowl-
edge into our model. DyAd can also be considered as a Thisareaisthendividedinto2564×64subregions,eachis
atasksincethemeantemperaturevariesalotalonglongi-
meta-learning model since the encoder is able to encode
tudeandlatitude. Fortheencodertraining,weuseseason
differenttasksandextracthigh-levelmetarepresentations
asanadditionalsupervisionsignalbesidesthemeantemper-
andtheforecasterthenperformsacrossdifferentsystems
atureofeachsubregion. Inotherwords,theencodershould
withoutretraining. Itisnoteworthythatmeta-learningfor
timeseriesforecasting(Lemke&Gabrys,2010;Talagala 1https://github.com/tum-pbs/PhiFlow
etal.,2018)wastermedforforecastmodelselection,which 2Thedataareavailableathttps://resources.marine.
hasadifferentobjectivefromours. copernicus.eu/?option=com_csw&view=details&
product_id=GLOBAL_ANALYSIS_FORECAST_PHY_
Style Transfer. Our approach is inspired by the style 001_024Meta-LearningDynamicsForecastingUsingTaskInference
Table1.PredictionRMSEontheturbulentflowandseasurfacetemperaturedatasets.PredictionRMSEandESE(energyspectrumerrors)
onthetest-futureandtest-domaintestsetsofoceancurrentsdataset.
TurbulentFlows SeaTemperature OceanCurrents
Model
test-future test-domain test-future test-domain test-future test-domain
ResNet 0.936±0.098 0.652±0.019 1.522±0.099 1.504±0.078 1.172±0.091|0.942±0.139 1.148±0.036|1.136±0.117
U-Net 0.921±0.020 0.675±0.021 1.106±0.142 1.137±0.153 1.116±0.103|0.756±0.056 1.154±0.118|0.876±0.083
Mod-ind 1.126±0.050 ———- 2.486±0.167 ———- 1.305±0.156|1.160±0.096 ———-|———-
Mod-attn 0.626±0.022 0.919±0.026 2.039±0.139 2.038±0.118 1.200±0.177|0.804±0.121 1.224±0.119|1.109±0.084
Mod-wt 0.579±0.033 0.601±0.071 0.834±0.093 0.721±0.101 1.220±0.092|0.835±0.044 1.245±0.108|1.300±0.073
MetaNet 0.759±0.132 0.764±0.079 2.911±0.219 1.938±0.171 1.245±0.086|0.971±0.097 1.240±0.087|1.269±0.040
MAML 0.627±0.003 0.677±0.019 1.862±0.187 1.935±0.039 1.387±0.191|1.077±0.124 1.297±0.143|1.199±0.086
DyAd 0.423±0.011 0.533±0.017 0.741±0.063 0.689±0.062 1.006±0.067|0.486±0.041 0.955±0.093|0.544±0.113
Figure5.TargetandpredictionsbyResNet,Modular-wtandDyAdattime1,5,10forturbulentflowswithbuoyancyfactors9(left)
and21(right)respectively.WecanseethatDyAdcaneasilygeneratepredictionsforvariousflowswhileResNetandModular-wt
havetroubleunderstandinganddisentanglingbuoyancyfactors.
beabletoinferthemeantemperatureofthesubregionas modulesthroughthefinaloutput.
wellastoclassifyfourseasonsgiventhetemperatureseries. • Mod-wt:Amodularmeta-learningvariantwhichuses
attention weights to combine the parameters of the
Ocean Currents. We also experiment with the velocity
convolutionalkernelsfornewtasks.
fieldsofoceancurrentsfromthesameregionandusethe
• MetaNet(Munkhdalai&Yu,2017): Amodel-based
sametaskdivisionastheseasurfacetemperaturedataset.
meta-learningmethodwhichrequiresafewlabelsfrom
Similar to the turbulent flow data set, we use the mean
testtasksasasupportsettoadapt.
vorticityofeachsubregionastheweak-supervisionsignal.
• MAML (Finn et al., 2017): A popular gradient-based
5.2.Baselines meta-learningapproach. Wereplacedtheclassifierin
theoriginalmodelwithaResNetforregression.
WeincludeseveralSoTAbaselinesfrommeta-learning,as
wellascommonmethodsfordynamicsforecasting. Both Mod-attn and Mod-wt have a convolutional en-
codertogenerateattentionweights. MetaNetrequiresa
• ResNet(Heetal.,2016):Awidelyadoptedvideopre- fewsamplesfromtesttasksasasupportsetandMAMLneeds
dictionmodel(Opreaetal.,2020;Wangetal.,2020b).
adaptationretrainingontesttasks,whileothermodelsdo
• U-net(Ronnebergeretal.,2015): Originallydevel-
not need any information from the test domains. Details
opedforbiomedicalimagesegmentation,adaptedfor
aboutbaselinescanbefoundinAppendixA.2.
dynamicsforecasting(deBezenacetal.,2018)
• Mod-ind: Asimplebaselinewithindependentneural 5.3.ExperimentSetup
networkmodulestrainedfordifferenttasks.
Foralldatasets,weuseaslidingwindowapproachtogener-
• Mod-attn: Amodularmeta-learningmethodwhich
atesamplesofsequences. Fortest-future,wetrainandtest
combines modules to generalize to new tasks (Alet
onthesametasksbutdifferenttimesteps. Fortest-domain,
etal.,2018). Attentionmechanismisusedtocombine
wetrainandtestondifferenttaskswitha80-20split. AllMeta-LearningDynamicsForecastingUsingTaskInference
modelsaretrainedtomakenextsteppredictiongiventhe
Table2.Ablationstudy:predictionRMSEofDyAd,DyAdwith-
previous20stepsasinput. Weforecastinanautoregressive
outencoder,DyAdwithencodertrainedbywrongsupervisionc
mannertogeneratemulti-stepaheadpredictions. Allresults
andDyAdwithendtoendtraining.
areaveragedover3runswithrandominitialization.
Model test-future test-domain
Apartfromrootmeansquareerror,wealsoreporttheen-
DyAd(ours) 0.423±0.011 0.533±0.017
ergyspectrumerrorforoceancurrentpredictionwhichis
No_enc 0.627±0.033 0.601±0.022
theRMSEregardingthelogofenergyspectrum. ESEcan
Wrong_enc 0.658±0.019 0.617±0.031
indicatewhetherthepredictionspreservethecorrectstatisti-
End2End 0.449±0.003 0.536±0.003
caldistributionandobeytheenergyconservationlaw,which
isacriticalmetricforphysicalconsistency. Detailsabout
importanttohaveacthatisrelatedtothetaskdomain. As
energyspectrumcanbefoundinAppendixA.3.
an ablative study, we fed the encoder in DyAd a random
c,leadingtoWrong_enc. Weseethathavingthewrong
supervisionmayhurttheforecastingperformance. Wealso
tried to train DyAd end-to-end (End2End) but observed
worseperformancethanthetwo-stagetrainingapproach.
ControllableForecast. DyAdinfersthecharacteristicla-
tentvariablefromdata,whichallowsdirectcontrolofthe
latent characteristics. We tried to vary the encoder input
whilekeepingthepredictionnetworkinputfixed. Figure7
showstheoutputsfromDyAdwhentheencoderisfedwith
flowwithdifferentbuoyancyfactorsc=5,15,25. Wecan
Figure6.The energy spectrum of target and predictions by seewithhigherbuoyancyfactors,thepredictionsbecome
ResNet,U-netandDyAdonfuturetestset(left)anddomain moreturbulent. Thisdemonstratesthattheencodercandis-
testset(right)ofoceancurrents. entangleandcontrolthelatentcharacteristicsofpredictions.
5.4.ExperimentResults
PredictionPerformance. Table1showstheRMSEof10-
stepaheadpredictionsontwotestsetsofturbulentflowsand
seasurfacetemperature,andDyAdmakesthemostaccurate
predictionsforbothtestsetsofbothdatasets.Figure5shows
thetargetandpredictionsbyResNet,Modular-wtand
DyAd at time 1, 5, 10 for turbulent flows with buoyancy
factors9(left)and21(right)respectively. Wecanseethat
DyAdcangeneraterealisticflowswiththecorresponding
characteristics while the other two models have trouble
Figure7.Outputs from DyAd while we vary encoder input but
understandinganddisentanglingthebuoyancyfactor.
keepthepredictionnetworkinputfixed.Fromlefttoright,theen-
Table1alsoreportstheRMSEandESEof20-stepahead coderisfedwithflowwithdifferentbuoyancyfactorc=5,15,25.
oceancurrentspredictions. DyAdnotonlyhassmallRMSE thepredictionnetworkinputhasfixedbuoyancyc=15.
butalsoobtainsthesmallestESE,suggestingitcapturesthe
6.Conclusion
statisticaldistributionofoceancurrentswell.Figure6shows
theenergyspectrumoftargetandpredictionsbyResNet,
We propose a model-based meta-learning method, DyAd
U-netandDyAdontwotestsetsofoceancurrents,and
to forecast physical dynamics. DyAd uses an encoder to
wecanseethatDyAdistheclosesttothetarget.
infer the parameters of the task and a prediction network
Ablation Study. We also performed an ablation study of toadaptandforecastgivingtheinferredtask. Ourmodel
DyAd to understand the contribution of each component, canalsoleverageanyweaksupervisionsignalsthatcanhelp
showninTable2. WefirstremovetheencoderfromDyAd distinguishdifferenttasks,allowingtheincorporationofad-
whilekeepingthesamepredictionnetwork(No_enc). The ditionaldomainknowledge. Onchallengingturbulentflow
resulting model degrades but still outperforms ResNet. predictionandreal-worldoceantemperatureandcurrents
ThisdemonstratestheeffectivenessofAdaINandAdaPad forecastingtasks,weobservesuperiorperformanceofour
forprediction. Anothernotablefeatureofourmodelisthe modelacrossheterogeneousdynamics. Futureworkwould
abilitytoinfertaskswithweaklysupervisedsignalc. Itis considernon-griddatasuchasflowsonagraphorasphere.Meta-LearningDynamicsForecastingUsingTaskInference
References Dumoulin,V.,Shlens,J.,andKudlur,M.Alearnedrepresen-
tationforartisticstyle. arXivpreprintarXiv:1610.07629,
Alet,F.,Lozano-Perez,T.,andKaelbling,L. Modularmeta-
2016.
learning. ArXiv,abs/1806.10166,2018.
Dumoulin,V.,Perez,E.,Schucher,N.,Strub,F.,Vries,H.d.,
Anderson,B.,Hy,T.-S.,andKondor,R. Cormorant: Co-
Courville,A.,andBengio,Y. Feature-wisetransforma-
variantmolecularneuralnetworks. InAdvancesinneural
tions. Distill,3(7):e11,2018.
informationprocessingsystems(NeurIPS),2019.
Finn,C.,Abbeel,P.,andLevine,S. Model-agnosticmeta-
Ando,R.K.,Zhang,T.,andBartlett,P. Aframeworkfor
learningforfastadaptationofdeepnetworks. InInterna-
learning predictive structures from multiple tasks and
tionalConferenceofMachineLearning,2017.
unlabeleddata. JournalofMachineLearningResearch,
6(11),2005. Gatys,L.A.,Ecker,A.S.,andBethge,M.Imagestyletrans-
ferusingconvolutionalneuralnetworks. InProceedings
Andrychowicz, M., Denil, M., Colmenarejo, S. G., Hoff-
oftheIEEEconferenceoncomputervisionandpattern
man, M. W., Pfau, D., Schaul, T., and Freitas, N. D.
recognition,pp.2414–2423,2016.
Learningtolearnbygradientdescentbygradientdescent.
InAdvancesinNeuralInformationProcessingSystems, Ghiasi, G., Lee, H., Kudlur, M., Dumoulin, V., and
2016. Shlens, J. Exploring the structure of a real-time, arbi-
traryneuralartisticstylizationnetwork. arXivpreprint
Ayed,I.,Bézenac,E.D.,Pajot,A.,andGallinari,P. Learn-
arXiv:1705.06830,2017.
ing partially observed PDE dynamics with neural net-
works, 2019a. URL https://openreview.net/ Grant,E.,Finn,C.,Levine,S.,Darrell,T.,andGriffiths,T.
forum?id=HyefgnCqFm. Recastinggradient-basedmeta-learningashierarchical
bayes. ArXivPreprint,abs/1801.08930,2018.
Ayed,I.,deBézenac,E.,Pajot,A.,Brajard,J.,andGallinari,
P. Learningdynamicalsystemsfrompartialobservations. Gupta,A.,Mendonca,R.,Liu,Y.,Abbeel,P.,andLevine,
ArXiv,abs/1902.11136,2019b. S. Meta-reinforcementlearningofstructuredexploration
strategies. InProceedingsofthe32ndInternationalCon-
Azencot, O., Erichson, N., Lin, V., and Mahoney, M. W.
ferenceonNeuralInformationProcessingSystems,pp.
Forecasting sequential data using consistent koopman
5307–5316,2018.
autoencoders. InInternationalConferenceonMachine
Learning,2020a. He,K.,Zhang,X.,Ren,S.,andSun,J. Deepresiduallearn-
ingforimagerecognition. InProceedingsoftheIEEE
Azencot, O., Erichson, N. B., Lin, V., and Mahoney, M.
conferenceoncomputervisionandpatternrecognition,
Forecasting sequential data using consistent koopman
pp.770–778,2016.
autoencoders. InInternationalConferenceonMachine
Learning,pp.475–485.PMLR,2020b. Huang,X.andBelongie,S. Arbitrarystyletransferinreal-
timewithadaptiveinstancenormalization. InProceed-
Baxter,J. Theoreticalmodelsoflearningtolearn. InLearn-
ingsoftheIEEEInternationalConferenceonComputer
ingtolearn,pp.71–94.Springer,1998.
Vision,pp.1501–1510,2017.
Chen,R.T.,Rubanova,Y.,Bettencourt,J.,andDuvenaud,D.
Jing, Y., Yang, Y., Feng, Z., Ye, J., Yu, Y., and Song, M.
Neuralordinarydifferentialequations. InProceedingsof
Neural style transfer: A review. IEEE transactions on
the32ndInternationalConferenceonNeuralInformation
visualizationandcomputergraphics,26(11):3365–3385,
ProcessingSystems,pp.6572–6583,2018.
2019.
de Bezenac, E., Pajot, A., and Gallinari, P. Deep learn-
Karras, T., Laine, S., and Aila, T. A style-based genera-
ingforphysicalprocesses: Incorporatingpriorscientific
torarchitectureforgenerativeadversarialnetworks. In
knowledge. In International Conference on Learning
ProceedingsoftheIEEE/CVFConferenceonComputer
Representations,2018. URLhttps://openreview.
VisionandPatternRecognition,pp.4401–4410,2019.
net/forum?id=By4HsfWAZ.
Kim,J.andLee,C. Deepunsupervisedlearningofturbu-
Duan,Y.,Andrychowicz,M.,Stadie,B.C.,Ho,J.,Schnei-
lenceforinflowgenerationatvariousreynoldsnumbers.
der,J.,Sutskever,I.,Abbeel,P.,andZaremba,W. One-
arXiv:1908.10515,2019.
shotimitationlearning. InAdvancesinNeuralInforma-
tionProcessingSystems,2017. Kolter,J.Z.andManek,G. Learningstabledeepdynamics
models. InAdvancesinNeuralInformationProcessing
Systems(NeurIPS),volume32,pp.11128–11136,2019.Meta-LearningDynamicsForecastingUsingTaskInference
Kyprianidis,J.E.,Collomosse,J.,Wang,T.,andIsenberg, Oprea,S.,Martinez-Gonzalez,P.,Garcia-Garcia,A.,Castro-
T. Stateofthe"art”: Ataxonomyofartisticstylization Vargas, J. A., Orts-Escolano, S., Garcia-Rodriguez, J.,
techniques for images and video. IEEE transactions andArgyros,A.Areviewondeeplearningtechniquesfor
onvisualizationandcomputergraphics,19(5):866–885, videoprediction. IEEETransactionsonPatternAnalysis
2012. andMachineIntelligence,2020.
Lemke, C. and Gabrys, B. Meta-learning for time series Oreshkin,B.N.,Carpov,D.,Chapados,N.,andBengio,Y.
forecastingandforecastcombination. Neurocomputing, N-beats:Neuralbasisexpansionanalysisforinterpretable
73(10-12):2006–2016,2010. timeseriesforecasting. InInternationalConferenceon
LearningRepresentations,2019.
Li, Z., Kovachki, N., Azizzadenesheli, K., Liu, B., Bhat-
tacharya,K.,Stuart,A.,andAnandkumar,A.Fourierneu-
Prabhumoye,S.,Tsvetkov,Y.,Salakhutdinov,R.,andBlack,
raloperatorforparametricpartialdifferentialequations.
A. W. Style transfer through back-translation. In Pro-
InternationalConferenceonLearningRepresentations,
ceedingsofthe56thAnnualMeetingoftheAssociation
2021.
forComputationalLinguistics(Volume1: LongPapers),
Locatello,F.,Bauer,S.,Lucic,M.,Raetsch,G.,Gelly,S., pp.866–876,2018.
Schölkopf, B., and Bachem, O. Challenging common
Raissi,M.,Perdikaris,P.,andKarniadakis,G.E. Physics
assumptionsintheunsupervisedlearningofdisentangled
informeddeeplearning(parti): Data-drivensolutionsof
representations. Ininternationalconferenceonmachine
nonlinear partial differential equations. arXiv preprint
learning,pp.4114–4124.PMLR,2019.
arXiv:1711.10561,2017.
Lutter, M., Ritter, C., andPeters, J. Deeplagrangiannet-
Redko,I.,Habrard,A.,andSebban,M. Theoreticalanalysis
works:Usingphysicsasmodelpriorfordeeplearning.In
ofdomainadaptationwithoptimaltransport. InJointEu-
InternationalConferenceonLearningRepresentations,
ropeanConferenceonMachineLearningandKnowledge
2018.
DiscoveryinDatabases,pp.737–753.Springer,2017.
Madec, G. et al. NEMO ocean engine, 2015. Technical
Ronneberger,O.,Fischer,P.,andBrox,T. U-net: Convolu-
Note. Institut Pierre-Simon Laplace (IPSL), France.
https://epic.awi.de/id/eprint/39698/ tionalnetworksforbiomedicalimagesegmentation.InIn-
1/NEMO_book_v6039.pdf. ternationalConferenceonMedicalimagecomputingand
computer-assistedintervention, pp.234–241.Springer,
Massague,A.C.,Zhang,C.,Feric,Z.,Camps,O.,andYu, 2015.
R. Learningdisentangledrepresentationsofvideowith
missingdata. arXivpreprintarXiv:2006.13391,2020. Ruder, M., Dosovitskiy, A., and Brox, T. Artistic style
transfer for videos. In German conference on pattern
Maurer,A.,Pontil,M.,andRomera-Paredes,B. Thebenefit
recognition,pp.26–36.Springer,2016.
ofmultitaskrepresentationlearning. JournalofMachine
LearningResearch,17(81):1–32,2016. Rusu,A.A.,Rao,D.,Sygnowski,J.,Vinyals,O.,Pascanu,
R., Osindero, S., and Hadsell, R. Meta-learning with
MaziarRaissi,ParisPerdikaris,G.E.K. Physics-informed
latentembeddingoptimization. InInternationalConfer-
neuralnetworks: Adeeplearningframeworkforsolving
enceonLearningRepresentations,2019. URLhttps:
forwardandinverseproblemsinvolvingnonlinearpartial
//openreview.net/forum?id=BJgklhAcK7.
differentialequations. JournalofComputationalPhysics,
378:686–707,2019. Santoro,A.,Bartunov,S.,Botvinick,M.,Wierstra,D.,and
Lillicrap,T. Meta-learningwithmemory-augmentedneu-
Mohri,M.,Rostamizadeh,A.,andTalwalkar,A. Founda-
ralnetworks. InICML,2016.
tionsofmachinelearning. MITpress,2018.
Morton,J.,Jameson,A.,J.Kochenderfer,M.,andWither- Sato, S., Dobashi, Y., Kim, T., and Nishita, T. Example-
den,F.Deepdynamicalmodelingandcontrolofunsteady based turbulence style transfer. ACM Transactions on
fluidflows. InAdvancesinNeuralInformationProcess- Graphics(TOG),37(4):1–9,2018.
ingSystems(NeurIPS),2018.
Seo, S., Meng, C., Rambhatla, S., and Liu, Y. Physics-
Munkhdalai,T.andYu,H. Metanetworks. Proceedingsof aware spatiotemporal modules with auxiliary tasks for
machinelearningresearch,70:2554–2563,2017. meta-learning. ArXiv,abs/2006.08831,2020.
Nie,W.,Karras,T.,Garg,A.,Debhath,S.,Patney,A.,Patel,
A.B.,andAnandkumar,A. Semi-supervisedstyleganfor
disentanglementlearning. InInternationalConference
onMachineLearning,2020.Meta-LearningDynamicsForecastingUsingTaskInference
Shi,X.,Gao,Z.,Lausen,L.,Wang,H.,Yeung,D.,Wong, flowprediction.InProceedingsofthe26thACMSIGKDD
W., andchunWoo, W. Deeplearningforprecipitation International Conference on Knowledge Discovery &
nowcasting:Abenchmarkandanewmodel. InAdvances DataMining,pp.1457–1466,2020a.
inneuralinformationprocessingsystems,2017.
Wang,R.,Walters,R.,andYu,R. Incorporatingsymmetry
Snell,J.,Swersky,K.,andZemel,R. Prototypicalnetworks intodeepdynamicsmodelsforimprovedgeneralization.
forfew-shotlearning. InAdvancesinNeuralInformation arXivpreprintarXiv:2002.03061,2020b.
ProcessingSystems,2017.
Xie, Y., Franz, E., Chu, M., and Thuerey, N. tem-
Talagala,T.S.,Hyndman,R.J.,Athanasopoulos,G.,etal.
poGAN: A temporally coherent, volumetric GAN for
Meta-learning how to forecast time series. Technical
super-resolutionfluidflow. ACMTransactionsonGraph-
report,MonashUniversity,DepartmentofEconometrics
ics(TOG),37(4):95,2018.
andBusinessStatistics,2018.
Yao, H., Wei, Y., Huang, J., and Li, Z. Hierarchically
Thrun,S.andPratt,L. Learningtolearn: Introductionand
structuredmeta-learning. InInternationalConferenceon
overview. InLearningtolearn,pp.3–17.Springer,1998.
MachineLearning,pp.7045–7054.PMLR,2019.
Tompson,J.,Schlachter,K.,Sprechmann,P.,andPerlin,K.
Yoon,J.,Kim,T.,Dia,O.,Kim,S.,Bengio,Y.,andAhn,S.
AcceleratingEulerianfluidsimulationwithconvolutional
Bayesianmodel-agnosticmeta-learning. InProceedings
networks. InICML’17Proceedingsofthe34thInterna-
ofthe32ndInternationalConferenceonNeuralInforma-
tionalConferenceonMachineLearning,volume70,pp.
tionProcessingSystems,pp.7343–7353,2018.
3424–3433,2017.
Vinyals, O., Blundell, C., Lillicrap, T., Kavukcuoglu, K., Zoltowski,D.,Pillow,J.,andLinderman,S.Ageneralrecur-
andWierstra,D.Matchingnetworksforoneshotlearning. rentstatespaceframeworkformodelingneuraldynamics
InNIPS,2016. duringdecision-making. InInternationalConferenceon
MachineLearning,pp.11680–11691.PMLR,2020.
Wang,R.,Kashinath,K.,Mustafa,M.,Albert,A.,andYu,
R. Towardsphysics-informeddeeplearningforturbulentMeta-LearningDynamicsForecastingUsingTaskInference
A.ImplementationDetails
A.1.ModelDesign
The prediction network yˆ = f(x,z) is composed of 8 blocks. Each block operates on a hidden state h(i) of shape
B×H×W ×C andyieldsanewhiddenstateh(i+1)oftheshapeB×H×W ×C . Thefirstinputish =xandthe
in out 0
finaloutputiscomputedfromthefinalhiddenstateasyˆ=Conv2D(h(8)). Wedefineeachblockas
a(i) =σ(Conv2D(AdaPad(h(i),z)))
b(i) =σ(Conv2D(AdaPad(a(i),z)))+h(i)
h =AdaIN(b(i),z)
i+1
asillustratedinFigure4.
A.2.ExperimentDetails
Forfaircomparison,wesetthesemodelstohaveequalcapacityasDyAdintermsofnumberofparameters.Hyperparameters
includinglearningrate,inputlengthandthenumberofstepsofaccumulatedlossfortrainingaretunedonvalidationsets.
Baselines. Modular-attnhasaconvolutionalencoderf thattakesthesameinputxaseachmoduleM togenerate
attention weights, (cid:80)m exp[f(x)(l)] M (x). Modular-wt also has the same encoder but to generate weights for
l=1 (cid:80)m exp[f(x)(k)] l
k=1
combiningtheconvolutionparametersofallmodules. Weuseadditionalsamplesofupto20%ofthetestsetfromtesttasks.
MetaNetusestheseasasupportset. MAMLisretrainedonthesesamplesfor10epochforadaptation.
Hyperparametertuning. Wetunedlearningrate(1e-3∼1e-5),batchsize(16∼64),thenumberofaccumulatederrorsfor
backpropogation(2∼5),andhiddensize(64∼512)ofModularNetworksandMeta-Nets. Wefixedthenumberofhistoric
inputframesas20. Whenwetrainedtheencoderonturbulentflowsandseasurfacetemperature,weusedα=1andβ =1.
Foroceancurrents,weusedα=0.2andβ =0.2.
ModelCapacity. Table3displaysthenumberofparametersofeachtunedmodel.
ResNet U-net Mod-ind Mod-attn Mod-wt MetaNets MAML DyAd
20.32 9.69 13.01 13.19 13.19 9.63 20.32 15.60
Table3. Thenumberofparametersofeachmodel.
A.3.Turbulencekineticenergyspectrum
TheturbulencekineticenergyspectrumE(k)isrelatedtothemeanturbulencekineticenergyas
(cid:90) ∞
E(k)dk =((u(cid:48))2+(v(cid:48))2)/2,
0
T
1 (cid:88)
(u(cid:48))2 = (u(t)−u¯)2,
T
t=0
wherethekisthewavenumberandtisthetimestep. Figure8showsatheoreticalturbulencekineticenergyspectrumplot.
Thespectrumcandescribethetransferofenergyfromlargescalesofmotiontothesmallscalesandprovidesarepresentation
ofthedependenceofenergyonfrequency. Thus,theEnergySpectrumErrorcanindicatewhetherthepredictionspreserve
thecorrectstatisticaldistributionandobeytheenergyconservationlaw. Atrivialexamplethatcanillustratewhyweneed
ESEisthatifamodelsimplyoutputsmovingaveragesofinputframes,theaccumulatedRMSEofpredictionsmightnotbe
highbuttheESEwouldbereallybigbecauseallthesmallorevenmediumeddiesaresmoothedout.
B.TheoreticalAnalysis
Thehigh-levelideaofourmethodistolearnagoodrepresentationoftheunderlyingdynamicsfrommultipletasks,andthen
transferthisrepresentationtoatargettask(domainadaptation).Meta-LearningDynamicsForecastingUsingTaskInference
Figure8. Spectrumplot
Definition1(Forecastingtask). Eachforecastingtaskx =f(x ,...)istolearnaconditionaldistributionµoverthe
t+1 t
systemstatesµ:p(x |x ,...)conditionedonthesequenceofpreviousstateswhereµisaprobabilitymeasure.
t+1 t
In our setting, we have K tasks, each of which is sampled from a continuous, finite space {c } ∼ C. Let µ be the
k k
corresponding conditional probability measure p(x ,...,x |c ). For each task c , we have a collection of n series as
t 1 k k
realizationsfromthedynamicalsystemX ={(x ,...,x ;c )(i)}n sampledfromµ . Thesemicolonhererepresents
k t 1 k i=1 k
(cid:83)
thesystembehaviorinaspecificdomainc . LetX= X betheunionofsamplesoveralltasks.
k k
k
Inpractice,weoftenhavesomeintuitionofthevariablesthatdictatethedomain. Therefore,wehavetwopossiblescenarios
fortheroleofcindynamicalsystems:
1. cfullydistinguishesthetask: thedifferencesinX canbecompletelyexplainedbythedifferencesinc ;
k k
2. cpartiallydistinguishesthetask: amorerealisticscenariowhereweonlyhavepartialknowledgeofthedomain. There
existlatentvariablesz(cid:48) thatneedtobeinferredfromrawdata. Togetherz =[c,z(cid:48)]candescribethebehaviorofthe
systeminadomain.
WeassumeScenario1,whichresemblesthemulti-taskrepresentationlearningsetting(Maureretal.,2016)withjointtrue
riskoveralltasks(cid:15)andindividualtasktruerisk(cid:15) definedrespectively
k
K
1 (cid:88) (cid:104) l(cid:16) f(cid:16) x(i)(cid:17)(cid:17)(cid:105)
(cid:15)(f)= (cid:15) (f), (cid:15) (f)=E (6)
K k k x( ki)∼µk k
k=1
andcorrespondingempiricalrisks
K
1 (cid:88)
(cid:15)ˆ(f,X)= l(f(X )), (cid:15)ˆ (f,X )=l(f(X )),
K k k k k
k=1
wherelisalossfunction.
B.1.Multi-TaskLearningError
Wewanttoboundthetrueloss(cid:15)usingtheempiricalloss(cid:15)ˆandRademachercomplexityofthehypothesisclassF. Wecan
usetheclassicresultsfrom(Andoetal.,2005). DefineempiricalRademachercomplexityforsamplesfromalltasksas
(cid:34) (cid:32) K n (cid:33)(cid:35)
Rˆ (F)=E sup 1 (cid:88)(cid:88) σ(i)l(f(x(i))) (7)
X σ nK k k
f∈F
k=1i=1Meta-LearningDynamicsForecastingUsingTaskInference
where{σ(i)}areindependentbinaryvariablesσ(i) ∈{−1,1}. ThetrueRademachercomplexityisthendefinedR(F)=
k k
E (Rˆ (F)).
X X
Thefollowingtheoremrestatesthemainresultfrom(Andoetal.,2005). WesimplifythestatementbyusingtheRademacher
complexityratherthanthesetcovernumberargumentusedintheoriginalproof.
TheoremB.1. (Andoetal.,2005)GivendatafromK differentforecastingtasksµ ,··· ,µ andf inhypothesisclassF,
1 k
forsomeconstantC withprobabilityatleast1−δ,thefollowinginequalityholds:
(cid:114)
1 (cid:88) 1 (cid:88) log1/δ
(cid:15) (f)≤ (cid:15)ˆ (f)+2R(F)+C . (8)
K k K k nK
k k
√
Ifweassumethelossisboundedl≤1/2,thenwemaytakeC =1/ 2.
Proof. Consider{x(i)}asindependentrandomvariables. Forafunctionφthatsatisfies
k
|φ(x(1),··· ,x(i),···x(n))−φ(x(1),··· ,x˜(i),···x(n))|≤c
i
byMcDiarmid’sinequality,wehave
(cid:16) (cid:17) (cid:18) 2t2 (cid:19)
p φ(x(1),··· ,x(n))−E[φ]≥t ≤exp − .
(cid:80) c2
i i
ApplyingthisinequalitytothemaxdifferenceQ(X)=sup [(cid:15)(f)−(cid:15)ˆ(f,X)],thenwithprobabilityatleast1−δ,we
f∈F
have
(cid:114)
log1/δ
Q(X)−E [Q(X)]≤C
X nK
whereC isaconstantdependingontheboundsc . Ifthelossl≤1/2,then|Q|≤1/2andsowecantakec =1leading
√ i i
toC =1/ 2. Astandardcomputation(see(Mohrietal.,2018),Theorem3.3)usingthelawoftotalexpectationshows
E [Q(X)]≤2R(F),whichfinishestheproof.
X
Wecanusethistocomparethegeneralizationerrorofmulti-tasklearningversusthatoflearningtheindividualtasks. The
followinginequalitycomparestheRademachercomplexityformulti-tasklearningtothatofindividualtasklearning. Denote
Rˆ andR theempiricalandtrueRademachercomplexityforF overµ .
Xk k k
TheRademachercomplexityformulti-tasklearningisboundedR(F)≤(1/K)(cid:80)K
LemmaB.2. R (F).
k=1 k
Proof. WecomputetheempiricalRademachercomplexity,
(cid:34) (cid:32) K n (cid:33)(cid:35) (cid:34) K (cid:32) n (cid:33)(cid:35)
Rˆ (F)=E sup 1 (cid:88)(cid:88) σ(i)l(cid:16) f(cid:16) x(i)(cid:17)(cid:17) ≤E (cid:88) sup 1 (cid:88) σ(i)l(cid:16) f(cid:16) x(i)(cid:17)(cid:17)
X σ nK k k σ nK k k
f∈F f∈F
k=1i=1 k=1 i=1
K (cid:34) (cid:32) n (cid:33)(cid:35)
1 (cid:88) 1 (cid:88) σ(i)l(cid:16) f(cid:16) x(i)(cid:17)(cid:17)
= E sup
K σ n k k
f∈F
k=1 i=1
K (cid:34) (cid:32) n (cid:33)(cid:35)
1 (cid:88) 1 (cid:88) σ(i)l(cid:16) f(cid:16) x(i)(cid:17)(cid:17)
= E sup
K σk n k k
f∈F
k=1 i=1
K
= 1 (cid:88) Rˆ (F)
K Xk
k=1
Thefirstinequalityfollowsfromthesub-additivityofthesupremumfunction. Thenextequalityisduetothefactpositive
scalarscommutewithsupremum,andbythelinearityofexpectation. TheexpectationE reducestotheexpectationE
σ σk
over only those Rademacher variables appearing inside the expectation. R (F) is the Rademacher complexity of the
k
functionontheindividualtaskk. TakingexpectationoverallsamplesXgivestheresult.Meta-LearningDynamicsForecastingUsingTaskInference
ItisinstructivetocomparetheboundfromTheoremB.1withthegeneralizationerrorboundobtainedbyconsideringeach
taskindividually.
PropositionB.3. Assumen=n foralltaskskandthelosslisboundedl≤1/2,thenthegeneralizationboundgivenby
k
consideringeachtaskindividuallyis
(cid:32) K (cid:33) (cid:114)
1 (cid:88) log1/δ
(cid:15)(f)≤(cid:15)ˆ(f)+2 R (F) + . (9)
K k 2n
k=1
whichisstrictlylooserthantheboundfromTheoremB.1underthesameassumptions.
This result helps to explain why our multitask learning framework has better generalization than learning each task
independently. Theshareddatatightensthegeneralizationbound.
Proof. ApplyingtheclassicalanalogofTheoremB.1toasingletask,wefindwithprobabilitygreaterthan1−δ,
(cid:114)
log1/δ
(cid:15) (f)≤(cid:15)ˆ (f)+2R (F)+C .
k k k k n
Averagingoveralltasksyields
K K K K (cid:114)
1 (cid:88) 1 (cid:88) 1 (cid:88) 1 (cid:88) log1/δ
(cid:15) (f)≤ (cid:15)ˆ (f)+2 R (F)+ C .
K k K k K k K k n
k=1 k=1 k=1 k=1
√
Sincethelosslisboundedl≤1/2,wecantakeC =C =1/ 2,givingthegeneralizationupperboundforthejointerror
k
ofEquation9.
√ √
ByLemmaB.2andthefact1/ 2nK ≤1/ 2n,theboundinTheoremB.1isstrictlytighter.
B.2.DomainAdaptationError
Sincewetestonc ∼ C outsidethetrainingset{c },weincurerrorduetodomainadaptationfromthesourcedomains
k
µ ,...,µ totargetdomainµ withµbeingthetruedistribution. Denotethecorrespondingempiricaldistributionsofn
sac m1 plespec rK (cid:80)nc
taskbyµˆ = 1 c δ . Fordifferentcandc(cid:48),thedomainsµ andµ mayhavelargelydisjointsupport,
c nc i=1 x( ci) c c(cid:48)
leadingtoveryhighKLdivergence. However,ifcandc(cid:48) areclose,samplesx ∼ µ andx ∼ µ maybecloseinthe
c c c(cid:48) c(cid:48)
domainX withrespecttothemetric(cid:107)·(cid:107) . Forexample,iftheexternalforcescandc(cid:48)areclose,thedistancebetweenthe
X
velocityfields(cid:107)x −x (cid:107)maybesmall. Choosingameasurementbetweenµ andµ whichdependsonthemetricinthe
c c(cid:48) c c(cid:48)
spaceX suchastheWassersteindistanceW (µ ,µ )isthusappropriate. Theboundfrom(Redkoetal.,2017)applieswell
1 c c(cid:48)
tooursettingassuch:
(cid:16) (cid:17)
(f)+1/K(cid:80)K
Theorem B.4 ((Redko et al., 2017),Theorem 2). Let λ = min (cid:15) (cid:15) (f) . There is N =
c f∈F c k=1 ck
N(dim(X))suchthatforn>N,foranyhypothesisf,withprobabilityatleast1−δ,
K (cid:32) K (cid:33)
1 (cid:88) 1 (cid:88)
(cid:15) (f)≤ (cid:15) (f)+W µˆ , µˆ
c K ck 1 c K ck
k=1 k=1
(cid:112) (cid:16)(cid:112) (cid:112) (cid:17)
+ 2log(1/δ) 1/n+ 1/(nK) +λ .
c
=1/K(cid:80)K
Proof. Weapply(Redkoetal.,2017)Theorem2totargetdomainµ =µ andjointsourcedomainµ µ
T c S k=1 ck
=1/K(cid:80)K
withempiricalsamplesµˆ =µˆ andµˆ µˆ .
T c S k=1 ck
B.3.EncoderversusPredictionNetworkError
OurgoalistolearnajointhypothesishovertheentiredomainX intwosteps,firstinferringthetaskcandtheninferring
x conditionedonc. ErrorfromDyAdmayresultfromeithertheencoderg orthepredictionnetworkf . Ourhypothesis
t+1 φ θ
spacehastheform{x(cid:55)→f (x,g (x))}whereφandθaretheweightsoftheencoderandpredictionnetworkrespectively.
θ φMeta-LearningDynamicsForecastingUsingTaskInference
Let(cid:15) betheerrorovertheentiredomainX,thatis,forallc. Let(cid:15) (g )=E (L (g(x),g (x))betheencodererror
X enc φ x∼X 1 φ
whereg: X →C isthegroundtruth. Westatearesultthatdecomposesthefinalerrorintothatattributabletotheencoder
andthattothepredictionnetwork.
PropositionB.5. Assumec(cid:55)→f (·,c)isLipschitzcontinuouswithLipschitzconstantγ uniformlyinθ. Thenwebound
θ
(cid:15) (f (·,g (·)))≤γ(cid:15) (g )+E [(cid:15) (f (x,c))] (10)
X θ φ enc φ c∼C c θ
wherethefirsttermistheerrorduetotheencoderincorrectlyidentifyingthetaskandthesecondtermistheerrorduethe
predictionnetworkalone.
The hypothesis in the second term consists of the prediction network combined with the ground truth task label x (cid:55)→
f (x,g(x)).
θ
Proof. Bythetriangleinequalityandlinearityofexpectation,
(cid:15) (f (·,g (·)))=E [E [(cid:107)f (x,g (x))−f(x)(cid:107) ]]
X θ φ c∼C x∼µc θ φ Y
≤E [E [(cid:107)f (x,g (x))−f (x,c)(cid:107) ]]+E [E [(cid:107)f (x,c)(cid:107) −f(x)(cid:107) ]].
c∼C x∼µc θ φ θ Y c∼C x∼µc θ Y Y
ByLipschitzcontinuity,
≤E [E [γ(cid:107)g (x)−c(cid:107) ]]+E [E [(cid:107)f (x,c)(cid:107) −f(x)(cid:107) ]],
c∼C x∼µc φ C c∼C x∼µc θ Y Y
which,sinceg(x)=candbylinearityofexpectation,
=γE [E [(cid:107)g (x)−g(x)(cid:107) ]]+E [E [(cid:107)f (x,c)(cid:107) −f(x)(cid:107) ]]
c∼C x∼µc φ C c∼C x∼µc θ Y Y
andbydefinitionof(cid:15) and(cid:15) ,
enc c
=γ(cid:15) (g )+E [(cid:15) (f (x,c))]
enc φ c∼C c θ
asdesired.
BycombiningTheoremB.1,PropositionB.5, andTheoremB.4, wecanboundthegeneralizationerrorintermsofthe
empiricalerrorofthepredictionnetworkonthesourcedomains,theWassersteindistancebetweenthesourceandtarget
domains,andtheempiricalerroroftheencoder.
LetG ={g : X →C}bethetaskencoderhypothesisspace. Denotetheempiricalriskoftheencoderg withrespecttoX
φ φ
by(cid:15)ˆ (g ).
enc φ
PropositionB.6. AssumingthehypothesesofTheoremB.1,PropositionB.5,andTheoremB.4,
K
1 (cid:88)
(cid:15) (f (·,g (·)))≤γ(cid:15)ˆ (g )+ (cid:15)ˆ (f (·,c ))+2γR(G)+2R(F)
X θ φ enc φ K ck θ k
k=1
(cid:114)
log(1/δ) (cid:112) (cid:16)(cid:112) (cid:112) (cid:17)
+(γ+1) + 2log(1/δ) 1/n+ 1/(nK)
2nK
(cid:34) (cid:32) K (cid:33) (cid:35)
1 (cid:88)
+E W µˆ , µˆ +λ .
c∼C 1 c K ck c
k=1
Proof. WestartwiththeboundofPropositionB.5,
(cid:15) (f (·,g (·)))≤γ(cid:15) (g )+E [(cid:15) (f (x,c))]. (11)
X θ φ enc φ c∼C c θ
ByTheoremB.1or(Mohrietal.,2018),Theorem3.3,wecanbound
(cid:114)
log(1/δ)
(cid:15) (g )≤(cid:15)ˆ (g )+2R(G)+ . (12)
enc φ enc φ 2nKMeta-LearningDynamicsForecastingUsingTaskInference
InordertoapplyTheoremB.1totherisk(cid:15) andrelateittotheempiricalrisk,weneedtofirstrelatetheerroronthetarget
c
domainbacktothesourcedomainofourempiricalsamples. ByTheoremB.4,
K (cid:32) K (cid:33)
1 (cid:88) 1 (cid:88) (cid:112) (cid:16)(cid:112) (cid:112) (cid:17)
(cid:15) (f (·,c))≤ (cid:15) (f (·,c ))+W µˆ , µˆ + 2log(1/δ) 1/n+ 1/(nK) +λ . (13)
c θ K ck θ k 1 c K ck c
k=1 k=1
ApplyingTheoremB.1,thisis
K (cid:114) (cid:32) K (cid:33)
1 (cid:88) log1/δ 1 (cid:88) (cid:112) (cid:16)(cid:112) (cid:112) (cid:17)
≤ (cid:15)ˆ (f (·,c ))+2R(F)+ +W µˆ , µˆ + 2log(1/δ) 1/n+ 1/(nK) +λ . (14)
K ck θ k 2nK 1 c K ck c
k=1 k=1
Substituting(12)and(14)into(11)gives
(cid:32) (cid:114) (cid:33)
log(1/δ)
(cid:15) (f (·,g (·)))≤γ (cid:15)ˆ (g )+2R(G)+
X θ φ enc φ 2nK
(cid:34) K (cid:114)
1 (cid:88) log1/δ
+E (cid:15)ˆ (f (·,c ))+2R(F)+
c∼C K ck θ k 2nK
k=1
(cid:32) K (cid:33) (cid:35)
1 (cid:88) (cid:112) (cid:16)(cid:112) (cid:112) (cid:17)
+ W µˆ , µˆ + 2log(1/δ) 1/n+ 1/(nK) +λ .
1 c K ck c
k=1
Finallyusinglinearityoftheexpectationoverc∼C,removingitwherethereisnodependenceonc,andrearrangingterms
givestheresult.