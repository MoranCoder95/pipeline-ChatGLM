Sample Complexity of Asynchronous Q-Learning:
Sharper Analysis and Variance Reduction
Gen Li∗ Yuting Wei† Yuejie Chi‡ Yuantao Gu∗ Yuxin Chen§
Tsinghua CMU CMU Tsinghua Princeton
0202
June, 2020; Revised: September 2020
peS
Abstract
AsynchronousQ-learningaimstolearntheoptimalaction-valuefunction(orQ-function)ofaMarkov
82
decisionprocess(MDP),basedonasingletrajectoryofMarkoviansamplesinducedbyabehaviorpolicy.
Focusing on a γ-discounted MDP with state space S and action space A, we demonstrate that the ℓ ∞-
]GL.sc[ basedsample complexityofclassical asynchronousQ-learning—namely,thenumberofsamples needed
toyield an entrywiseε-accurate estimate of theQ-function — is at most on the order of
1 + t mix
µ min(1−γ)5ε2 µ min(1−γ)
up to some logarithmic factor, provided that a proper constant learning rate is adopted. Here, t mix and
2v14030.6002:viXra µ min denote respectively the mixing time and the minimum state-action occupancy probability of the
sample trajectory. The first term of this bound matches the complexity in the synchronous case with
independentsamples drawn from the stationary distribution of the trajectory. The second term reflects
thecosttakenfortheempiricaldistributionoftheMarkoviantrajectorytoreachasteadystate,whichis
incurred at thevery beginning and becomes amortized as thealgorithm runs. Encouragingly, theabove
bound improves upon the state-of-the-art result Qu and Wierman (2020) by a factor of at least |S||A|.
Further,thescaling on thediscount complexity can be improved bymeans of variance reduction.
Keywords: model-free reinforcement learning, asynchronous Q-learning, Markovian samples, variance re-
duction, TD learning, mixing time
Contents
1 Introduction 2
2 Models and background 4
3 Asynchronous Q-learning on a single trajectory 6
3.1 Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
3.2 Theoretical guarantees for asynchronous Q-learning. . . . . . . . . . . . . . . . . . . . . . . . 6
3.3 A special case: TD learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
3.4 Adaptive and implementable learning rates . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
4 Extension: asynchronous variance-reduced Q-learning 10
4.1 Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
4.2 Theoretical guarantees for variance-reduced Q-learning . . . . . . . . . . . . . . . . . . . . . . 11
∗Department ofElectronicEngineering, TsinghuaUniversity,Beijing100084, China.
†Department ofStatistics andDataScience, CarnegieMellonUniversity,Pittsburgh, PA15213, USA.
‡Department ofElectrical andComputerEngineering, CarnegieMellonUniversity,Pittsburgh, PA15213, USA.
§Department ofElectrical Engineering,Princeton University,Princeton, NJ08544,USA.
15 Related work 12
6 Analysis of asynchronous Q-learning 13
6.1 Error decay under constant learning rates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
6.2 Proof of Theorem 5. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
6.2.1 Key decomposition and a recursive formula . . . . . . . . . . . . . . . . . . . . . . . . 14
6.2.2 Recursive analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
6.3 Proof of Theorem 1. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
7 Discussion 17
A Preliminaries on Markov chains 18
A.1 Concentration of empirical distributions of Markov chains . . . . . . . . . . . . . . . . . . . . 18
A.2 Connection between the mixing time and the cover time . . . . . . . . . . . . . . . . . . . . . 19
B Cover-time-based analysis of asynchronous Q-learning 21
C Analysis under adaptive learning rates (proof of Theorem 3) 22
D Analysis of asynchronous variance-reduced Q-learning 23
D.1 Per-epoch analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
D.1.1 Phase 1: when Q Q⋆ >1/√1 γ . . . . . . . . . . . . . . . . . . . . . . . . . . 24
k Q− Q⋆k∞ 1/√1−
D.1.2 Phase 2: when γ . . . . . . . . . . . . . . . . . . . . . . . . . . 25
k − k∞ ≤ −
D.2 How many epochs are needed? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
E Proofs of technical lemmas 27
E.1 Proof of Lemma 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
E.2 Proof of Lemma 2 and Lemma 7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
E.3 Proof of Lemma 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
E.4 Proof of Lemma 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
E.5 Proof of Lemma 8 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
E.6 Proof of Lemma 6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
1 Introduction
Model-free algorithms such as Q-learning (Watkins and Dayan, 1992) play a central role in recent break-
throughs of reinforcement learning (RL) (Mnih et al., 2015). In contrast to model-based algorithms that
decouple model estimation and planning, model-free algorithms attempt to directly interact with the envi-
ronment—in the formofapolicy thatselects actionsbasedonperceivedstatesofthe environment—from
the collected data samples, without modeling the environment explicitly. Therefore, model-free algorithms
are able to process data in an online fashion and are often memory-efficient. Understanding and improving
the sampleefficiencyofmodel-freealgorithmslie atthe coreofrecentresearchactivity(Dulac-Arnold et al.,
2019), whose importance is particularly evident for the class of RL applications in which data collection is
costly and time-consuming (such as clinical trials, online advertisements, and so on).
The current paper concentrates on Q-learning — an off-policy model-free algorithm that seeks to learn
theoptimalaction-valuefunctionbyobservingwhathappensunderabehaviorpolicy. Theoff-policyfeature
makes it appealing in various RL applications where it is infeasible to change the policy under evaluation
on the fly. There are two basic update models in Q-learning. The first one is termed a synchronous setting,
which hypothesizes on the existence of a simulator (or a generative model); at each time, the simulator
generates an independent sample for every state-action pair, and the estimates are updated simultaneously
acrossallstate-actionpairs. Thesecondmodelconcernsanasynchronoussetting,whereonlyasinglesample
trajectoryfollowingabehaviorpolicyisaccessible;ateachtime,thealgorithmupdatesitsestimateofasingle
state-action pair using one state transition from the trajectory. Obviously, understanding the asynchronous
2Paper Sample complexity Learning rate
1
Even-Dar and Mansour (2003) (tcover)1−γ linear: 1
(1 γ)4ε2 t
−
Even-Dar and Mansour (2003) t1 co+ ve3 rω ω1 + tcover 1−1 ω polynomial: 1 , ω (1,1)
(1 γ)4ε2 1 γ tω ∈ 2
− −
Beck and Srikant (2012) (cid:0) t (3 c(cid:1) 1over| γS )| 5| εA(cid:0) 2| (cid:1) constant: (1 −γ) t4 2ε2
− |S||A| cover
1
Qu and Wierman (2020) tmix rescaled linear: µmin(1−γ)
µ2 min(1 −γ)5ε2 t+max {µmin(1 1−γ),tmix}
This work (Theorem 1) µmin(1 −1 γ)5ε2 + µmint (m 1ix −γ) constant: min (1 − γγ 2)4ε2 , tm1 ix
This work (Theorem 2) (1tc γov )e 5r ε2 constant: min(cid:8) (1 − γγ 2)4ε2 ,1 (cid:9)
−
(cid:8) (cid:9)
Table1: SamplecomplexityofasynchronousQ-learningtocomputeanε-optimalQ-functionintheℓ norm,
∞
where we hide all logarithmic factors. With regards to the Markovian trajectory induced by the behavior
policy, we denote by t , t , and µ the covertime, mixing time, and minimum state-actionoccupancy
cover mix min
probability of the associated stationary distribution, respectively.
setting is considerably more challenging than the synchronous model, due to the Markovian (and hence
non-i.i.d.) nature of its sampling process.
Focusing on an infinite-horizon Markov decision process (MDP) with state space and action space ,
S A
this work investigates asynchronous Q-learning on a single Markovian trajectory. We ask a fundamental
question:
How many samples are needed for asynchronous Q-learning to learn the optimal Q-function?
Despite a considerable amount of prior work exploring this algorithm (ranging from the classical work
Jaakkola et al. (1994); Tsitsiklis (1994) to the very recent paper Qu and Wierman (2020)), it remains un-
clearwhether existing sample complexityanalysisofasynchronousQ-learningis tight. As weshallelucidate
momentarily, there exists a large gap — at least as large as — between the state-of-the-art sample
|S||A|
complexity bound for asynchronous Q-learning Qu and Wierman (2020) and the one derived for the syn-
chronous counterpart Wainwright (2019a). This raises a natural desire to examine whether there is any
bottleneck intrinsic to the asynchronous setting that significantly limits its performance.
Our contributions. This paper develops a refined analysis framework that sharpens our understanding
about the sample efficiency of classical asynchronous Q-learning on a single sample trajectory. Setting the
stage,consideraninfinite-horizonMDP withstate space , actionspace , anda discountfactorγ (0,1).
S A ∈
WhatwehaveaccesstoisasampletrajectoryoftheMDPinducedbyastationarybehaviorpolicy. Incontrast
tothesynchronoussettingwithi.i.d.samples,wesingleouttwoparametersintrinsictotheMarkoviansample
trajectory: (i) the mixing time t , which characterizes how fast the trajectory disentangle itself from the
mix
initial state; (ii) the smallest state-action occupancy probability µ of the stationary distribution of the
min
trajectory, which captures how frequent each state-action pair has been at least visited.
Withtheseparametersinplace,ourfindingsunveilthat: thesamplecomplexityrequiredforasynchronous
1
Q-learning to yield an ε-optimal Q-function estimate — in a strong ℓ sense — is at most
∞
1 t
mix
O + . (1)
µ (1 γ)5ε2 µ (1 γ)
min min
(cid:16) − − (cid:17)
e
Thefirstcomponentof (1)isconsistentwiththesamplecomplexityderivedforthesettingwithindependent
samples drawn from the stationary distribution of the trajectory (Wainwright, 2019a). In comparison, the
second term of (1) — which is unaffected by the accuracy level ε — is intrinsic to the Markovian nature
1Let X := (cid:0)|S|,|A|, 1−1 γ,1 ε(cid:1). The notation f(X) = O(g(X)) means there exists a universal constant C1 > 0 such that
f ≤C1g. ThenotationO(·)isdefinedanalogouslyexcept thatithidesanylogarithmicfactor.
e
3of the trajectory; in essence, this term reflects the cost taken for the empirical distribution of the sample
trajectory to converge to a steady state, and becomes amortized as the algorithm runs. In other words, the
behaviorofasynchronousQ-learningwouldresemblewhathappensinthesettingwithindependentsamples,
aslongasthealgorithmhasbeenrunforreasonablylong. Inaddition,ouranalysisframeworkreadilyyields
another sample complexity bound
t
cover
O , (2)
(1 γ)5ε2
(cid:16) − (cid:17)
where t stands for the cover time — nameely, the time taken for the trajectory to visit all state-action
cover
pairs at least once. This facilitates comparisons with several prior results based on the cover time.
Furthermore, we leverage the idea of variance reduction to improve the scaling with the discount com-
plexity 1 . We demonstratethatavariance-reducedvariantofasynchronousQ-learningattainsε-accuracy
1 γ
using at−most
1 t
mix
O + (3)
µ (1 γ)3min 1,ε2 µ (1 γ)
min min
(cid:16) − { } − (cid:17)
samples, matching the complexitey of its synchronous counterpart if ε min 1, 1 (Wainwright,
≤ (1 −γ)√tmix
2019b). Moreover,by taking the action space to be a singleton set, the aforementioned results immediately
(cid:8) (cid:9)
lead to ℓ -based sample complexity guarantees for temporal difference (TD) learning (Sutton, 1988) on
∞
Markoviansamples.
Comparisons with past work. A large fractionofthe classicalliterature focused onasymptotic conver-
gence analysis of asynchronous Q-learning (e.g. Jaakkola et al. (1994); Szepesvári (1998); Tsitsiklis (1994));
these results, however, did not lead to non-asymptotic sample complexity bounds. The state-of-the-art
sample complexity analysis was due to the recent work Qu and Wierman (2020), which derived a sample
complexity bound O µ2 min(1tm −ix γ)5ε2 . Given the obvious lower bound 1/µ min ≥|S||A|, our result (1) improves
upon that of Qu and Wierman (2020) by a factor at least on the order of min t , 1 . In
e(cid:0) (cid:1) |S||A| mix (1 −γ)4ε2
addition,wenotethatseveralpriorworkBeck and Srikant(2012);Even-Dar and Mansour(2003)developed
(cid:8) (cid:9)
sample complexity bounds in terms of the cover time t of the sample trajectory; our result strengthens
cover
these bounds by a factor of at least t2 3 3. The interested reader is referred to Table 1 for
cover|S||A| ≥ |S| |A|
more precise comparisons, and to Section 5 for discussions of further related work.
Notation. Denote by ∆( ) (resp. ∆( )) the probability simplex over the set (resp. ). For any vector
z = [z ] Rn, we oS verload theA notation √ and to denote entry-S wise opeA rations, such that
i 1 i n
√z := [√≤ z i≤ ] i∈ and z := [z ] n. For any v· ectors| z· =| [a i] and w = [w i] n, the notation
1 n i 1 i 1 i n 1 i
z w (resp.≤ z≤ w) m| e| ans z| | w≤≤ (resp. z w ) for all 1 i ≤≤ n. Additionally, w≤ e≤ denote by 1 the
i i i i
all≥ -one vector, I≤ the identity mat≥ rix, and 1 t≤ he indicator fun≤ ction≤ . For any matrix P =[P ], we denote
ij
{·}
P :=max P . Throughoutthispaper,weusec,c ,c , todenoteuniversalconstantsthatdonot
k k1 i j| ij | 0 1 ···
depend either on the parameters of the MDP or the target levels (ε,δ), and their exact values may change
P
from line to line.
2 Models and background
This paper studies an infinite-horizon MDP with discounted rewards, as represented by a quintuple =
M
( , ,P,r,γ). Here, and denoterespectivelythe(finite)statespaceandactionspace,whereasγ (0,1)
S A S A ∈
indicates the discount factor. We use P : ∆( ) to represent the probability transition kernel of
S ×A → S
the MDP, where for each state-action pair (s,a) , P(s s,a) denotes the probability of transiting
′
∈ S ×A |
to state s from state s when action a is executed. The rewardfunction is representedby r : [0,1],
′
S×A→
suchthatr(s,a)denotestheimmediaterewardfromstateswhenactionaistaken;forsimplicity,weassume
throughout that all rewards lie within [0,1]. We focus on the tabular setting which, despite its basic form,
is not yet well understood. See Bertsekas (2017) for an in-depth introduction of this model.
4Q-function and the Bellman operator. An action selection rule is termed a policy and representedby
amapping π : ∆( ), whichmapsa stateto a distributionoverthe setofactions. Apolicy is saidto be
S → A
stationaryifitistime-invariant. Wedenoteby s ,a ,r asampletrajectory,wheres (resp.a )denotes
t t t }∞t=0 t t
{
the state (resp. the action taken), and r = r(s ,a ) denotes the reward received at time t. It is assumed
t t t
throughout that the rewards are deterministic and depend solely upon the current state-action pair. We
denote by Vπ : R the value function of a policy π, namely,
S →
s : Vπ(s):=E ∞ γtr(s ,a ) s =s ,
t t 0
∀ ∈S " #
t=0
X (cid:12)
(cid:12)
which is the expected discounted cumulative reward received when (i) the initial state is s = s, (ii) the
0
actions are taken based on the policy π (namely, a π(s ) for all t 0) and the trajectory is generated
t t
∼ ≥
based on the transition kernel (namely, s P( s ,a )). It can be easily verified that 0 Vπ(s) 1
t+1 ∼ ·| t t ≤ ≤ 1 γ
for any π. The action-value function (also Q-function) Qπ : R of a policy π is defined by −
S×A→
(s,a) : Qπ(s,a):=E ∞ γtr(s ,a ) s =s,a =a ,
t t 0 0
∀ ∈S×A " #
t=0
X (cid:12)
(cid:12)
where the actions are taken according to the policy π except the initial action (i.e. a π(s ) for all t 1).
t t
∼ ≥
As is well-known, there exists an optimal policy — denoted by π⋆ — that simultaneously maximizes Vπ(s)
and Qπ(s,a) uniformly overallstate-actionpairs(s,a) ( ). Here and throughout,we shall denote by
V⋆ :=Vπ⋆ and Q⋆ :=Qπ⋆ the optimal value function a∈ ndS th× eA optimal Q-function, respectively.
In addition, the Bellmanoperator , which is a mapping from R to itself, is defined suchthat the
|S|×|A|
T
(s,a)-th entry of (Q) is given by
T
(Q)(s,a):=r(s,a)+γE maxQ(s′,a′) .
s′ P( s,a)
T ∼ ·| a′
h ∈A i
It is well known that the optimal Q-function Q⋆ is the unique fixed point of the Bellman operator.
Sample trajectory and behavior policy. Imagine we have access to a sample trajectory s ,a ,r
t t t }∞t=0
{
generatedbytheMDP underagivenstationarypolicyπ —calledabehavior policy. Thebehaviorpolicy
b
M
is deployed to help one learn the “behavior” of the MDP under consideration, which often differs from the
optimal policy being sought. Given the stationarity of π , the sample trajectory can be viewed as a sample
b
path of a time-homogeneous Markov chain over all state-action pairs. Throughout this paper, we impose
the following assumption regarding uniform ergodicity (Paulin, 2015).
Assumption 1. The Markov chain induced by the stationary behavior policy π is uniformly ergodic.
b
There are several properties concerning the behavior policy and its resulting Markov chain that play a
crucial role in learning the optimal Q-function. Specifically, denote by µ the stationary distribution (over
πb
all state-action pairs) of the aforementioned behavior Markov chain, and define
µ := min µ (s,a). (4)
min πb
(s,a)
∈S×A
Intuitively, µ reflects an information bottleneck — the smaller µ is, the more samples are needed in
min min
ordertoensureallstate-actionpairsarevisitedsufficientlymanytimes. Inaddition,wedefinetheassociated
mixing time of the chain as
1
t :=min t max d Pt( s ,a ),µ , (5)
tm hix (cid:12)n( os0 f,a (s0) coT nV ition· a| l0 on0 iπ nb ti≤ al4
where Pt( s 0,a 0) denotes e distribun tio(cid:12) (cid:12) t∈ ,S a× t)A d(cid:0) the i(cid:1) sto ate-actionpair (s 0,a 0), and
·|
d (µ,ν)standsfor the totalvariationdistance betweentwodistributions µ andν (Paulin,2015). Inwords,
TV
the mixing time t captures how fast the sample trajectory decorrelates from its initial state. Moreover,
mix
we define the cover time associated with this Markov chain as follows
1
t :=min t min P s ,a , (6)
cover t 0 0
|(s0,a0) (cid:0)B | (cid:1)≥ 2
n ∈S×A o
5where denotes the event such that all (s,a) have been visited at least once between time 0 and
t
time t,B and P s ,a denotes the probability∈ oS f ×A conditional on the initial state (s ,a ).
t 0 0 t 0 0
B | B
(cid:0) (cid:1)
Goal. Given a single sample trajectory s ,a ,r generated by the behavior policy π , we aim to
t t t }∞t=0 b
{
compute/approximatetheoptimalQ-functionQ⋆ inanℓ sense. Thissetting—inwhichastate-actionpair
∞
canbeupdatedonlywhentheMarkoviantrajectoryreachesit—iscommonlyreferredtoasasynchronousQ-
learning(Qu and Wierman,2020;Tsitsiklis,1994)intabularRL.Thecurrentpaperfocusesoncharacterizing,
in a non-asymptotic manner, the sample efficiency of classical Q-learning and its variance-reduced variant.
3 Asynchronous Q-learning on a single trajectory
3.1 Algorithm
The Q-learning algorithm (Watkins and Dayan, 1992) is arguably one of the most famous off-policy algo-
rithms aimed at learning the optimal Q-function. Given the Markovian trajectory s ,a ,r gener-
t t t }∞t=0
{
ated by the behavior policy π , the asynchronous Q-learning algorithm maintains a Q-function estimate
b
Q : R at each time t and adopts the following iterative update rule
t
S×A→
Q (s ,a )=(1 η )Q (s ,a )+η (Q )(s ,a )
t t 1 t 1 t t 1 t 1 t 1 t t t 1 t 1 t 1
− − − − − − T − − − (7)
Q (s,a)=Q (s,a), (s,a)=(s ,a )
t t 1 t 1 t 1
− ∀ 6 − −
for any t 0, whereas η denotes the learning rate or the step size. Here denotes the empirical Bellman
t t
≥ T
operator w.r.t. the t-th sample, that is,
t(Q)(s 1,a 1):=r(s 1,a 1)+γmaxQ(s t,a′). (8)
t t t t
T − − − − a′
∈A
It is worth emphasizing that at each time t, only a single entry — the one corresponding to the sampled
state-action pair (s ,a ) — is updated, with all remaining entries unaltered. While the estimate Q
t 1 t 1 0
− −
can be initialized to arbitrary values, we shall set Q (s,a) = 0 for all (s,a) unless otherwise noted. The
0
corresponding value function estimate V : R at time t is thus given by
t
S →
s : V (s):=maxQ (s,a). (9)
t t
∀ ∈S a
∈A
The complete algorithm is described in Algorithm 1.
Algorithm 1: Asynchronous Q-learning
1 input parameters: learning rates η , number of iterations T.
t
{ }
2 initialization: Q =0.
0
3 for t=1,2, ,T do
···
4 Draw action a π b(s 1) and next state s P( s 1,a 1).
t 1 t t t t
− ∼ − ∼ ·| − −
5 Update Q according to (7).
t
3.2 Theoretical guarantees for asynchronous Q-learning
We are in a position to present our main theory regarding the non-asymptotic sample complexity of asyn-
chronous Q-learning, for which the key parameters µ and t defined respectively in (4) and (5) play a
min mix
vital role. The proof of this result is provided in Section 6.
Theorem1(AsynchronousQ-learning). FortheasynchronousQ-learningalgorithmdetailedinAlgorithm1,
there exist some universal constants c ,c >0 such that for any 0<δ <1 and 0<ε 1 , one has
0 1 ≤ 1 γ
−
(s,a) : Q (s,a) Q⋆(s,a) ε
T
∀ ∈S×A | − |≤
6with probability at least 1 δ, provided that the iteration number T and the learning rates η η obey
t
− ≡
c 1 t T 1
0 mix
T + log |S||A| log , (10a)
≥ µ (1 γ)5ε2 1 γ δ (1 γ)2ε
min (cid:26) − − (cid:27) (cid:16) (cid:17) (cid:16) − (cid:17)
c (1 γ)4ε2 1
1
η = min − , . (10b)
log |S|| δA|T (cid:26) γ2 t mix(cid:27)
Theorem 1 delivers a finite(cid:0)-sample(cid:1)/finite-time analysis of asynchronous Q-learning, given that a fixed
learningrateisadoptedandchosenappropriately. Theℓ -basedsamplecomplexityrequiredforAlgorithm1
∞
to attain ε accuracy is at most
1 t
mix
O + . (11)
µ (1 γ)5ε2 µ (1 γ)
min min
(cid:16) − − (cid:17)
A few implications are in order. e
Dependency on the minimum state-action occupancy probability µ . Our sample complexity
min
bound (11) scales linearly in 1/µ , which is in general unimprovable. Consider, for instance, the ideal
min
scenariowhere state-actionoccupancy is nearly uniformacrossallstate-actionpairs,in whichcase 1/µ is
min
on the order of . In such a “near-uniform” case, the sample complexity scales linearly with , and
|S||A| |S||A|
this dependency matches the known minimax lower bound Azar et al. (2013) derived for the setting with
independent samples. In comparison, Qu and Wierman (2020, Theorem 7) depends at least quadratically
on 1/µ , which is at least times larger than our result (11).
min
|S||A|
Dependencyonthediscountcomplexity 1 . Thesamplesizebound(11)scalesas 1 ,whichco-
1 γ (1 γ)5ε2
− −
incideswithbothChen et al.(2020);Wainwright(2019a)(forthesynchronoussetting)andBeck and Srikant
(2012); Qu and Wierman (2020) (for the asynchronoussetting) with either a rescaledlinear learning rate or
a constant learning rate. This turns out to be the sharpest scaling known to date for the classical form of
Q-learning.
Dependency onthe mixingtime t . Thesecondadditivetermofoursamplecomplexity(11)depends
mix
linearly on the mixing time t and is (almost) independent of the target accuracy ε. The influence of this
mix
mixingtermisaconsequenceoftheexpensetakenfortheMarkoviantrajectorytoreachasteadystate,which
isaone-timecostthatcanbe amortizedoverlateriterationsifthe algorithmisrunforreasonablylong. Put
another way,if the behavior chainmixes not too slowlywith respectto ε (in the sense that t 1 ),
mix ≤ (1 γ)4ε2
then the algorithm behaves as if the samples were independently drawn from the stationary distribu−tion of
thetrajectory. Incomparison,theinfluencesoft and 1 inQu and Wierman(2020)(cf.Table1)are
mix (1 γ)5ε2
multiplicative regardless of the value of ε, thus resulting i−n a much higher sample complexity. For instance,
ifε=O (1 −γ)1 2√tmix ,thenthesamplecomplexityresultthereinisatleast µtm mi ix n ≥t mix |S||A|timeslargerthan
our result (modulo some log factor).
(cid:0) (cid:1)
Schedule of learning rates. Aninterestingaspectofouranalysisliesinthe adoptionofatime-invariant
learning rate, under which the ℓ error decays linearly — down to some error floor whose value is dic-
∞
tated by the learning rate. Therefore, a desired statistical accuracy can be achieved by properly setting
the learning rate based on the target accuracy level ε and then determining the sample complexity accord-
ingly. In comparison, classical analyses typically adopted a (rescaled) linear or a polynomial learning rule
Even-Dar and Mansour (2003); Qu and Wierman (2020). While the work Beck and Srikant (2012) studied
Q-learning with a constant learning rate, their bounds were conservative and fell short of revealing the op-
timal scaling. Furthermore, we note that adopting time-invariant learning rates is not the only option that
enables the advertised sample complexity; as we shall elucidate in Section 3.4, one can also adopt carefully
designed diminishing learning rates to achieve the same performance guarantees.
Inaddition,ouranalysisframeworkimmediately leads to anothersamplecomplexity guaranteestatedin
termsofthecovertimet (cf.(6)),whichfacilitatescomparisonswithseveralpastworkBeck and Srikant
cover
7(2012); Even-Dar and Mansour (2003). The proof follows essentially that of Theorem 1, with a sketch
provided in Section B.
Theorem 2. For the asynchronous Q-learning algorithm detailed in Algorithm 1, there exist some universal
constants c ,c >0 such that for any 0<δ <1 and 0<ε 1 , one has
0 1 ≤ 1 γ
−
(s,a) : Q (s,a) Q⋆(s,a) ε
T
∀ ∈S×A | − |≤
with probability at least 1 δ, provided that the iteration number T and the learning rates η η obey
t
− ≡
c t T 1
T 0 cover log2 |S||A| log , (12a)
≥ (1 γ)5ε2 δ (1 γ)2ε
− (cid:16) (cid:17) (cid:16) − (cid:17)
c (1 γ)4ε2
1
η = min − , 1 . (12b)
log |S|| δA|T (cid:26) γ2 (cid:27)
In a nutshell, this theorem tells us(cid:0)that th(cid:1)e ℓ -based sample complexity of classical asynchronous Q-
∞
learning is bounded above by
t
cover
O , (13)
(1 γ)5ε2
(cid:16) − (cid:17)
whichscaleslinearlywiththecovertime. ThiseimprovesuponthepriorresultEven-Dar and Mansour(2003)
(resp. Beck and Srikant (2012)) by an order of at least t3.29 3.29 3.29 (resp. t2 3 3).
cover ≥ |S| |A| cover|S||A| ≥ |S| |A|
See Table 1 for detailed comparisons. We shall further make note of some connections between t and
cover
t /µ to help compare Theorem 1 and Theorem 2: (1) in general, t = O(t /µ ) for uniformly
mix min cover mix min
ergodicchains;(2)onecanfindsomecaseswheret /µ =O(t ). Consequently,while Theorem1does
mix min cover
not strictly dominate Theorem 2 in all instances, the aforementioned connectionsereveal that Theorem 1 is
tighter for the worst-case scenarios. The interested reader is reeferred to Section A.2 for details.
3.3 A special case: TD learning
Inthespecialcircumstancethatthesetofallowableactions isasingleton,thecorrespondingMDPreduces
A
toaMarkovrewardprocess(MRP),wherethestatetransitionkernelP : ∆( )describestheprobability
S → S
of transitioning between different states, and r : [0,1] denotes the reward function (so that r(s) is the
immediate reward in state s). The goal is to estS im→ ate the value function V : R from the trajectory
S →
s ,r , which arises commonly in the task of policy evaluation for a given deterministic policy.
t t }∞t=0
{
TheQ-learningprocedureinthis specialsettingreducestothe well-knownTDlearningalgorithm,which
maintains an estimate V : R at each time t and proceeds according to the following iterative update2
t
S →
V (s )=(1 η )V (s )+η (r(s )+γV (s )),
t t 1 t t 1 t 1 t t 1 t 1 t
− − − − − − (14)
V (s)=V (s), s=s .
t t 1 t 1
− ∀ 6 −
As usual, η denotes the learning rate at time t, and V is taken to be 0. Consequently, our analysis for
t 0
asynchronous Q-learning with a Markovian trajectory immediately leads to non-asymptotic ℓ guarantees
∞
for TD learning, stated below as a corollary of Theorem 1. A similar result can be stated in terms of the
cover time as a corollary to Theorem 2, which we omit for brevity.
Corollary 1 (Asynchronous TD learning). Consider the TD learning algorithm (14). There exist some
universal constants c ,c >0 such that for any 0<δ <1 and 0<ε 1 , one has
0 1 ≤ 1 γ
−
s : V (s) V(s) ε
T
∀ ∈S | − |≤
with probability at least 1 δ, provided that the iteration number T and the learning rates η η obey
t
− ≡
c 1 t T 1
0 mix
T + log |S| log , (15a)
≥ µ (1 γ)5ε2 1 γ δ (1 γ)2ε
min (cid:26) − − (cid:27) (cid:16) (cid:17) (cid:16) − (cid:17)
c (1 γ)4ε2 1
1
η = min − , . (15b)
log |S δ|T (cid:26) γ2 t mix(cid:27)
2WhenA={a}isasingleton,theQ-learningupdaterule(7)reducestotheTDupdaterule(14)byrelatingQ(s,a)=V(s).
(cid:0) (cid:1)
8The above result reveals that the ℓ -sample complexity for TD learning is at most
∞
1 t
mix
O + , (16)
µ (1 γ)5ε2 µ (1 γ)
min min
(cid:16) − − (cid:17)
providedthatanappropriateconstanetlearningrateisadopted. We notethatpriorfinite-sampleanalysison
asynchronousTD learning typically focused on (weighted) ℓ estimation errors with linear function approx-
2
imation (Bhandari et al., 2018; Srikant and Ying, 2019), and it is hence difficult to make fair comparisons.
The recent papers Khamaru et al. (2020); Mou et al. (2020) develop ℓ guarantees for TD learning, with
∞
their focus on the synchronous settings with i.i.d. samples rather than Markoviansamples.
3.4 Adaptive and implementable learning rates
Thecarefulreadermightalreadynoticethatthe learningratesrecommendedin(10b)dependonthemixing
time t — a parameter that might be either a priori unknown or difficult to estimate. Fortunately, it is
mix
feasible to adopt a more adaptive learning rate schedule which does not rely on prior knowledge of t and
mix
which is still capable of achieving the performance advertised in Theorem 1.
Learning rates. Inordertodescribeournewlearningrateschedule,weneedtokeeptrackofthefollowing
quantities for all (s,a) :
∈S×A
K (s,a): the number of times that the sample trajectory visits (s,a) during the first t iterations.
t
•
In addition, we maintain an estimate µ of µ , computed recursively as follows
min,t min
1 , min K (s,a)=0;
b s,a t
µ min,t =  µ|S m|| inA ,t| −1, 1 2 < mins µb,a mK in,tt −(s 1,a)/t <2; (17)
 min s,aK t(s,a)/t, otherwise.
b b
With the above quantities in place, we propose the following learning rate schedule:
logt
η =min 1,c exp log , (18)
t η µ (1 γ)γ2t
min,t
n (cid:16)j − k(cid:17)o
where c > 0 is some sufficiently large constant, and x denotes the nearest integer less than or equal
η ⌊b⌋
to x. If µ forms a reliable estimate of µ , then one can view (18) as a sort of “piecewise constant
min,t min
approximation” ofthe rescaledlinear stepsizes cηlogt . Clearly,suchlearningratesarefully data-driven
anddo nobrely on anyprior knowledgeabout thµ emin M(1 a− rγ k) oγ v2t chain(like t andµ ) or the targetaccuracyε.
mix min
Performance guarantees. Encouragingly, our theoretical framework can be extended without difficulty
to accommodate this adaptive learning rate choice. Specifically, for the Q-function estimates
Q , if η =η ,
t t+1 t
Q = 6 (19)
t Q , otherwise,
(cid:26) t −1
b
where Q is provided by the Q-learning steps b(c.f. (7)). We then are ensured of the following theoretical
t
guarantees whose proof is deferred to Appendix C.
Theorem 3. Consider asynchronous Q-learning with learning rates (18). There exists some universal
constant C >0 such that: for any 0<δ <1 and 0<ε 1 , one has
≤ 1 γ
−
(s,a) : Q (s,a) Q⋆(s,a) ε (20)
T
∀ ∈S×A − ≤
with probability at least 1 δ, provided that (cid:12) (cid:12)
(cid:12)b (cid:12)
−
1 t T T
mix
T Cmax , log |S||A| log . (21)
≥ µ (1 γ)5ε2 µ (1 γ) δ (1 γ)2ε
min min
n − − o (cid:16) (cid:17) (cid:16) − (cid:17)
94 Extension: asynchronous variance-reduced Q-learning
As pointed out in prior literature, the classical form of Q-learning (7) often suffers from sub-optimal de-
pendence on the discount complexity 1 . For instance, in the synchronous setting, the minimax lower
1 γ
bound is proportional to 1 (see, Az− ar et al. (2013)), while the sharpest known upper bound for vanilla
(1 γ)3
Q-learning scales as 1 − ; see detailed discussions in Wainwright (2019a). To remedy this issue, recent
(1 γ)5
work proposed to lever−age the idea of variance reduction to develop accelerated RL algorithms in the syn-
chronous setting (Sidford et al., 2018a; Wainwright, 2019b), as inspired by the seminal SVRG algorithm
(Johnson and Zhang, 2013) that originates from the stochastic optimization literature. In this section, we
adapt this idea to asynchronous Q-learning and characterize its sample efficiency.
4.1 Algorithm
In order to accelerate the convergence, it is instrumental to reduce the variability of the empirical Bellman
operator employed in the update rule (7) of classical Q-learning. This can be achieved via the following
t
T
means. Simply put, assuming we have access to (i) a reference Q-function estimate, denoted by Q, and (ii)
an estimate of (Q), denoted by (Q), the variance-reducedQ-learning update rule is given by
T T
Q (s ,a )=(1 ηe)Q (s ,a )+η (Q ) (Q)+ (Q) (s ,a ),
t t 1 t 1 t t 1 t 1 t 1 t t t 1 t t 1 t 1
− − − − − − T − −T T − − (22)
Q (s,a)=Q (s,a), (s,a)=(s (cid:16),a ), (cid:17)
t t −1 ∀ 6 t −1 t −1 e
where denotestheempiricalBellmanoperatorattimet(cf.(8)). Theempiricalestimate (Q)canbecom-
t
T T
putedusingasetofsamples;morespecifically,bydrawingN consecutivesampletransitions (s ,a ,s )
i i i+1 0 i<N
{ } ≤
from the observed trajectory, we compute e
γ N i=− 011 {(s i,a i)=(s,a) }max a′Q(s i+1,a ′)
(Q)(s,a)=r(s,a)+ . (23)
T P N i=− 011 {(s i,a i)=(s,a)
}
e
Compared with the classical form (7), the original uPpdate term (Q ) has been replaced by (Q )
t t 1 t t 1
T − T − −
(Q)+ (Q), in the hope of achieving reduced variance as long as Q (which serves as a proxy to Q⋆) is
t
T T
chosen properly.
For coenvenience of presentation, we introduce the following notation
Q=Vr-q-run-epoch(Q,N,t ) (24)
epoch
to represent the above-mentioned update rule, which starts with a reference point Q and operates upon a
total number of N +t consecutive sample transitions. The first N samples are employed to construct
epoch
(Q) via (23), with the remaining samples employed in t iterative updates (22); see Algorithm 3. To
epoch
T
achieve the desired acceleration, the proxy Q needs to be periodically updated so as to better approximate
tehe truth Q⋆ and hence reduce the bias. It is thus natural to run the algorithm in a multi-epoch manner.
Specifically, we divide the samples into contiguous subsets called epochs, each containing t iterations
epoch
and using N +t samples. We then proceed as follows
epoch
Qepoch =Vr-q-run-epoch(Qepoch,N,t ), m=1,...,M, (25)
m m 1 epoch
−
where M is the total number of epochs, and Qepoch denotes the output of the m-th epoch. The whole
m
procedure is summarized in Algorithm 2. Clearly, the total number of samples used in this algorithm is
given by M(N +t ). We remark that the idea of performing variance reduction in RL is certainly not
epoch
new,andhasbeenexploredinanumberofrecentworkDu et al.(2017);Khamaru et al.(2020);Sidford et al.
(2018a,b); Wainwright (2019b); Xu et al. (2020).
10Algorithm 2: Asynchronous variance-reduced Q-learning
1 input parameters: number of epochs M, epoch length t epoch, recentering length N, learning rate η.
2 initialization: set Qepoch 0.
0 ←
3 for each epoch m=1, ,M do
···
/* Call Algorithm 3. */
4 Qe mpoch = Vr-q-run-epoch(Qe mpoc 1h,N,t epoch) .
−
4.2 Theoretical guarantees for variance-reduced Q-learning
This subsection develops a non-asymptotic sample complexity bound for asynchronous variance-reducedQ-
learning on a single trajectory. Before presenting our theoretical guarantees, there are several algorithmic
parameters that we shall specify; for given target levels (ε,δ), choose
c (1 γ)2 1
0
η η = min − , , (26a)
t ≡ log |S||A δ|tepoch (cid:26) γ2 t mix(cid:27)
c 1 t
N ≥ µ 1(cid:0) (1 γ)3(cid:1) min 1,ε2 +t mix log |S||A δ| epoch , (26b)
min
(cid:16) − { } (cid:17) (cid:16) (cid:17)
c 1 t 1 t
2 mix epoch
t epoch ≥ µ (1 γ)3 + 1 γ log (1 γ)2ε log |S||A δ| , (26c)
min
(cid:16) − − (cid:17) (cid:16) − (cid:17) (cid:16) (cid:17)
where c > 0 is some sufficiently small constant, c ,c > 0 are some sufficiently large constants, and we
0 1 2
recall the definitions of µ and t in (4) and (5), respectively. Note that the learning rate (26a) chosen
min mix
here could be larger than the choice (10b) for the classical form by a factor of O 1 (which happens if
(1 γ)2
−
t is not too large), allowing the algorithm to progress more aggressively.
mix (cid:0) (cid:1)
Theorem 4 (Asynchronous variance-reduced Q-learning). Let Qepoch be the output of Algorithm 2 with
M
parameters chosen according to (26). There exists some constant c > 0 such that for any 0 < δ < 1 and
3
0<ε 1 , one has
≤ 1 γ
−
(s,a) : Qepoch(s,a) Q⋆(s,a) ε
∀ ∈S×A | M − |≤
with probability at least 1 δ, provided that the total number of epochs exceeds
−
1
M c log . (27)
≥ 3 ε(1 γ)2
−
The proof of this result is postponed to Section D.
InviewofTheorem4,theℓ -basedsamplecomplexityforvariance-reducedQ-learningtoyieldεaccuracy
∞
— which is characterizedby M(N +t ) — can be as low as
epoch
1 t
mix
O + . (28)
µ (1 γ)3min 1,ε2 µ (1 γ)
min min
(cid:16) − { } − (cid:17)
e
Except for the second term that depends on the mixing time, the first term matches Wainwright (2019b)
derived for the synchronous settings with independent samples. In the range ε (0,min 1, 1 ], the
∈ { (1 −γ)√tmix}
sample complexity reduce to O 1 ; the scaling 1 matches the minimax lower bound derived
µmin(1 −γ)3ε2 (1 −γ)3
in Azar et al. (2013) for the synchronous setting.
(cid:0) (cid:1)
Once again, we can immedeiately deduce guarantees for asynchronous variance-reduced TD learning by
reducing the action space to a singleton (similar to Section 3.3), which extends the analysis Khamaru et al.
(2020)toMarkoviannoise. We donotelaborateonthishereasitisnotthe mainfocusofthecurrentpaper.
11Algorithm 3: function Q=Vr-q-run-epoch(Q,N,t )
epoch
1 Draw N new consecutive samples from the sample trajectory; compute (Q) according to (23).
T
2 Set s current state, and Q Q.
0 0
← ←
3 for t=1,2, ,t epoch do e
···
4 Draw action a π b(s 1) and next state s P( s 1,a 1).
t 1 t t t t
− ∼ − ∼ ·| − −
5 Update Q according to (22).
t
6 return: Q Q .
tepoch
←
5 Related work
The Q-learning algorithm and its variants. TheQ-learningalgorithm,originallyproposedinWatkins
(1989), has been analyzed in the asymptotic regime by Borkar and Meyn (2000); Jaakkola et al. (1994);
Szepesvári (1998); Tsitsiklis (1994) since more than two decades ago. Additionally, finite-time perfor-
mance of Q-learning and its variants have been analyzed by Beck and Srikant (2012); Chen et al. (2020);
Even-Dar and Mansour (2003); Kearns and Singh (1999); Qu and Wierman (2020); Wainwright (2019a) in
the tabular setting, by Bhandari et al. (2018); Cai et al. (2019); Chen et al. (2019); Du et al. (2020, 2019);
Fan et al.(2019);Weng et al.(2020a,b);Xu and Gu(2020);Yang and Wang(2019)inthecontextoffunction
approximations,andbyShah and Xie(2018)withnonparametricregression. Inaddition,Azar et al.(2011);
Devraj and Meyn(2020);Ghavamzadeh et al.(2011);Sidford et al.(2018a);Strehl et al.(2006);Wainwright
(2019b) studied modified Q-learningalgorithmsthat mightpotentially improvesample complexities andac-
celerateconvergence. Another line ofworkstudiedQ-learningwith sophisticatedexplorationstrategiessuch
as UCB exploration(e.g. Bai et al. (2019); Jin et al. (2018); Wang et al. (2020)), which is beyond the scope
of the current work.
Finite-sampleℓ guarantees for Q-learning. Wenowexpandonnon-asymptoticℓ guaranteesavail-
∞ ∞
able in priorliterature,whichare the mostrelevantto the currentwork. An interesting aspectthat we shall
highlight is the importance of learning rates. For instance, when a linear learning rate (i.e. η = 1/t)
t
is adopted, the sample complexity results derived in past work Even-Dar and Mansour (2003); Szepesvári
(1998) exhibit an exponential blow-up in 1 , which is clearly undesirable. In the synchronous setting,
1 γ
Beck and Srikant(2012);Chen et al.(2020);−Even-Dar and Mansour(2003);Wainwright(2019a)studiedthe
finite-sample complexity of Q-learning under various learning rate rules; the best sample complexity known
to date is O (1|S γ|| )A5| ε2 , achieved via either a rescaled linear learning rate (Chen et al., 2020; Wainwright,
−
2019a) or a constant learning rate (Chen et al., 2020). When it comes to asynchronous Q-learning (in its
(cid:0) (cid:1)
classical forem), our work provides the first analysis that achieves linear scaling with 1/µ or t ; see Ta-
min cover
ble1fordetailedcomparisons. GoingbeyondclassicalQ-learning,thespeedyQ-learningalgorithmprovably
achieves a sample complexity of O tcover (Azar et al., 2011) in the asynchronous setting, whose update
(1 γ)4ε2
rule takes twice the storage of classi−cal Q-learning. In comparison, our analysis of the variance-reduced
(cid:0) (cid:1)
Q-learning algorithm achieves a seample complexity of O 1 + tmix when ε<1.
µmin(1 −γ)3ε2 µmin(1 −γ)
(cid:0) (cid:1)
Finite-sample guarantees for model-free algorithems. Convergence properties of several model-free
RLalgorithmshavebeenstudiedrecentlyinthepresenceofMarkoviandata,includingbutnotlimitedtoTD
learning and its variants (Bhandari et al., 2018; Dalal et al., 2018a,b; Doan et al., 2019; Gupta et al., 2019;
Kaledin et al., 2020; Lee and He, 2019; Lin et al., 2020; Srikant and Ying, 2019; Xu et al., 2020, 2019), Q-
learning(Chen et al.,2019;Xu and Gu,2020),andSARSA(Zou et al.,2019). However,theserecentpapers
typically focused on the (weighted) ℓ error rather than the ℓ risk, where the latter is often more relevant
2
∞
in the context of RL. In addition, Khamaru et al. (2020); Mou et al. (2020) investigated the ℓ bounds of
∞
(variance-reduced) TD learning, although they did not account for Markoviannoise.
Finite-sample guarantees for model-based algorithms. Another contrasting approach for learning
the optimal Q-function is the class of model-based algorithms, which has been shown to enjoy minimax-
optimal sample complexity in the synchronous setting. More precisely, it is known that by planning over
12an empirical MDP constructed from O (1|S γ|| )A3| ε2 samples, we are guaranteed to find not only an ε-optimal
−
Q-function but also an ε-optimal policy (Agarwal et al., 2019; Azar et al., 2013; Li et al., 2020). It is worth
(cid:0) (cid:1)
emphasizing that the minimax optimeality of model-based approach has been shown to hold for the entire
ε-range;in comparison,the sample optimality of the model-free approachhas only been shownfor a smaller
rangeofaccuracylevelεinthesynchronoussetting. Wealsoremarkthatexistingsamplecomplexityanalysis
for model-based approaches might be generalizable to Markoviandata.
6 Analysis of asynchronous Q-learning
This section is devoted to establishing Theorem 1. Before proceeding, we find it convenient to introduce
some matrix notation. Let Λ R be a diagonal matrix obeying
t |S||A|×|S||A|
∈
η, if (s,a)=(s ,a ),
Λ (s,a),(s,a) := t −1 t −1 (29)
t
(0, otherwise,
(cid:0) (cid:1)
where η > 0 is the learning rate. In addition, we use the vector Q R (resp. V R ) to represent
t |S||A| t |S|
∈ ∈
our estimate Q (resp. V ) in the t-th iteration, so that the (s,a)-th (resp. sth) entry of Q (resp. V ) is
t t t t
given by Q (s,a) (resp. V (s)). Similarly, let the vectors Q⋆ R and V⋆ R represent the optimal
t t |S||A| |S|
Q-function Q⋆ and the optimal value function V⋆, respective∈ ly. We also let the∈ vector r R stand for
|S||A|
∈
the reward function r, so that the (s,a)-th entry of r is given by r(s,a). In addition, we define the matrix
P 0,1 such that
t |S||A|×|S|
∈{ }
1, if (s,a,s)=(s ,a ,s ),
P (s,a),s := ′ t −1 t −1 t (30)
t ′
(0, otherwise.
(cid:0) (cid:1)
Clearly,this set ofnotation allowsus to express the Q-learningupdate rule (7) in the following matrix form
Q = I Λ Q +Λ r+γP V . (31)
t t t 1 t t t 1
− − −
(cid:0) (cid:1) (cid:0) (cid:1)
6.1 Error decay under constant learning rates
The main step of the analysis is to establish the following result concerning the dynamics of asynchronous
Q-learning. In order to state it formally, we find it convenient to introduce severalauxiliary quantities
443t 4 T
mix
t := log |S||A| , (32a)
frame
µ δ
min
(cid:16) (cid:17)
2log 1
(1 γ)2ε
t th :=max − , t frame , (32b)
ηµ
( min )
1
µ := µ t , (32c)
frame min frame
2
ρ:=(1 γ) 1 (1 η)µframe . (32d)
− − −
With these quantities in mind, we have the followin(cid:0)g result. (cid:1)
Theorem5. ConsidertheasynchronousQ-learningalgorithm inAlgorithm1withη η. Foranyδ (0,1)
t
≡ ∈
and any ε (0, 1 ], there exists a universal constant c > 0 such that with probability at least 1 6δ, the
∈ 1 γ −
following relation−holds uniformly for all t T (defined in (10a))
≤
Q Q⋆ cγ T
Q Q⋆ (1 ρ)kk 0 − k∞ + V⋆ ηlog |S||A| +ε, (33)
t
k − k∞ ≤ − 1 γ 1 γk k∞r δ
− − (cid:16) (cid:17)
provided that 0<ηlog |S|| δA|T <1. Here, we define k :=max 0, t t− frat mt eh .
Inwords,Theorem(cid:0)5 asserts(cid:1)that the ℓ estimation errorde(cid:8)cays(cid:4)linear(cid:5)l(cid:9)y —in a blockwise manner —to
∞
some error floor that scales with √η. This result suggests how to set the learning rate based on the target
accuracy level, which in turn allows us to pin down the sample complexity under consideration. In what
follows, we shall first establish Theorem 5, and then return to prove Theorem 1 using this result.
136.2 Proof of Theorem 5
6.2.1 Key decomposition and a recursive formula
The starting point of our proof is the following elementary decomposition
∆ :=Q Q⋆ = I Λ Q +Λ r+γP V Q⋆
t t t t 1 t t t 1
− − − − −
=(cid:0)I Λ Q Q (cid:0)⋆ +Λ r+γ(cid:1)P tV Q⋆
t(cid:1) t 1 t t 1
− − − − −
=(cid:0)I Λ t(cid:1)(cid:0)Q Q⋆ (cid:1)+γΛ P tV PV⋆
t 1 (cid:0)t t 1 (cid:1)
− − − − −
=(cid:0)I Λ t(cid:1)(cid:0)∆ t 1+γΛ t(cid:1)P t P (cid:0)V⋆+γΛ tP t V (cid:1)t 1 V⋆ (34)
− − − − −
for any t > 0, where the first line r(cid:0)esults fr(cid:1)om the upda(cid:0)te rule ((cid:1)31), and the (cid:0)penultimate(cid:1)line follows from
the Bellman equation Q⋆ =r+γPV⋆ (see Bertsekas (2017)). Applying this relation recursively gives
t t t t t
∆ =γ I Λ Λ P P V⋆+γ I Λ Λ P V V⋆ + I Λ ∆ . (35)
t j i i j i i i 1 j 0
− − − − − −
i=1j=i+1 i=1j=i+1 j=1
X Y (cid:0) (cid:1) (cid:0) (cid:1) X Y (cid:0) (cid:1) (cid:0) (cid:1) Y(cid:0) (cid:1)
=:β1,t =:β2,t =:β3,t
Applyin|g the triangle in{ezquality, we obtai}n | {z } | {z }
∆ β + β + β , (36)
t 1,t 2,t 3,t
| |≤| | | | | |
where we recallthe notation z :=[z ] for any vectorz =[z ] . In whatfollows, we shalllook at
i 1 i n i 1 i n
| | | | ≤≤ ≤≤
these terms separately.
First of all, given that I Λ and Λ are both non-negative diagonal matrices and that
j j
• −
P V V⋆ P V V⋆ = V V⋆ Q Q⋆ = ∆ ,
i i 1 i 1 i 1 i 1 i 1 i 1
− − ∞ ≤k k k − − k∞ k − − k∞ ≤k − − k∞ k − k∞
we ca(cid:13)n e(cid:0)asily see tha(cid:1)t(cid:13)
(cid:13) (cid:13)
t t
β γ ∆ I Λ Λ 1. (37)
2,t i 1 j i
≤ k − k∞ −
i=1 j=i+1
(cid:12) (cid:12) X Y (cid:0) (cid:1)
(cid:12) (cid:12)
Next,thetermβ canbecontrolledbyexploitingsomesortofstatisticalindependenceacrossdifferent
1,t
•
transitions and applying the Bernstein inequality. This is summarized in the following lemma, with
the proof deferred to Section E.1.
Lemma 1. Consider any fixed vector V⋆ R . There exists some universal constant c > 0 such
|S|
∈
that for any 0<δ <1, one has
t t
1 t T : γ I Λ Λ P P V⋆ τ V⋆ 1 (38)
j i i 1
∀ ≤ ≤ (cid:12) − − (cid:12)≤ k k∞
(cid:12) Xi=1j= Yi+1 (cid:12)
(cid:12) (cid:0) (cid:1) (cid:0) (cid:1) (cid:12)
with probability at least 1 δ, pro(cid:12) (cid:12)vided that 0<ηlog |S||A|T <1. H(cid:12) (cid:12)ere, we define
− δ
(cid:0) (cid:1)
T
τ :=cγ ηlog |S||A| . (39)
1
δ
r
(cid:16) (cid:17)
Additionally,wedevelopanupperboundonthetermβ ,whichfollowsdirectlyfromtheconcentration
3,t
•
ofthe empiricaldistribution ofthe Markovchain(see Lemma 5). The proofisdeferredto Section E.2.
Lemma 2. For any δ > 0, recall the definition of t in (32a). Suppose that T > t and
frame frame
0<η <1. Then with probability exceeding 1 δ one has
−
t
I Λ ∆ (1 η)21tµmin ∆ (1 η)1 2tµmin ∆ 1 (40)
j 0 0 0
(cid:12) − (cid:12)≤ − ≤ − k k∞
(cid:12) (cid:12)j Y=1 (cid:0) (cid:1) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)
uniformly over all t(cid:12) (cid:12)obeying T t (cid:12) (cid:12)t and all vector ∆ R |S||A|.
frame 0
≥ ≥ ∈
14Moreover,in the case where t<t , we make note of the straightforwardbound
frame
t
I Λ ∆ ∆ 1, (41)
j 0 0
(cid:12) − (cid:12)≤k k∞
(cid:12) (cid:12)j Y=1 (cid:12)
(cid:0) (cid:1) (cid:12)
given that I Λ is a diagonal non-(cid:12) (cid:12)negative matrix(cid:12) (cid:12)whose entries are bounded by 1 η <1.
j
− −
Substituting the preceding bounds into (36), we arrive at
γ t ∆ t (I Λ )Λ 1+τ V⋆ 1+ ∆ 1, t<t
|∆ t |≤(γ Pi t i= =1 (cid:13)∆i i− −1 (cid:13)∞ ∞Qj t j= =i i+ +1 1(I − −Λj j)Λi i1+τ1 1k kV⋆k k∞ ∞1+ (cid:13)(1 −0 (cid:13)η∞ )21tµmin ∆ ∞1, t framefr ≤ame t ≤T (42)
1(cid:13) 1(cid:13) (cid:13) (cid:13) 0
with probabPility a(cid:13) (cid:13)t least(cid:13) (cid:13)1 Q2δ, where t is defined in (32a). The rest of(cid:13) (cid:13)the(cid:13) (cid:13)proof is thus dedicated to
frame
bounding ∆ based on the− above recursive formula (42).
t
| |
6.2.2 Recursive analysis
A crude bound. We start by observing the following recursive relation
t t
∆ γ ∆ (I Λ )Λ 1+τ V⋆ 1+ ∆ 1, 1 t T, (43)
t i 1 j i 1 0
| |≤ i=1 − ∞j=i+1 − k k∞ k k∞ ≤ ≤
X(cid:13) (cid:13) Y
(cid:13) (cid:13)
which is a direct consequence of (42). In the sequel, we invoke mathematical induction to establish, for all
1 t T, the following crude upper bound
≤ ≤
τ V⋆ + ∆
∆ 1 k k∞ k 0 k∞, (44)
t
≤ 1 γ
∞ −
(cid:13) (cid:13)
which implies the stability of the asyn(cid:13)chro(cid:13)nous Q-learning updates.
Towards this, we first observe that (44) holds trivially for the base case (namely, t = 0). Now suppose
that the inequality (44) holds for all iterations up to t 1. In view of (43) and the induction hypotheses,
−
γ τ V⋆ + ∆ t t
∆ 1 k k∞ 0 (I Λ j)Λ i1+τ V⋆ 1+ ∆ 1, (45)
t ∞ 1 0
| |≤ 1 −γ − k k∞ k k∞
(cid:0) (cid:13) (cid:13) (cid:13) (cid:13) (cid:1) Xi=1j= Yi+1
where we invoke the fact that the vector t (I Λ )Λ 1 is non-negative. Next, define the diagonal
j=i+1 − j i
matrix M := t (I Λ )Λ , and denote by Nj(s,a) the number ofvisits to the state-actionpair (s,a)
i j=i+1 − j i Q i
between the i-th and the j-th iterations (including i and j). Then the diagonal entries of M satisfy
i
Q
η(1 −η)N it +1(s,a), if (s,a)=(s −1,a −1),
M i((s,a),(s,a))= i i
(0, if (s,a)=(s 1,a 1).
i i
6 − −
Letting e R be a standard basis vector whose only nonzero entry is the (s,a)-th entry, we can
(s,a) |S||A|
∈
easily verify that
t
(I −Λ j)Λ i1=M i1=M ie =η(1 −η)N it +1(si−1,ai−1)e (46a)
(si−1,ai−1) (si−1,ai−1)
j=i+1
Y
and
t t t
(I −Λ j)Λ i1= η(1 −η)N it +1(si−1,ai−1)e
(si−1,ai−1)
i=1j=i+1 i=1
X Y X
t
= η(1 η)N it +1(s,a)1 (s 1,a 1)=(s,a) e
i i (s,a)
( − − − )
(s,aX) Xi=1
∈S×A (cid:8) (cid:9)
15∞ η(1 η)je = ∞ η(1 η)j1=1. (46b)
(s,a)
≤ − −
(s,aX) ∈S×AXj=0 Xj=0
Combining the above relations with the inequality (45), one deduces that
γ(τ V⋆ + ∆ ) τ V⋆ + ∆
∆ 1 k k∞ 0 +τ V⋆ + ∆ = 1 k k∞ 0 ∞,
t ∞ 1 0
≤ 1 −γ k k∞ 1 −γ
(cid:13)∞ (cid:13) (cid:13) (cid:13)∞ (cid:13) (cid:13)
(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)
thus establish(cid:13)ing (cid:13)(44) for the t-th iteration. This induction ana(cid:13)lysis(cid:13)thus validates (44) for all 1 t T.
≤ ≤
Refined analysis. Now, we strengthen the bound (44) by means of a recursive argument. To begin with,
it is easily seen that the term (1 η)1 2tµmin ∆ is bounded above by (1 γ)ε for any t > t th, where we
0
remind the reader of the definition− of t ink (32bk )∞ and the fact that ∆ − = Q⋆ 1 . It is assumed
argument th ok au0 xk∞ iliaryk quak n∞ tit≤ ies1 u−γ
that T >t . To facilitate our , we introduce a collection f as follows
th t
∆
0
u = k k∞, (47a)
0
1 γ
−
γ t t (I Λ )Λ 1u + ∆ 1, for 1 t t ,
u t = kv t k∞, v t = (γ Pt ii == 11 Qt jj == ii ++ 11 (I −− Λ jj )Λ ii 1u ii −− 11 , k 0 k∞ for t>≤ t th≤ . th (47b)
These auxiliary quantities are usefuPl as thQey provide upper bounds on ∆ , as asserted by the following
t
k k∞
lemma. The proof is deferred to Section E.3.
Lemma 3. Recall the definition (39) of τ in Lemma 1. With probability at least 1 2δ, the quantities u
1 t
− { }
defined in (47) satisfy
τ V⋆
∆ 1 k k∞ +u t+ε. (48)
t
k k∞ ≤ 1 γ
−
The preceding result motivates us to turn attention to bounding the quantities u . Towards this end,
t
{ }
we resort to a frame-based analysis by dividing the iterations [1,t] into contiguous frames each comprising
t (cf. (32a)) iterations. Further, we define another auxiliary sequence:
frame
∆ Q Q⋆
w :=(1 ρ)kk 0 k∞ =(1 ρ)kk 0 − k∞, (49)
k
− 1 γ − 1 γ
− −
where we remind the reader of the definition of ρ in (32d). The connection between w and u is made
k t
{ } { }
precise as follows, whose proof is postponed to Section E.4.
Lemma 4. For any δ (0,1), with probability at least 1 2δ, one has
∈ 2 −
t t
th
u w , with k =max 0, − . (50)
t k
≤ t
(cid:26) j frame k(cid:27)
Combining Lemmas 3-4, we arrive at
τ V⋆ (1 ρ)k Q Q⋆ τ V⋆
Q Q⋆ = ∆ 1 k k∞ +w k+ε − k 0 − k∞ + 1 k k∞ +ε,
t t
k − k∞ k k∞ ≤ 1 γ ≤ 1 γ 1 γ
− − −
which finishes the proof of Theorem 5.
6.3 Proof of Theorem 1
Now we return to complete the proof of Theorem 1. To control ∆ to the desired level, we first claim
t
k k∞
that the first term of (33) obeys
∆
(1 ρ)kk 0 k∞ ε (51)
− 1 γ ≤
−
16whenever
2 ∆
0
t t th+t frame+ log k k∞ , (52)
≥ (1 γ)ηµ ε(1 γ)
− min (cid:18) − (cid:19)
provided that η <1/µ . Furthermore, by taking the learning rate as
frame
(1 γ)4ε2 1
η =min − , , (53)
(c2γ2log |S||A|T µ frame)
δ
one can easily verify that the second term of (33) satisfies
cγ T
V⋆ ηlog |S||A| ε, (54)
1 −γk k∞r δ (cid:17)≤
(cid:16)
where the last step follows since V⋆ 1 . Putting the above bounds together ensures ∆ 3ε.
proofk ask l∞ ong≤ as1 −tγ k t k∞ ≤
We have thus concluded the , he claim (51) can be justified.
Proof of the inequality (51). Observe that (1 ρ)k k∆ 0k∞ exp( ρk)k∆ 0k∞ ε holds true whenever k
− 1 γ ≤ − 1 γ ≤ ≥
log(k∆0k∞) − −
ε(1−γ) , which would hold as long as (according to the definition (50) of k)
ρ
t ∆
frame 0
t t th+t frame+ log k k∞ . (55)
≥ ρ ε(1 γ)
(cid:18) − (cid:19)
In addition, if η <1/µ frame, then one has (1 η)µframe 1 ηµ frame/2, thus guaranteeing that
− ≤ −
ηµ 1
ρ=(1 γ) 1 (1 η)µframe) (1 γ) 1 1+ frame = (1 γ)ηµ frame.
− − − ≥ − − 2 2 −
(cid:0) (cid:16) (cid:17)
As a consequence, the condition (55) would hold as long as
2t 1 2 ∆
frame 0
t ≥t th+t frame+ log ≥t th+t frame+ log εk k∞ ,
(1 γ)ηµ ε(1 γ)2 (1 γ)ηµ (1 γ)
− frame (cid:18) − (cid:19) − min (cid:18) − (cid:19)
where we have made use of the simple bound ∆ = Q⋆ 1 with Q =0.
k 0 k∞ k k∞ ≤ 1 −γ 0
7 Discussion
This work develops a sharper finite-sample analysis of the classical asynchronous Q-learning algorithm,
highlighting and refining its dependency on intrinsic features of the Markovian trajectory induced by the
behaviorpolicy. Our samplecomplexitybound strengthensthe state-of-the-artresultby anorderofatleast
. A variance-reducedvariantof asynchronousQ-learningis also analyzed,exhibiting improvedscaling
|S||A|
with the discount complexity 1 .
1 γ
Our findings and the analys−is framework developed herein suggest a couple of directions for future in-
vestigation. For instance, our improved sample complexity of asynchronous Q-learning has a dependence
of 1 on the discount complexity, which is inferior to its model-based counterpart. In the synchronous
(1 γ)5
settin− g, Wainwright (2019a) demonstrated an empirical lower bound 1 for Q-learning. It would be
(1 γ)4
important to determine the exact scaling in this regard. In addition, it w−ould be interesting to see whether
the techniques developed herein can be exploited towards understanding model-free algorithms with more
sophisticated exploration schemes Dann and Brunskill (2015). Finally, asynchronous Q-learning on a single
MarkoviantrajectoryiscloselyrelatedtocoordinatedescentwithcoordinatesselectedaccordingtoaMarkov
chain; one would naturally ask whether our analysis framework can yield improved convergence guarantees
for general Markov-chain-basedoptimization algorithms Doan et al. (2020); Sun et al. (2020).
17Acknowledgements
Y. Wei is supported inpartby the NSF grantCCF-2007911andDMS-2015447. Y. Chi is supported in part
by the grants ONR N00014-18-1-2142and N00014-19-1-2404,ARO W911NF-18-1-0303,NSF CCF-1806154
and CCF-2007911. Y. Chen is supported in partby the grants AFOSRYIP awardFA9550-19-1-0030,ONR
N00014-19-1-2120,ARO YIP awardW911NF-20-1-0097,ARO W911NF-18-1-0303,NSF CCF-1907661,IIS-
1900140andDMS-2014279,andthePrincetonSEASInnovationAward. WethankShicongCen,ChenCheng
and Cong Ma for numerous discussions about reinforcement learning.
A Preliminaries on Markov chains
Foranytwoprobabilitydistributionsµandν,denotebyd (µ,ν)thetotalvariationdistancebetweenµand
TV
ν (Brémaud, 2013). For any time-homogeneous and uniformly ergodic Markov chain (X ,X ,X , ) with
0 1 2
···
transitionkernelP,finite statespace andstationarydistributionµ,we letPt( x) denote the distribution
X ·|
of X conditioned on X =x. Then the mixing time t of this Markov chain is defined by
t 0 mix
t (ǫ):=min t maxd Pt( x),µ ǫ ; (56a)
mix TV
x ·| ≤
t :=t mix(n 1/4(cid:12) (cid:12)). ∈X (cid:0) (cid:1) o (56b)
mix
(cid:12)
A.1 Concentration of empirical distributions of Markov chains
We first recorda result concerningthe concentrationof measureof the empiricaldistribution of a uniformly
ergodic Markov chain, which makes clear the role of the mixing time.
Lemma 5. Consider the above-mentioned Markov chain. For any 0<δ <1, if t 443tmix log4 |X|, then
≥ µmin δ
t
1
y : P x : 1 X =x tµ(x) δ. (57)
X1=y i
∀ ∈X (∃ ∈X { }≤ 2 )≤
i=1
X
Proof. To begin with, consider the scenario when X µ (namely, X follows the stationary distribution of
1 1
∼
the chain). Then (Paulin, 2015, Theorem 3.4) tells us that: for any given x Ω and any τ 0,
∈ ≥
t τ2γ
P 1 X =x tµ(x) τ 2exp ps
X1∼µ i
( i=1 { }≤ − )≤ (cid:18)−8(t+1/γ ps)µ(x)+20τ (cid:19)
X
τ2/t
mix
2exp , (58)
≤ −16(t+2t )µ(x)+40τ
(cid:18) mix (cid:19)
whereγ standsforthe so-calledpseudo spectral gap asdefinedin Paulin(2015,Section3.1). Here,the first
ps
inequality relies on the fact Var [1 X = x ] = µ(x)(1 µ(x)) µ(x), while the last inequality results
Xi∼µ i
{ } − ≤
from the fact γ 1/(2t ) that holds for uniformly ergodic chains (cf. Paulin (2015, Proposition 3.4)).
ps mix
≥
Consequently, for any t t and any τ 0, continue the bound (58) to obtain
mix
≥ ≥
τ2 τ2 τ δ
(58) 2exp 2max exp ,exp ,
≤ −48tµ(x)t +40τt ≤ −96tµ(x)t −80t ≤
(cid:18) mix mix(cid:19) (cid:26) (cid:18) mix(cid:19) (cid:18) mix(cid:19)(cid:27) |X|
provided that τ max 10 tµ(x)t mixlog2 | δX|, 80t mixlog2 | δX| . As a result, by taking τ = 21 10tµ(x) and
≥
applying the union bound, wqe reach
(cid:8) (cid:9)
t t
11 11
P x : 1 X =x tµ(x) P 1 X =x tµ(x) δ, (59)
X1∼µ i X1∼µ i
(∃ ∈X { }≤ 21 )≤ ( { }≤ 21 )≤
i=1 x i=1
X X∈X X
as long as 21 10tµ(x) max 10 tµ(x)t mixlog2 | δX|, 80t mixlog2 | δX| for all x X, or equivalently, when t
≥ ∈ ≥
44 µ1 mt im nix log2 | δX| with µ min :=(cid:8)minq x µ(x). (cid:9)
∈X
18Next, we move on to the case when X takes an arbitrary state y . From the definition of t ()
1 mix
∈ X ·
(cf. (56a)), we know that
d supPtmix(δ)( y), µ δ.
TV
·| ≤
y
(cid:16) ∈X (cid:17)
This together with the definition of d (cf. Paulin (2015, Equation (1.1))) reveals that: for any event
TV
B
that can be fully determined by X , one has
τ }τ ≥tmix(δ)
{
P X =y P X µ
1 1
{B| }− {B| ∼ }
(cid:12)= P X =s P X (cid:12) =s X =y P X =s P X =s X µ
(cid:12) {B | tmix(δ) } { tmi(cid:12)x(δ) | 1 }− {B| tmix(δ) } { tmix(δ) | 1 ∼ }
(cid:12) (cid:12)s X∈S Xs ∈S (cid:12) (cid:12)
(cid:12) P X =s X =y P X =s X µ δ, (cid:12)
tmix(δ) 1 tmix(δ) 1
≤ { | }− { | ∼ } ≤
s X∈S(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
and hence
t
11
supP x : 1 X =x (t t (δ))µ(x)
X1=y i mix
y (∃ ∈X { }≤ 21 − )
∈X i=Xtmix(δ)
t
11
P x : 1 X =x (t t (δ))µ(x) +δ 2δ, (60)
X1∼µ i mix
≤ (∃ ∈X { }≤ 21 − ) ≤
i=Xtmix(δ)
with the proviso that t ≥t mix(δ)+ 44 µ1 mt im nix log2 | δX|.
To finish up, we recall from Paulin (2015, Section 1.1) that t (δ) 2t log2, which together with
mix ≤ mix δ
the above constraint on t necessarily implies that 21 11(t −t mix(δ)) 21t. To conclude, if t 44 µ3 mt im nix log2 | δX|
≥ ≥ ≥
t mix(δ)+ 44 µ1 mt im nix log2 | δX|, one has
t t
1 1
sup P x : 1 X =x tµ(x) sup P x : 1 X =x tµ(x) 2δ.
X1=y i X1=y i
(∃ ∈X { }≤ 2 )≤ (∃ ∈X { }≤ 2 )≤
y y
∈X Xi=1 ∈X i=Xtmix(δ)
as claimed.
A.2 Connection between the mixing time and the cover time
Lemma5combinedwiththe definition(6)immediatelyrevealsthefollowingupperboundonthecovertime:
t
mix
t =O log . (61)
cover
µ |X|
min
(cid:16) (cid:17)
Inaddition,whileageneralmatchingconversebound(namely,t /µ =O(t ))isnotavailable,wecan
mix min cover
come up with some special examples for which the bound (61) is provably tight.
e
Example. Consideratime-homogeneousMarkovchainwithstatespace := 1, , andprobability
X { ··· |X|}
transition matrix
q(k+1) q
P = 1 − 2 I |X|+ k1 |X|1 ⊤ |X|/2 1 |X|1 ⊤ |X|/2 ∈R |X|×|X| (62)
(cid:16) (cid:17) |X|h i
for some quantities q >0 and k 1. Suppose q(k+1)<2 and 3. Then this chain obeys
≥ |X|≥
t
mix
t .
cover ≥ 8log2+4log 1 µ
µmin min
(cid:0) (cid:1)
19Proof. As canbe easilyverified,this chainis reversible,whosestationarydistributionvectorµ R obeys
|X|
∈
2 k1
µ= 1|X|/2 .
(k+1)
/2
|X|(cid:20) |X| (cid:21)
As a result, the minimum state occupancy probability of the stationary distribution is given by
2
µ := min µ = . (63)
min x
1 x (k+1)
≤ ≤|X| |X|
In addition, the reversibility of this chain implies that the matrix Pd := D21PD −21 with D := diag[µ] is
symmetric and has the same set of eigenvalues as P (Brémaud, 2013). A little algebra yields
Pd = (cid:16)1 q(k 2+1) (cid:17)I |X|+ |Xq √k k1 1|X |X|/ |2 /21 1⊤ |X |X|/ |2 √ 1k |1 X| |X /2|/ 12 |1 X⊤ | |X /2|/2 #,
− | " ⊤ /2 ⊤
allowing us to determine the eigenvalues λ as follows
i 1 i
{ } ≤≤|X|
q(k+1)
λ =1 and λ =1 >0 (i 2).
1 i
− 2 ≥
Wearenowreadytoestablishthelowerboundonthe covertime. Firstofall,the well-knownconnection
between the spectral gap and the mixing time gives Paulin (2015, Proposition 3.3)
2log2+log 1 2log2+log 1
t µmin = µmin. (64)
mix
≤ 2(1 λ ) q(k+1)
2
−
In addition, let (x ,x , ) be the corresponding Markov chain, and assume that x µ, where µ stands
0 1 0
··· ∼
for the stationary distribution. Consider the last state — denoted by , which enjoys the minimum state
|X|
occupancy probability µ . For any integer t>0 one has
min
t
P x = , 0 l t ( =i)P x = P x = x = , ,x =
l 0 l 0 l 1
{ 6 |X| ∀ ≤ ≤ } { 6 |X|} 6 |X| 6 |X| ··· − 6 |X|
Yl=1 n (cid:12) o
(ii) t (cid:12) (cid:12)
P x = min P x = x =j
0 l l 1
≥ { 6 |X|} j:j= 6 |X| −
l=1 6 |X|
Y (cid:8) (cid:12) (cid:9)
(iii) 2 q t (cid:12)
= 1 1
− (k+1) −
(cid:16) |X|(cid:17)(cid:18) |X|(cid:19)
(iv) 2 2qt
1 1 ,
≥ − (k+1) −
|X|(cid:17)(cid:18) |X|(cid:19)
(cid:16)
where(i)followsfromthechainrule,(ii)reliesontheMarkovianproperty,(iii)resultsfromtheconstruction
(62), and (iv) holds as long as q t< 1. Consequently, if 3 and if t< |X|, then one necessarily has
2 |X|≥ 8q
|X|
2 2qt 1
P x = , 0 l t 1 1 > .
l
{ 6 |X| ∀ ≤ ≤ }≥ − (k+1) − 2
|X|(cid:17)(cid:18) |X|(cid:19)
(cid:16)
This taken collectively with the definition of t (cf. (6)) reveals that
cover
t
mix
t cover ≥ | 8X q| ≥ 8log2+4log 1 µ ,
µmin min
(cid:0) (cid:1)
where the last inequality is a direct consequence of (63) and (64).
20B Cover-time-based analysis of asynchronous Q-learning
In this section, we sketch the proof of Theorem 2. Before continuing, we recall the definition of t in (6),
cover
and further introduce a quantity
T
t :=t log . (65)
cover,all cover
δ
There are two useful facts regarding t that play an important role in the analysis.
cover,all
Lemma 6. Define the event
:= (s,a) s.t. it is not visited within iterations lt ,(l+1)t ,
l cover,all cover,all
K ∃ ∈S×A
n (cid:0) (cid:3)o
and set L:= T . Then one has P L δ.
⌊tcover,all⌋ l=0Kl ≤
n o
Proof. See Section E.6. S
In other words, Lemma 6 tells us that with high probability, all state-action pairs are visited at least
once in everytime frame (lt ,(l+1)t ] with 0 l T/t . The next resultis animmediate
cover,all cover,all cover,all
≤ ≤⌊ ⌋
consequence of Lemma 6; the proof can be found in Section E.2.
Lemma 7. For any δ >0, recall the definition of t in (65). Suppose that T >t and 0<η <1.
cover,all cover,all
Then with probability exceeding 1 δ one has
−
t
I Λ j ∆ 0 (1 η)2tcovt er,all ∆ 0 1 (66)
(cid:12) − (cid:12)≤ − k k∞
(cid:12) (cid:12)j Y=1 (cid:12)
(cid:0) (cid:1) (cid:12)
(cid:12) (cid:12)
uniformly over all t obeying T (cid:12)t t and (cid:12)all vector ∆ R .
cover,all 0 |S||A|
≥ ≥ ∈
With the abovetwolemmas inmind, we arenow positionedto proveTheorem2. Repeating the analysis
of (42) (except that Lemma 2 is replaced by Lemma 7) yields
γ t ∆ t (I Λ )Λ 1+τ V⋆ 1+ ∆ 1, t<t
|∆ t |≤(γ Pt ii == 11 (cid:13) (cid:13)∆ ii −− 11 (cid:13) (cid:13)∞∞ Qt jj == ii ++ 11 (I −− Λ jj )Λ ii 1+τ 11 kk V⋆ kk ∞∞ 1+(cid:13) (cid:13)(1 −0 (cid:13) (cid:13)η∞ )2tcovt er,all ∆ 0 ∞1, t cover,c ao lv le ≤r,al tl ≤T
P (cid:13) (cid:13) Q (cid:13) (cid:13)
with probability a(cid:13)t least(cid:13)1 2δ. This observation resembles (42), except that(cid:13)t fram(cid:13)e (resp. µ min) is replaced
−
by t (resp. 1 ). As a consequence, we can immediately use the recursive analysis carried out in
cover,all tcover,all
Section 6.2.2 to establish a convergence guarantee based on the cover time. More specifically, define
η)2t tc co ov ve er r, ,a all η)1
ρ:=(1 γ) 1 (1 ll =(1 γ) 1 (1 2 . (67)
− − − − − −
(cid:16) (cid:17) (cid:16) (cid:17)
Replacing ρ by ρ in Theoerem 5 reveals that with probability at least 1 6δ,
−
Q Q⋆ cγ T
eQ Q⋆ (1 ρ)kk 0 − k∞ + V⋆ ηlog |S||A| +ε (68)
t
k − k∞ ≤ − 1 γ 1 γk k∞r δ
− − (cid:16) (cid:17)
holds for all t ≤T, where k :=max e0, tt co− vet rt ,h and we abuse notation to define
all
(cid:8) (cid:4) (cid:5)(cid:9)
1
t :=2t log .
th cover,all (1 γ)2ε
−
Repeating the proof of the inequality (51) yields
∆
(1 ρ)kk 0 k∞ ε,
− 1 γ ≤
−
e
21whenever t t + t + 2tcover,all log 1 , with the proviso that η < 1/2. In addition, setting
≥ th cover,all (1 γ)η ε(1 γ)2
η = (1 −γ)4 guarantees− that (cid:0) − (cid:1)
c2γ2ε2log |S||A|T
δ
(cid:0) (cid:1)
cγ T cγ T
V⋆ ηlog |S||A| ηlog |S||A| ε.
1 γk k∞r δ ≤ (1 γ)2 δ ≤
r
− (cid:16) (cid:17) − (cid:16) (cid:17)
In conclusion, we have Q Q⋆ 3ε as long as
t
k − k∞ ≤
ct T 1
′ cover,all
t log |S||A| log ,
≥ (1 γ)5ε2 δ ε(1 γ)2
− (cid:16) (cid:17) (cid:16) − (cid:17)
for some sufficiently large constant c >0. This together with the definition (65) completes the proof.
′
C Analysis under adaptive learning rates (proof of Theorem 3)
Useful preliminary facts. We first make note of severaluseful properties about η .
t
InvokingtheconcentrationresultinLemma5, onecaneasilyshowthatwithprobabilityatleast1 δ,
• −
1 K (s,a)
t
µ <min <2µ (69)
min min
2 s,a t
tmixlog(|S|| δA|t)
holds simultaneously for all t obeying T t & . In addition, this concentration result
≥ µmin
taken collectively with the update rule (17) of µ (in particular, the second case of (17)) implies
min,t
that µ stabilizes as t grows; more precisely, there exists some quantity c [1/4,4] such that
min,t ′
∈
b
µ cµ (70)
b min,t ≡ ′ min
tmixlog(|S|| δA|t)
holds simultaneously for all t obeying T tb& .
≥ µmin
tmixlog(|S|| δA|t)
For any t obeying t & (so that logt is small enough), the learning rate (18)
• µmin(1 −γ) µb min,t(1 −γ)γ2t
simplifies to
logt
η =c exp log . (71)
t η cµ (1 γ)γ2t
(cid:16)j ′ min − k(cid:17)
Clearly,there exists a sequence ofendpoints t <t <t <... aswell asa thresholdk such that: for
1 2 3 th
any k k one has
th
≥
2t <t <3t and (72)
k k+1 k
α logt
k k+1
η =η := , t <t t (73)
t (k) µ (1 γ)γ2t ∀ k ≤ k+1
min k+1
−
for some constant α > 0; in words, (73) provides a concrete expression for the piecewise constant
k
learning rate, where the t ’s form the change points.
k
tmixlog(|S|| δA|t)
Combining (73) with the definition of Q (cf. (17)), one can easily check that for t& ,
t µmin(1 −γ)
Q =b Q , t <t t , (74)
t tk k k+1
∀ ≤
sothatQ remainsfixedwithineachtimbe segment(t ,t ]. As aconsequence,weonlyneedtoanalyzeQ
t k k+1 tk
in the sequel, which can be easily accomplished by invoking Theorem 1.
b
22A crude bound. Given that 0<η 1 and 0 r(s,a) 1, the update rule (7) of Q implies that
t t
≤ ≤ ≤
Q max (1 η ) Q +η (1+γ Q ), Q Q +γ,
t t t 1 t t 1 t 1 t 1
k k∞ ≤ − k − k∞ k − k∞ k − k∞ ≤k − k∞
thus leading to the followin(cid:8)g crude bound (cid:9)
2 1
Q Q⋆ t+ Q + Q⋆ t+ 3t, for any t> . (75)
t 0
k − k∞ ≤ k k∞ k k∞ ≤ 1 γ ≤ 1 γ
− −
Refined analysis. Define
c k,0log(|S|| δA|tk)logt
k
ε := , (76)
k s µ min(1 γ)5γ2t
k
−
where c = α /c > 0 is some constant, and c > 0 is the constant stated in Theorem 1. The property
k,0 k 1 1 1
−
(73) of η together with the definition (76) implies that
t
c (1 γ)4ε2 c 1
η = 1 − k = 1 min (1 γ)4ε2, , t (t ,t ],
t log(|S||A|tk) log(|S||A|tk) − k t mix ∀ ∈ k −1 k
δ δ n o
as long as (1 γ)4ε2 1/t , or more explicitly,
− k ≤ mix
c k,0t mixlog(|S|| δA|tk)logt
k
t . (77)
k ≥ µ (1 γ)γ2
min
−
In addition, condition (72) further tells us that
c k,0log |S|| δA|tk logt k
t t t =
k − k −1 ≍ k µ min( (cid:0)1 −γ)5γ (cid:1)2ε2
k
1 t t
mix k
max , log |S||A| logt
≍ µ (1 γ)5ε2 µ (1 γ) δ k
n min − k min − o (cid:16) (cid:17)
underthesamplesizecondition(77). Nowsupposethatc issufficientlylarge(whichcanbeguaranteedby
k,0
adjusting the constant c in (18)). Invoking Theorem 1 with an initialization Q (which clearly satisfies
η tk−1
the crude bound (75)) ensures that
Q Q⋆ ε (78)
tk k
k − k∞ ≤
with probability at least 1 δ, with the proviso that
−
c 1 t t t
5 mix k k
t + log |S||A| log (79)
k ≥ µ (1 γ)5ε2 1 γ δ (1 γ)2ε
min (cid:26) − k − (cid:27) (cid:16) (cid:17) (cid:16) − k (cid:17)
for some large enough constant c >0.
5
Finally, taking t to be the largest change point that does not exceed T, we see from (72) that
kmax
1T t T. By choosing the constant c to be sufficiently large, we can ensure that ε ε. These
3 ≤ kmax ≤ 5 kmax ≤
immediately conclude the proof of the theorem under the sample size condition (21).
D Analysis of asynchronous variance-reduced Q-learning
This section aims to establish Theorem 4. We carry out an epoch-based analysis, that is, we first quantify
the progress made over each epoch, and then demonstrate how many epochs are sufficient to attain the
desired accuracy. In what follows, we shall overloadthe notation by defining
443t 4 t
mix epoch
t frame := log |S||A| , (80a)
µ δ
min
(cid:16) (cid:17)
232log 1
(1 γ)2ε
t th :=max − , t frame , (80b)
ηµ
( min )
ρ:=(1 γ) 1 (1 η)µframe , (80c)
− − −
1
µ := µ t(cid:0) . (cid:1) (80d)
frame min frame
2
D.1 Per-epoch analysis
Westartbyanalyzingtheprogressmadeovereachepoch. Beforeproceeding,wedenotebyP [0,1]
|S||A|×|S|
∈
a matrix corresponding to the empirical probability transition kernel used in (23) from N new sample
transitions. Further, we use the vector Q R to represent the reference Q-function, aend introduce the
|S||A|
vector V R to represent the correspon∈ ding value function so that V(s):=max Q(s,a) for all s .
|S| a
∈ ∈S
Forconvenience,thissubsectionabusesnotationtoassumethatanepochstartswithanestimateQ =Q,
0
and consists of the subsequent
8log 2
1 γ
t epoch :=t frame+t th+ − (81)
(1 γ)ηµ
min
−
iterationsofvariance-reducedQ-learningupdates,wheret andt aredefinedin(80a)and(80b),respec-
frame th
tively. In the sequel, we divide all epochs into two phases, depending on the quality of the initial estimate
Q in each epoch.
D.1.1 Phase 1: when Q Q⋆ >1/√1 γ
k − k∞ −
Recalling the matrix notation of Λ and P in (29) and (30), respectively, we can rewrite (22) as follows
t t
Q = I Λ Q +Λ r+γP (V V)+γPV . (82)
t t t 1 t t t 1
− − − −
Following similar steps as in th(cid:0)e expres(cid:1)sion (34), w(cid:16) e arrive at the following error(cid:17) decomposition
e
Θ :=Q Q⋆ = I Λ Q +Λ r+γP (V V)+γPV Q⋆
t t t t 1 t t t 1
− − − − − −
=(cid:0)I Λ t(cid:1) Q Q⋆(cid:16) +Λ r+γP t(V eV)(cid:17) +γPV Q⋆
t 1 t t 1
− − − − − −
(cid:16) (cid:17)
=(cid:0)I Λ t(cid:1)(cid:0)Q Q⋆(cid:1)+γΛ P t(V V)+PV ePV⋆
t 1 t t 1
− − − − − −
=(cid:0)I Λ (cid:1)Θ(cid:0) +γΛ (cid:1)P P (cid:16) V +γΛ P P (V⋆ V)+(cid:17) γΛ P V V⋆ , (83)
− t t −1 t − t t − e − t t t −1 −
which once again lea(cid:0)ds to a r(cid:1)ecursive rela(cid:0)tion (cid:1) (cid:0) (cid:1) (cid:0) (cid:1)
e
t t t t
Θ =γ I Λ Λ P P V +γ I Λ Λ P P (V⋆ V)
t j i j i i
− − − − −
i=1j=i+1 i=1j=i+1
X Y (cid:0) (cid:1) (cid:0) (cid:1) X Y (cid:0) (cid:1) (cid:0) (cid:1)
e
=:h0,t =:h1,t
t t t
| {z } | {z }
+γ I Λ Λ P V V⋆ + I Λ Θ . (84)
j i i i 1 j 0
− − − −
i=1j=i+1 j=1
X Y (cid:0) (cid:1) (cid:0) (cid:1) Y(cid:0) (cid:1)
=:h2,t =:h3,t
This identity takes a very sim|ilar form as (35) {ezxcept for the addi}tion|al term{zh . }
0,t
Let us begin by controlling the first term, towards which we have the following lemma. The proof is
postponed to Section E.5.
Lemma 8. Suppose that P is constructed using N consecutive sample transitions. If N >t , then with
frame
probability greater than 1 δ, one has
−
e
4log 6N |S||A| γ 4log 6N |S||A|
h γ δ V V⋆ + δ . (85)
0,t
k k∞ ≤ s N (cid:0)µ min (cid:1) − ∞ 1 −γs N (cid:0)µ min (cid:1)
(cid:13) (cid:13)
(cid:13) (cid:13)
24Inheriting the results from Lemma 1 and Lemma 2, we are guaranteed that
h τ V⋆ V 1
1,t 2
≤ k − k∞
(cid:12) (cid:12) (1 −η)21tµmin kΘ k∞1, if t ≤t ≤t
(cid:12) (cid:12) 0 frame epoch
h
| 3,t |≤ Θ 1, if t<t
k 0 k∞ frame
with probability at least 1 2δ, where
−
t
epoch
τ :=c′γ ηlog |S||A|
2
δ
r
(cid:0) (cid:1)
for some constant c > 0 (similar to (39)). In addition, the term h can be bounded in the same way as
′ 2,t
β in (37). Therefore, repeating the same argument as for Theorem 5, we conclude that with probability
2,t
at least 1 δ,
−
Θ Q Q⋆
Θ (1 ρ)kk 0 k∞ +τ +ξ =(1 ρ)kk − k∞ +τ +ξ (86)
t
k k∞ ≤ − 1 γ − 1 γ
− −
holds simultaneously for all 0<t ≤t epoch, where ke=max 0, t − tfrt at mh e,ξ and e
(cid:8) (cid:4) (cid:5)(cid:9)
τ := cγ logN |S δ||A| + V⋆ V ηlog |S||A|t epoch + 4log 6N |S δ||A| ,
1 −γ  s(1 −γ)2Nµ min k − k∞ r δ s N (cid:0)µ min (cid:1)
(cid:16) (cid:17) 
e 2log 1  
 (1 γ)2ξ 
t th,ξ :=max − ,t frame
ηµ
( min )
for some constant c>0.
Let C >0 be some sufficient large constant. Setting η t ≡η =min Cγ2lo( g1 − |Sγ ||) A2 δ|tepoch, µfr1 ame , ξ = 16√1 1 −γ
logN|S||A| n o
and ensuring N ≥max {t frame,C −γ)3δ µmin}, we can easily demonstrate that
(1
Q Q⋆ 1 1
Θ (1 ρ)kk − k∞ + + V⋆ V .
t
k k∞ ≤ − 1 γ 8√1 γ 4k − k∞
− −
8log 2
As a consequence, if t t +t + 1−γ , one has
epoch ≥ frame th,ξ (1 −γ)ηµmin
1
(1 ρ)k (1 γ),
− ≤ 8 −
which in turn implies that
1 1 1 1 1
Θ Q Q⋆ + + V⋆ V max , Q Q⋆ , (87)
tepochk∞
k ≤ 8k − k∞ 8 (1 γ) 4k − k∞ ≤ 2 √1 γ k − k∞
− n − o
where the last step invokes the simplp e relation V⋆ V Q Q⋆ . Thus, we conclude that
k − k∞ ≤k − k∞
1 1
Q Q⋆ max , Q Q⋆ . (88)
tepoch
k − k∞ ≤ 2 √1 γ k − k∞
n − o
D.1.2 Phase 2: when Q Q⋆ 1/√1 γ
k − k∞ ≤ −
The analysis of Phase 2 follows by straightforwardly combining the analysis of Phase 1 and that of the
synchronous counterpart in Wainwright (2019b). For the sake of brevity, we only sketch the main steps.
25FollowingtheproofideaofWainwright(2019b,SectionB.2),weintroduceanauxiliaryvectorQwhichis
theuniquefixpointtothefollowingequation,whichcanberegardedasapopulation-levelBellmanequation
with proper rewardperturbation, namely, b
Q=r+γP(V V)+γPV. (89)
−
Here,asusual,V R representsthebvaluefunctionbcorrespondeingtoQ. ThiscanbeviewedasaBellman
|S|
∈
equationwhen the rewardvector r is replacedby r+γ(P P)V. Repeating the arguments in the proofof
−
Wainwright(201b9b,Lemma4)(exceptthatweneedtoapplythe measurbeconcentrationofP inthe manner
performed in the proof of Lemma 8 due to Markoviandaeta), we reach
e
log|S||A|
Q Q⋆ c δ ε (90)
′
− ≤ s(1 −γ)3Nµ ≤
∞ min
(cid:13) (cid:13)
(cid:13)b (cid:13)
with probability at least 1 −δ for some constant c ′ > 0, provided that N (c ′)2l (o 1g| γS )| δ 3|A ε2| and that kQ
≥ −
−
Q⋆ 1/√1 γ. It is worth noting that Q only serves as a helper in the proof and is never explicitly
k∞ ≤ −
constructed in the algorithm, as we don’t have access to the probability transition matrix P.
In addition, we claim that b
Q Q⋆ Q Q⋆
Q −Q k − k∞ + k − k∞ +ε. (91)
tepoch
≤ 8 8
∞
(cid:13) (cid:13) b
Under this claim, the triangl(cid:13)e inequalitby(cid:13)yields
1 9
Q Q⋆ Q Q + Q Q⋆ Q Q⋆ + Q Q⋆ +ε
tepoch tepoch
k − k∞ ≤k − k∞ k − k∞ ≤ 8k − k∞ 8k − k∞
1 17
Q Q⋆b + εb, b (92)
≤ 8k − k∞ 8
where the last inequality follows from (90).
Proofoftheinequality(91). Recallingthevariance-reducedupdaterule(82)andusingtheBellman-type
equation (89), we obtain
Θ :=Q Q= I Λ (Q Q)+Λ r+γP (V V)+γPV r γP(V V) γPV
t t t t 1 t t t 1
− − − − − − − − − −
b b =(cid:0)I Λ t(cid:1) (Q t 1 Qb)+Λ t(cid:16) γP t(V t 1 V) γP(Ve V) b e (cid:17)
− − − − − − −
(cid:16) (cid:17)
=(cid:0)I Λ t(cid:1)Θ t 1+γΛ bt (P t P)(V V)+P t(V t 1b V) . (93)
− − − − − −
(cid:0) (cid:1) (cid:16) (cid:17)
Adopting the same expansionbas before (see (35)), webarrive at b
t t t t t
Θ =γ I Λ Λ P P (V V)+γ I Λ Λ P V V + I Λ Θ .
t j i i j i i i 1 j 0
− − − − − − −
i=1j=i+1 i=1j=i+1 j=1
X Y (cid:0) (cid:1) (cid:0) (cid:1) X Y (cid:0) (cid:1) (cid:0) (cid:1) Y(cid:0) (cid:1)
b b b b
=:ϑ1,t =:ϑ2,t =:ϑ3,t
| {z } | {z } | {z }
Inheriting the results in Lemma 1 and Lemma 2, we can demonstrate that, with probability at least 1 2δ,
−
t
ϑ cγ V V ηlog |S||A| epoch 1;
1,t
≤ k − k∞r δ
(cid:12) (cid:12) (cid:12) (cid:12) (1b η)1 2tµmin Θ 0 (cid:16) 1, if t fram(cid:17) e t t epoch,
ϑ − k k∞ ≤ ≤
| 3,t |≤ Θ 1, if t<t .
k 0 k∞ b frame
 b
26Repeating the same argument as for Theorem 5, we reach
Q Q cγ t
Θ (1 ρ)kk − k∞ + V V ηlog |S||A| epoch +ε
t
k k∞ ≤ − 1 γ 1 γk − k∞r δ
b − − (cid:16) (cid:17)
for some constantbc>0, where k =max {0, t t− frat mt eh withbt th defined in (80b).
}
Bytakingη =c 5min γ2log(1 |− S|γ |A)2 |tepoch, µfr1 a(cid:4) fo(cid:5)rsomesufficientlysmallconstantc 5 >0andensuringthat
me
δ
(cid:8) (cid:9)
c 1
6
t t +t + log
epoch ≥ th frame (1 γ)ηµ (1 γ)2
min
− −
for some large constant c >0, we obtain
6
Q Q Q Q⋆ Q Q⋆
kΘ k − k∞ +ε k − k∞ + k − k∞ +ε,
tepochk∞
≤ 8 ≤ 8 8
b b
where the last line follobws by the triangle inequality.
D.2 How many epochs are needed?
We are now ready to pin down how many epochs are needed to achieve ε-accuracy.
InPhase1,the contractionresult(88)indicates that, ifthe algorithmis initializedwithQ =0 atthe
0
•
very beginning, then it takes at most
Q⋆ 1 1
log maxk ε,k∞ !≤log +log
2 1 2 √1 γ 2 ε(1 γ)
√1 −γ (cid:16) − (cid:17) (cid:16) − (cid:17)
epochs to yield Q Q⋆ m(cid:8) ax 1 (cid:9) ,ε (so as to enter Phase 2). Clearly, if the target accuracy
level ε> 1 ,k then− thek a∞ lgo≤ rithm{ te√ r1 m− iγ nat} es in this phase.
√1 γ
−
Supposenowthatthetargetaccuracylevelε 1 . OncethealgorithmentersPhase2,thedynamics
• ≤ √1 γ
−
canbe characterizedby (92). GiventhatQisalsothe lastiterateoftheprecedingepoch,theproperty
(92) provides a recursive relation across epochs. Standard recursive analysis thus reveals that: within
at most
1 1
c log c log
7 7
ε√1 γ ≤ ε(1 γ)
(cid:16) − (cid:17) (cid:16) − (cid:17)
epochs (with c >0 some constant), we are guaranteed to attain an ℓ estimation error at most 3ε.
7
∞
Tosummarize,atotalnumberofO log 1 +log 1 epochsaresufficientforourpurpose. Thisconcludes
ε(1 γ) 1 γ
the proof. − −
(cid:0) (cid:1)
E Proofs of technical lemmas
E.1 Proof of Lemma 1
Fix any state-action pair (s,a) , and let us look at β (s,a), namely, the (s,a)-th entry of
1,t
∈S×A
t t
β =γ I Λ Λ P P V⋆.
1,t j i i
− −
i=1j=i+1
X Y (cid:0) (cid:1) (cid:0) (cid:1)
For convenience of presentation, we abuse the notation to let Λ (s,a) denote the (s,a)-th diagonal entry
j
of the diagonal matrix Λ , and P (s,a) (resp. P(s,a)) the (s,a)-th row of P (resp. P). In view of the
j t t
definition (35), we can write
t t
β (s,a)=γ 1 Λ (s,a) Λ (s,a) P (s,a) P(s,a) V⋆. (94)
1,t j i i
− −
i=1j=i+1
X Y (cid:0) (cid:1) (cid:0) (cid:1)
27As it turns out, it is convenient to study this expression by defining
t (s,a):=the time stamp when the trajectory visits (s,a) for the k-th time (95)
k
and
K (s,a):=max k t (s,a) t , (96)
t k
{ | ≤ }
namely, the total number of times — during the first t iterations — that the sample trajectory visits (s,a).
With these in place, the special form of Λ (cf. (29)) allows us to rewrite (94) as
j
Kt(s,a)
β 1,t(s,a)=γ (1 −η)Kt(s,a) −kη P tk+1(s,a) −P(s,a) V⋆. (97)
k=1
X (cid:0) (cid:1)
where we suppress the dependency on (s,a) and write t := t (s,a) to streamline notation. The main step
k k
thus boils down to controlling (97).
Towards this, we claim that: there exists some constant c>0 such that with probability at least 1 δ,
−
K
T
(1 −η)K −kη P tk+1(s,a) −P(s,a) V⋆ (cid:12)≤c rηlog |S|| δA| kV⋆ (98)
(cid:12) k∞
(cid:12) (cid:12)Xk=1 (cid:12) (cid:16) (cid:17)
(cid:0) (cid:1) (cid:12)
holds simultaneou(cid:12) (cid:12)sly for all (s,a) and all 1 K(cid:12) (cid:12) T, provided that 0 < ηlog |S||A|T < 1.
∈ S ×A ≤ ≤ δ
Recognizing the trivial bound K (s,a) t T (by construction (96)) and substituting the bound (98) into
t
≤ ≤ (cid:0) (cid:1)
the expression (97), we arrive at
T
(s,a) : β (s,a) c ηlog |S||A| V⋆ , (99)
1,t
∀ ∈S×A | |≤ δ k k∞
r
(cid:16) (cid:17)
thus concluding the proof of this lemma. It remains to validate the inequality (98).
Proof of the inequality (98). Before proceeding, we introduce some additional notation. Let Var (V)⋆
P
R be a vector whose (s,a)-th entry is given by the variance of V⋆ w.r.t. the transition probabilit∈ y
|S||A|
P () from state s when action a is taken, namely,
s,a
·
(s,a) , Var (V⋆) := P (s) V⋆(s) 2 P (s)V⋆(s) 2 . (100)
∀ ∈S×A P (s,a) s,a ′ ′ − s,a ′ ′
(cid:2) (cid:3) s X′ ∈S (cid:0) (cid:1) (cid:16)s X′ ∈S (cid:17)
We first make the observation that: for any fixed integer K >0, the following vectors
P (s,a) 1 k K
tk+1
{ | ≤ ≤ }
are identically and independently distributed. To justify this observation, let us denote by P () the
s,a
·
transition probability from state s when action a is taken. For any i , ,i , one obtains
1 K
··· ∈S
P s =i ( 1 k K) =P s =i ( 1 k K 1) and s =i
tk+1 k tk+1 k tK+1 K
{ ∀ ≤ ≤ } { ∀ ≤ ≤ − }
= P s =i ( 1 k K 1) and t =m and s =i
tk+1 k K m+1 K
{ ∀ ≤ ≤ − }
m>0
X
( =i) P s =i ( 1 k K 1) and t =m P s =i s =s,a =a
tk+1 k K m+1 K m m
{ ∀ ≤ ≤ − } { | }
m>0
X
=P (i ) P s =i ( 1 k K 1) and t =m
s,a K tk+1 k K
{ ∀ ≤ ≤ − }
m>0
X
=P (i )P s =i ( 1 k K 1) ,
s,a K tk+1 k
{ ∀ ≤ ≤ − }
where (i) holds true from the Markov property as well as the fact that t is an iteration in which the
K
trajectory visits state s and takes action a. Invoking the above identity recursively,we arrive at
K
P s =i ( 1 k K) = P (i ), (101)
tk+1 k s,a j
{ ∀ ≤ ≤ }
j=1
Y
28meaning that the state transitions happening at times t , ,t are independent, each following the
1 K
distribution P (). This clearly demonstrates the indepen{ den· c· e· of } P (s,a) 1 k K .
s,a tk+1
· { | ≤ ≤ }
With the above observation in mind, we resort to the Bernstein inequality to bound the quantity of
interest (which has zero mean). To begin with, the variance parameter can be characterizedby
K K
Var (1 −η)K −kη P tk+1(s,a) −P(s,a) V⋆ #= (1 −η)2K −2kη2Var P tk+1(s,a) −P(s,a) V⋆
"
k=1 k=1
X (cid:0) (cid:1) X (cid:2)(cid:0) (cid:1) (cid:3)
K
=η2 (1 η)2K 2kVar V⋆
− Ps,a
−
k=1
X (cid:2) (cid:3)
η2Var V⋆ ∞ (1 η)j = η2 Var V⋆
Ps,a Ps,a
≤ − 1 (1 η)
j=0 − −
(cid:2) (cid:3)X (cid:2) (cid:3)
=ηVar V⋆ =:σ2 .
Ps,a K
In addition, each term in the summation clearly satisfies (cid:2) (cid:3)
(1 η)K kη P (s,a) P(s,a) V⋆ 2η V⋆ =:D, 1 k K.
− tk
− − ≤ k k∞ ≤ ≤
As a consequence(cid:12) (cid:12), invoking the(cid:0)Bernstein inequali(cid:1)ty im(cid:12) (cid:12)plies that
K
T T
(1 −η)K −kη P tk(s,a) −P(s,a) V⋆ (cid:12)≤c˜ rσ K2 log |S|| δA| +Dlog |S|| δA|
(cid:12) !
(cid:12) (cid:12)k X=1 (cid:0) (cid:1) (cid:12) (cid:12) (cid:16) (cid:17) (cid:16) (cid:17)
(cid:12) T (cid:12) T
(cid:12) c˜ η V⋆ 2 log |S||A| +2η(cid:12) V⋆ log |S||A|
≤ r k k∞ δ k k∞ δ !
(cid:16) (cid:17) (cid:16) (cid:17)
T
3c˜ ηlog |S||A| V⋆ (102)
≤ δ k k∞
r
(cid:16) (cid:17)
with probability exceeding 1 δ , where the second line relies on the simple bound Var V⋆
− T Ps,a ≤
|S||A|
V⋆ 2 ,andthelastlineholdsif0<ηlog |S||A|T <1. Takingtheunionboundoverall(s,a) (cid:2) (cid:3)and
ak 1k∞K probabδ sim∈ ulS ta× neA
ll T then reveals that: with ility at least 1 δ, the inequality (102) holds ously
≤ ≤ (cid:0) (cid:1) −
over all (s,a) and all 1 K T. This concludes the proof.
∈S×A ≤ ≤
E.2 Proof of Lemma 2 and Lemma 7
Proof of Lemma 2. Letβ = t I Λ ∆ . Denotebyβ (s,a)(resp.∆ (s,a))the(s,a)-thentry
of β (resp. ∆ ). From the3 d,t efinitioj= n1 of β− ,j it is0 easily seen tha3 t,t 0
3,t 0 3,t
Q (cid:0) (cid:1)
β (s,a) =(1 η)Kt(s,a) ∆ (s,a) , (103)
3,t 0
| | −
(cid:12) (cid:12)
where K t(s,a) denotes the number of times the sample traj(cid:12)ectory vi(cid:12)sits (s,a) during the iterations [1,t]
(cf. (96)). By virtue of Lemma 5 and the union bound, one has, with probability at least 1 δ, that
−
K (s,a) tµ /2 (104)
t min
≥
simultaneously over all (s,a) and all t obeying 443τmix log4 |S||A|T t T. Substitution into the
∈ S ×A µmin δ ≤ ≤
relation (103) establishes that, with probability greater than 1 δ,
−
β 3(s,a) (1 η)1 2tµmin ∆ 0(s,a) . (105)
| |≤ −
(cid:12) (cid:12)
holds uniformly over all (s,a) and all t obeying 443τ(cid:12)mix log4 |S(cid:12)||A|T t T, as claimed.
∈S×A µmin δ ≤ ≤
29Proof of Lemma 7. The proof of this lemma is essentially the same as that of Lemma 2, except that we
use instead the following lower bound on K (s,a) (which is an immediate consequence of Lemma 6)
t
t t
K (s,a) (106)
t
≥ t ≥ 2t
cover,all cover,all
j k
for all t>t . Therefore, replacing tµ with t/t in the above analysis, we establish Lemma 7.
cover,all min cover,all
E.3 Proof of Lemma 3
We provethis factvia aninductive argument. The basecasewitht=0is a consequenceofthe crude bound
(44). Now, assume that the claim holds for all iterations up to t 1, and we would like to justify it for the
−
t-th iteration as well. Towards this, define
∆ , if t t ,
0 th
h(t):= k k∞ ≤ (107)
((1 γ)ε, if t>t th.
−
Recall that (1 η)1 2tµmin (1 γ)ε for any t t th. Therefore, combining the inequality (42) with the
− ≤ − ≥
induction hypotheses indicates that
t t τ V⋆
∆ γ (I Λ j)Λ i1 1 k k∞ +u 1+ε +τ V⋆ 1+h(t)1
t i 1
| |≤ − · 1 γ − k k∞
i=1j=i+1 (cid:18) − (cid:19)
X Y
t t t t τ V⋆
=γ (I Λ j)Λ i1u 1+γ (I Λ j)Λ i1 1 k k∞ +ε +τ V⋆ 1+h(t)1.
i 1
− − − 1 γ k k∞
i=1j=i+1 i=1j=i+1 (cid:18) − (cid:19)
X Y X Y
Taking this together with the inequality (46b) and rearrangingterms, we obtain
t t γτ V⋆
∆ γ (I Λ j)Λ i1u 1+ 1 k k∞1+γε1+τ V⋆ 1+h(t)1
t i 1
| |≤ − − 1 γ k k∞
i=1j=i+1 −
X Y
τ V⋆ t t
= 1 k k∞1+γε1+γ (I Λ j)Λ i1u 1+h(t)1
i
1 γ − −
− i=1j=i+1
X Y
τ V⋆
= 1 k k∞1+γε1+v t+(1 γ)ε1 t>t 1
th
1 γ − { }
−
τ V⋆
1 k k∞1+ε1+v t, (108)
≤ 1 γ
−
where we have used the definition of v in (47). This taken collectively with the definition u = v
t t t
k k∞
establishes that
τ V⋆
∆ 1 k k∞ +ε+u
t t
k k∞ ≤ 1 γ
−
as claimed. This concludes the proof.
E.4 Proof of Lemma 4
We shall prove this result by induction over the index k. To start with, consider the base case where k =0
and t < t +t . By definition, it is straightforward to see that u ∆ /(1 γ) = w . In fact,
th frame 0 0 0
≤ k k∞ −
repeating our argument for the crude bound (see Section 6.2.2) immediately reveals that
∆
0
t 0: u k k∞ =w 0, (109)
t
∀ ≥ ≤ 1 γ
−
thus indicating that the inequality (50) holds for the base case. In what follows, we assume that the
inequality (50) holds up to k −1, and would like to extend it to the case with all t obeying t t− frat mt eh =k.
(cid:4) (cid:5)
30Let us focus on the case when t=t +kt ; the case with t=t +kt +j (1 j <t ) follows
th frame th frame frame
≤
from an analogous argument and is omitted for brevity. In view of the definition of v (cf. (47)) as well as
t
our induction hypotheses, one can arrange terms to derive
tth+ktframetth+ktframe
v =γ (I Λ )Λ 1u
tth+ktframe j i i −1
−
i=1 j=i+1
X Y
=γk −1 tth+ktframe
(I Λ )Λ 1u
j i i 1
( − − )
Xs=0 i:max iX−1−tth ,0 =s j= Yi+1
⌊ tframe ⌋
γk −1 (cid:8) (cid:9) tth+ktframe
(I Λ )Λ 1 w , (110)
j i s
≤ ( − )
Xs=0 i:max iX−1−tth ,0 =s j= Yi+1
⌊ tframe ⌋
where the last inequality follows from our ind(cid:8) uction hyp(cid:9) otheses and the non-negativity of (I Λ )Λ 1.
j i
−
Given any state-action pair (s,a) , let us look at the (s,a)-th entry of v — denoted by
v (s,a), towards which it is co∈ neS vn× ieA nt to pause and introduce some notationtth .+ Rkt efr cam ae ll that Nj(s,a)
tth+ktframe i
has been used to denote the number of visits to the state-actionpair (s,a) between iteration i and iteration
j (including i and j). To help study the behavior in each timeframe, we introduce the following quantities
Nk 1 :=Nj(s,a) with i=t +st +1, j =t +kt (111)
s− i th frame th frame
for every s k 1; in words, Nk 1 stands for the total number of visits to (s,a) between the s-th frame
s−
≤ −
and the (k 1)-th frame. Lemma 5 tells us that, with probability at least 1 2δ,
− −
1
Nk 1 (k s)µ with µ = µ t , (112)
s− frame frame min frame
≥ − 2
which actually holds uniformly over all state-action pairs (s,a). Armed with this set of notation, it is
straightforwardto use the expression (110) to verify that
k 1
v tth+ktframe(s,a) ≤γ − η (1 −η)N sk−1 −1+(1 −η)N sk−1 −2+ ···+(1 −η)N sk +− 11 w
s
Xs=0 n o
k 1 k 1
=γ − (1 η)N sk +− 11 (1 η)N sk−1 w =:γ − (α α s)w s, (113)
s s+1
− − − −
Xs=0(cid:16) (cid:17) Xs=0
where we denote α :=(1 η)N sk−1 for any s k 1 and α :=1.
s k
− ≤ −
A litter algebra further leads to
k 1 k 1
− −
γ (α α )w =γ(α w α w )+γ α (w w ). (114)
s+1 s s k k 1 0 0 s s 1 s
− − − − −
s=0 s=1
X X
Thus, in order to control the quantity v (s,a), it suffices to control the right-hand side of (114), for
tth+ktframe
which we start by bounding the last term. Plugging in the definitions of w and α yields
s s
k 1 k 1 k 1
1 ∆− γ − α s(w −w s)= − (1 −η)N sk−1 (1 −ρ)s −1ρ ≤ρ − (1 −η)(k −s)µframe(1 −ρ)s −1,
s −1
0
k k∞ Xs=1 Xs=1 Xs=1
where the last inequality resuls from the fact (112). Additionally, direct calculation yields
k 1 k 1
− − 1 ρ s 1
ρ (1 η)(k −s)µframe(1 ρ)s −1 =ρ(1 η)(k −1)µframe − −
− − − (1 η)µframe
Xs=1 Xs=1(cid:16) − (cid:17)
31=ρ(1 −η)(k −1)µframe1 − −(1 − (11 η− ) 1µ ηρ −f )ra µρm fre ek −1
1(cid:0) am(cid:1)
−
η)µframe(1 −ρ)k −1 −(1 −η)(k −1)µframe
=ρ(1
− (1 ρ) (1 η)µframe
− − −
(1 ρ)k 1
ρ(1 η)µframe − − , (115)
≤ − (1 ρ) (1 η)µframe
− − −
where the last inequality makes use of the fact that
(1 ρ) (1 η)µframe =1 (1 γ)(1 (1 η)µframe) (1 η)µframe
− − − − − − − − −
γ
=γ 1 (1 η)µframe = ρ 0. (116)
{ − − } 1 γ ≥
−
Combining the inequalities (113), (114) and (115) and using the fact α w 0 give
0 0
≥
k 1
−
v (s,a) γ α (w w )+γα w
tth+ktframe s s −1 s k k −1
≤ −
s=1
X
∆ 0 γρ(1 η)µframe (1 −ρ)k −1 +γ(1 ρ)k −1 . (117)
∞
≤ (cid:13)1 (cid:13)γ − (1 −ρ) −(1 −η)µframe −
(cid:13) −(cid:13) (cid:26) (cid:27)
We are now ready to justify v (s,a) w . Note that the observation (116) implies
tth+ktframe k
≤
ρ(1 η)µframe ρ(1 η)µframe
γ − =γ − =(1 γ)(1 η)µframe.
(1 ρ) (1 η)µframe γ ρ − −
− − − 1 γ
−
This combined with the bound (117) yields
∆
v tth+ktframe(s,a) 0 (1 −γ)(1 −η)µframe(1 −ρ)k −1+γ(1 −ρ)k −1
γ∞
≤ 1
(cid:13) (cid:13)
(cid:13)∆−(cid:13)
(cid:8) (cid:9)
0 γ+(1 γ)(1 η)µframe (1 ρ)k −1
∞
≤ 1 γ − − −
(cid:13) (cid:13)
(cid:13) −(cid:13) (cid:0)∆ (cid:1)
=(1 ρ)k 0 =w , (118)
∞ k
− 1 γ
(cid:13) (cid:13)
(cid:13) −(cid:13)
where the last line follows from the definition of ρ (cf. (32d)). Since the above inequality holds for all
state-action pair (s,a), we conclude that
u = v w . (119)
tth+ktframe tth+ktframe k
≤
∞
(cid:13) (cid:13)
We have thus finished the proof for the case w(cid:13)hen t = t th(cid:13)+kt frame. As mentioned before, the case with
t=t +kt +j (j =1,...,t 1)canbejustifiedusingthesameargument. Asaconsequence,wehave
th frame frame
established the inequality (50) for− all t obeying t t− frat mt eh = k, which together with the induction argument
completes the proof of this lemma.
(cid:4) (cid:5)
E.5 Proof of Lemma 8
Recalling that 0 t t (I Λ )Λ 1 1 (cf. (46b)), we obtain
≤ i=1 j=i+1 − j i ≤
P Q
t t
h γ I Λ Λ (P P V γ (P P V . (120)
0,t j i
k k∞ ≤ (cid:13) (cid:13) (cid:13)Xi=1j= Yi+1 (cid:0) − (cid:1) (cid:13) (cid:13) (cid:13)1 (cid:13) (cid:13) e− (cid:1) (cid:13) (cid:13)∞ ≤ (cid:13) (cid:13) e− (cid:1) (cid:13) (cid:13)∞
As a result, it remains to upper bound (P P V .
−
∞
(cid:13) (cid:1) (cid:13)
(cid:13) e (cid:13)
32SupposethatP isconstructedusingN consecutivesampletransitions. Withoutlossofgenerality,assume
that these N sample transitions are the transitions between the following N +1 samples
e
(s ,a ),(s ,a ),(s ,a ), ,(s ,a ).
0 0 1 1 2 2 N N
···
Then the (s,a)-th row of P — denoted by P(s,a) — is given by
1 eN −1 1e 1 KN(s,a)
P(s,a)= P (s,a)V (s ,a )=(s,a) = P (s,a)V, (121)
i+1 i i ti+1
K (s,a) { } K (s,a)
N N
i=0 i=1
X X
e
where P is defined in (30), and P (s,a) denotes its (s,a)-th row. Here, K (s,a) denotes the total number
i i N
ofvisits to(s,a)duringthe firstN time instances(cf. (96)),andt :=t (s,a)denotesthe time stampwhen
k k
the trajectory visits (s,a) for the k-th time (cf (95)).
In view of our derivationfor (101), the state transitions happening at time t ,t , ,t are independent
1 2 k
···
for any given integer k >0. This together with the Hoeffding inequality implies that
1 k kτ2
P P (s,a) P(s,a) V τ 2exp . (122)
ti+1
(k (cid:12) − (cid:12) (cid:12)≥ )≤ (cid:26)−2 kV k2
(cid:12) (cid:12)Xi=1 (cid:12) ∞(cid:27)
(cid:0) (cid:1)
(cid:12) (cid:12)
Consequently, with probabi(cid:12)lity at least 1 δ one ha(cid:12)s
−
|S||A|
1 k 2log 2N |S||A|
P (s,a) P(s,a) V δ V , 1 k N.
(cid:12) (cid:12) (cid:12) (cid:12)k Xi=1 (cid:0) ti+1 − (cid:1) (cid:12) (cid:12) (cid:12) (cid:12)≤s (cid:0) k (cid:1) (cid:13) (cid:13) (cid:13) (cid:13)∞ ≤ ≤
Recognizingthe(cid:12)simpleboundK N(s,a) N,th(cid:12)eaboveinequalityholdsforeachstate-actionpair(s,a)when
≤
k isreplacedbyK (s,a). ConditioningontheseK (s,a),applyingthe unionboundoverall(s,a) ,
N N
∈S×A
we obtain
2log 2N |S||A|
(P P)V max δ V (123)
− ≤(s,a) ∈S×As K (cid:0)N(s,a)
∞ (cid:1) ∞
(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13) e (cid:13) (cid:13) (cid:13)
with probability at least 1 δ.
−
In addition, for any N t , Lemma 5 guarantees that with probability 1 2δ, each state-action pair
frame
≥ −
(s,a) is visited at least Nµ /2 times, namely, K (s,a) 1Nµ for all (s,a). This combined with (124)
min N ≥ 2 min
yields
4log 2N |S||A|
(P P)V δ V
− ∞ ≤s N (cid:0)µ min (cid:1) ∞
(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13) e (cid:13) 4log 2N |S||A| (cid:13) (cid:13)
δ V V⋆ + V⋆
≤s N (cid:0)µ −
min (cid:1) ∞ ∞
(cid:0)(cid:13) (cid:13) (cid:13) (cid:13) (cid:1)
4log 2N |S||A| (cid:13) (cid:13) (cid:13) 1 (cid:13) 4log 2N |S||A|
δ V V⋆ + δ (124)
≤s N (cid:0)µ − 1 −γs N (cid:0)µ
min (cid:1) ∞ min (cid:1)
(cid:13) (cid:13)
with probability at least 1 3δ, where the second i(cid:13)nequality(cid:13)follows from the triangle inequality, and the
−
last inequality follows from V⋆ 1 . Putting this together with (120) concludes the proof.
≤ 1 γ
∞ −
(cid:13) (cid:13)
E.6 Proof of Lemm(cid:13)a 6(cid:13)
For notational convenience, set t :=t l, and define
l cover
:= (s,a) that is not visited within t ,t
l l l+1
H ∃ ∈S×A
n (cid:0) (cid:3)o
33for any integer l 0. In view of the definition of t , we see that for any given (s,a) ,
cover ′ ′
≥ ∈S×A
1
P (s ,a )=(s,a) . (125)
{Hl tl tl ′ ′
| }≤ 2
Consequently, for any integer L>0, one can invoke the Markovianproperty to obtain
P =P P
1 L 1 L 1 L 1 L 1
{H ∩···∩H } {H ∩···∩H − } {H |H ∩···∩H − }
=P P (s ,a )=(s,a) P (s ,a )=(s,a)
{H1 ∩···∩HL −1 {HL tl tl ′ ′ tl tl ′ ′ |H1 ∩···∩HL −1
} | } { }
s′,a′
X
1
P P (s ,a )=(s,a)
{H1 ∩···∩HL −1 tl tl ′ ′ |H1 ∩···∩HL −1
≤ 2 } { }
s′,a′
X
1
= P ,
1 L 1
2 {H ∩···∩H − }
where the inequality follows from (125). Repeating this derivation recursively, we deduce that
1
P .
{H1 ∩···∩HL }≤ 2L
This tells us that
1 δ
P (s,a) that is not visited between (0,t ] P = ,
{∃ ∈S×A cover,all }≤ H1 ∩···∩Hlog 2 T δ ≤ 2log 2 T δ T
n o
which in turn establishes the advertised result by applying the union bound.
References
Agarwal, A., Kakade, S., and Yang, L. F. (2019). Model-based reinforcement learning with a generative
model is minimax optimal. arXiv preprint arXiv:1906.03804.
Azar, M. G., Munos, R., Ghavamzadeh, M., and Kappen, H. (2011). Reinforcement learning with a near
optimal rate of convergence. Technical report, INRIA.
Azar, M. G., Munos, R., and Kappen, H. J. (2013). Minimax PAC bounds on the sample complexity of
reinforcement learning with a generative model. Machine learning, 91(3):325–349.
Bai, Y., Xie, T., Jiang, N., and Wang, Y.-X. (2019). Provably efficient q-learning with low switching cost.
In Advances in Neural Information Processing Systems, pages 8002–8011.
Beck,C.L.andSrikant,R.(2012). Errorboundsforconstantstep-sizeQ-learning. Systems& controlletters,
61(12):1203–1208.
Bertsekas, D. P. (2017). Dynamic programming and optimal control (4th edition). Athena Scientific.
Bhandari, J., Russo, D., and Singal, R. (2018). A finite time analysis of temporal difference learning with
linear function approximation. In Conference On Learning Theory, pages 1691–1692.
Borkar, V. S. and Meyn, S. P. (2000). The ODE method for convergence of stochastic approximation and
reinforcement learning. SIAM Journal on Control and Optimization, 38(2):447–469.
Brémaud,P.(2013). Markov chains: Gibbs fields, Monte Carlo simulation, and queues,volume31. Springer
Science & Business Media.
Cai, Q., Yang, Z., Lee, J. D., and Wang, Z. (2019). Neural temporal-difference and q-learning converges to
global optima. In Advances in Neural Information Processing Systems, pages 11312–11322.
Chen, Z., Maguluri, S. T., Shakkottai, S., and Shanmugam, K. (2020). Finite-sample analysis of stochastic
approximation using smooth convex envelopes. arXiv preprint arXiv:2002.00874.
34Chen, Z.,Zhang,S., Doan,T.T., Maguluri,S.T., andClarke,J.-P.(2019). PerformanceofQ-learningwith
linear function approximation: Stability and finite-time analysis. arXiv preprint arXiv:1905.11425.
Dalal,G.,Szörényi,B.,Thoppe,G.,andMannor,S.(2018a). FinitesampleanalysesforTD(0)withfunction
approximation. In Thirty-Second AAAI Conference on Artificial Intelligence.
Dalal, G., Thoppe, G., Szörényi, B., and Mannor, S. (2018b). Finite sample analysis of two-timescale
stochasticapproximationwithapplicationstoreinforcementlearning. InConference On Learning Theory,
pages 1199–1233.
Dann, C. and Brunskill, E. (2015). Sample complexity of episodic fixed-horizon reinforcement learning. In
Advances in Neural Information Processing Systems, pages 2818–2826.
Devraj, A. M. and Meyn, S. P. (2020). Q-learning with uniformly bounded variance: Large discounting is
not a barrier to fast learning. arXiv preprint arXiv:2002.10301.
Doan, T., Maguluri, S., and Romberg, J. (2019). Finite-time analysis of distributed TD(0) with linear
function approximation on multi-agent reinforcement learning. In International Conference on Machine
Learning, pages 1626–1635.
Doan,T.T.,Nguyen,L.M.,Pham,N.H.,andRomberg,J.(2020). Convergenceratesofacceleratedmarkov
gradient descent with applications in reinforcement learning. arXiv preprint arXiv:2002.02873.
Du, S. S., Chen, J., Li, L., Xiao, L., and Zhou, D. (2017). Stochastic variance reduction methods for policy
evaluation. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages
1049–1058.JMLR. org.
Du, S. S., Lee, J. D., Mahajan,G., and Wang, R. (2020). Agnostic Q-learning with function approximation
in deterministic systems: Tight bounds on approximation error and sample complexity. arXiv preprint
arXiv:2002.07125.
Du, S. S., Luo, Y., Wang, R., and Zhang,H. (2019). Provably efficientQ-learningwith function approxima-
tion via distribution shift error checking oracle. In Advances in Neural Information Processing Systems,
pages 8058–8068.
Dulac-Arnold, G., Mankowitz, D., and Hester, T. (2019). Challenges of real-world reinforcement learning.
arXiv preprint arXiv:1904.12901.
Even-Dar,E.andMansour,Y.(2003). LearningratesforQ-learning. Journal of machine learning Research,
5(Dec):1–25.
Fan, J., Wang, Z., Xie, Y., and Yang, Z. (2019). A theoretical analysis of deep Q-learning. arXiv preprint
arXiv:1901.00137.
Ghavamzadeh, M., Kappen, H. J., Azar, M. G., and Munos, R. (2011). Speedy Q-learning. In Advances in
neural information processing systems, pages 2411–2419.
Gupta, H., Srikant, R., and Ying, L. (2019). Finite-time performance bounds and adaptive learning rate
selectionfortwotime-scalereinforcementlearning.InAdvancesinNeuralInformationProcessingSystems,
pages 4706–4715.
Jaakkola,T., Jordan, M. I., and Singh, S. P. (1994). Convergence of stochastic iterative dynamic program-
ming algorithms. In Advances in neural information processing systems, pages 703–710.
Jin, C., Allen-Zhu, Z., Bubeck, S., and Jordan, M. I. (2018). Is Q-learning provably efficient? In Advances
in Neural Information Processing Systems, pages 4863–4873.
Johnson,R. andZhang,T.(2013). Accelerating stochasticgradientdescent using predictive variancereduc-
tion. In Advances in neural information processing systems, pages 315–323.
35Kaledin, M., Moulines, E., Naumov, A., Tadic, V., and Wai, H.-T. (2020). Finite time analysis of linear
two-timescale stochastic approximationwith Markoviannoise. arXiv preprint arXiv:2002.01268.
Kearns,M.J.andSingh,S.P.(1999).Finite-sampleconvergenceratesforQ-learningandindirectalgorithms.
In Advances in neural information processing systems, pages 996–1002.
Khamaru,K., Pananjady,A., Ruan, F., Wainwright,M.J., andJordan,M. I.(2020). Is temporaldifference
learning optimal? an instance-dependent analysis. arXiv preprint arXiv:2003.07337.
Lee, D. and He, N. (2019). Target-based temporal difference learning. arXiv preprint arXiv:1904.10945.
Li, G., Wei, Y., Chi, Y., Gu, Y., and Chen, Y. (2020). Breaking the sample size barrier in model-based
reinforcement learning with a generative model. arXiv preprint arXiv:2005.12900, accepted to Neural
Information Processing Systems.
Lin, Y., Qu, G., Huang, L., and Wierman, A. (2020). Distributed reinforcement learning in multi-agent
networked systems. arXiv preprint arXiv:2006.06555.
Mnih,V.,Kavukcuoglu,K.,Silver,D.,Rusu,A.A.,Veness,J.,Bellemare,M.G.,Graves,A.,Riedmiller,M.,
Fidjeland, A. K., Ostrovski, G., et al. (2015). Human-level control through deep reinforcement learning.
Nature, 518(7540):529–533.
Mou, W., Li, C. J., Wainwright, M. J., Bartlett, P. L., and Jordan, M. I. (2020). On linear stochas-
tic approximation: Fine-grained Polyak-Ruppert and non-asymptotic concentration. arXiv preprint
arXiv:2004.04719.
Paulin, D. (2015). Concentrationinequalities for Markovchains by Martoncouplings andspectralmethods.
Electronic Journal of Probability, 20.
Qu, G. and Wierman, A. (2020). Finite-time analysis of asynchronous stochastic approximation and Q-
learning. Conference on Learning Theory.
Shah, D. and Xie, Q. (2018). Q-learning with nearest neighbors. In Advances in Neural Information
Processing Systems, pages 3111–3121.
Sidford,A.,Wang,M.,Wu,X.,Yang,L.,andYe,Y.(2018a). Near-optimaltimeandsamplecomplexitiesfor
solvingMarkovdecisionprocesseswithagenerativemodel. InAdvances in Neural Information Processing
Systems, pages 5186–5196.
Sidford, A., Wang, M., Wu, X., and Ye, Y. (2018b). Variance reduced value iteration and faster algorithms
forsolvingMarkovdecisionprocesses.InProceedings oftheTwenty-NinthAnnualACM-SIAMSymposium
on Discrete Algorithms, pages 770–787.SIAM.
Srikant, R. and Ying, L. (2019). Finite-time error bounds for linear stochastic approximation and TD
learning. In Conference on Learning Theory, pages 2803–2830.
Strehl, A. L., Li, L., Wiewiora, E., Langford, J., and Littman, M. L. (2006). PAC model-free reinforcement
learning. In Proceedings of the 23rd international conference on Machine learning, pages 881–888.
Sun, T., Sun, Y., Xu, Y., and Yin, W. (2020). Markov chain block coordinate descent. Computational
Optimization and Applications, pages 1–27.
Sutton, R. S. (1988). Learningto predictby the methods oftemporaldifferences. Machine learning, 3(1):9–
44.
Szepesvári, C. (1998). The asymptotic convergence-rate of Q-learning. In Advances in Neural Information
Processing Systems, pages 1064–1070.
Tsitsiklis,J.N.(1994).AsynchronousstochasticapproximationandQ-learning.Machinelearning,16(3):185–
202.
36Wainwright,M.J.(2019a). Stochasticapproximationwithcone-contractiveoperators: Sharpℓ -boundsfor
∞
Q-learning. arXiv preprint arXiv:1905.06265.
Wainwright, M. J. (2019b). Variance-reduced Q-learning is minimax optimal. arXiv preprint
arXiv:1906.04697.
Wang, Y., Dong, K., Chen, X., and Wang, L. (2020). Q-learning with UCB exploration is sample efficient
for infinite-horizon MDP. In International Conference on Learning Representations.
Watkins, C. J. and Dayan, P. (1992). Q-learning. Machine learning, 8(3-4):279–292.
Watkins, C. J. C. H. (1989). Learning from delayed rewards.
Weng,B.,Xiong,H.,Zhao,L.,Liang,Y., andZhang,W.(2020a). MomentumQ-learningwithfinite-sample
convergence guarantee. arXiv preprint arXiv:2007.15418.
Weng,W.,Gupta,H.,He,N.,Ying,L.,andSrikant,R.(2020b). Provably-efficientdoubleQ-learning. arXiv
preprint arXiv:2007.05034.
Xu, P. andGu, Q.(2020). A finite-time analysis ofQ-learningwith neuralnetworkfunction approximation.
accepted to International Conference on Machine Learning.
Xu, T., Wang, Z., Zhou, Y., and Liang, Y. (2020). Reanalysis of variance reduced temporal difference
learning. ICLR, arXiv preprint arXiv:2001.01898.
Xu, T., Zou, S., and Liang, Y. (2019). Two time-scale off-policy TD learning: Non-asymptotic analysis over
Markoviansamples. In Advances in Neural Information Processing Systems, pages 10633–10643.
Yang, L. and Wang, M. (2019). Sample-optimal parametric Q-learning using linearly additive features. In
International Conference on Machine Learning, pages 6995–7004.
Zou,S.,Xu,T.,andLiang,Y.(2019). Finite-sampleanalysisforSARSAwithlinearfunctionapproximation.
In Advances in Neural Information Processing Systems, pages 8665–8675.
37