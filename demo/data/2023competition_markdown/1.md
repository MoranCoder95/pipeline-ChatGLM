SG-NN: Sparse Generative Neural Networks for
Self-Supervised Scene Completion of RGB-D Scans
AngelaDai ChristianDiller MatthiasNießner
TechnicalUniversityofMunich
0202
raM
42
]VC.sc[
Figure1: OurmethodtakesasinputapartialRGB-Dscanandpredictsahigh-resolution3Dreconstructionwhilepredicting
unseen, missing geometry. Key to our approach is its self-supervised formulation, enabling training solely on real-world,
2v63000.2191:viXra incompletescans. Thisnotonlyobviatestheneedforsyntheticgroundtruth,butisalsocapableofgeneratingmorecomplete
scenesthananysingletargetsceneseenduringtraining. Toachievehigh-qualitysurfaces,wefurtherproposeanewsparse
generativeneuralnetwork,capableofgeneratinglarge-scalescenesatmuchhigherresolutionthanexistingtechniques.
Abstract 1.Introduction
We present a novel approach that converts partial and In recent years, we have seen incredible progress on
noisyRGB-Dscansintohigh-quality3Dscenereconstruc- RGB-D reconstruction of indoor environments using com-
tions by inferring unobserved scene geometry. Our ap- modity RGB-D sensors such as the Microsoft Kinect,
proach is fully self-supervised and can hence be trained Google Tango, or Intel RealSense [22, 16, 23, 36, 5,
solely on real-world, incomplete scans. To achieve self- 10]. However, despite remarkable achievements in RGB-
supervision, we remove frames from a given (incomplete) D tracking and reconstruction quality, a fundamental chal-
3D scan in order to make it even more incomplete; self- lengestillremains–theincompletenatureofresulting3D
supervisionisthenformulatedbycorrelatingthetwolevels scanscausedbyinherentocclusionsduetothephysicallim-
of partialness of the same scan while masking out regions itationsofthescanningprocess;i.e.,eveninacarefulscan-
that have never been observed. Through generalization ningsessionitisinevitablethatsomeregionsofa3Dscene
across a large training set, we can then predict 3D scene remainunobserved.Thisunfortunatelyrenderstheresulting
completionwithouteverseeingany3Dscanofentirelycom- reconstructions unsuitable for many applications, not only
plete geometry. Combined with a new 3D sparse genera- thosethatrequirequality3Dcontent, suchasvideogames
tiveneuralnetworkarchitecture,ourmethodisabletopre- orAR/VR,butalsoroboticswhereacompleted3Dmapsig-
dict highly-detailed surfaces in a coarse-to-fine hierarchi- nificantlyfacilitatestaskssuchasgraspingorquerying3D
calfashion, generating3Dscenesat2cmresolution, more objectsina3Denvironment.
than twice the resolution of existing state-of-the-art meth- In order to overcome the incomplete and partial nature
odsaswellasoutperformingthembyasignificantmargin of 3D reconstructions, various geometric inpainting tech-
inreconstructionquality.1 niqueshavebeenproposed,forinstance,surfaceinterpola-
tionbasedonthePoissonequation[17,18]orCADshape-
1Sourcecodeavailablehere. fittingtechniques[1,2,8].Averyrecentdirectionleveragesgenerative deep neural networks, often focusing volumet- 2.RelatedWork
ricrepresentationsforshapes[11]orentirescenes[31,12].
RGB-DReconstruction Scanningandreconstructing3D
These techniques show great promise since they can learn
surfaces has a long history across several research com-
generalized patterns in a large variety of environments;
munities. With the increase in availability of commod-
however, existing data-driven scene completion methods
ity range sensors, capturing and reconstructing 3D scenes
relyonsupervisedtraining,requiringfullycompleteground
has become a vital area of research. One seminal tech-
truth 3D models, thus depending on large-scale synthetic
nique is the volumetric fusion approach of Curless and
datasets such as ShapeNet [4] or SUNCG [31]. As a re-
Levoy [7], operating on truncated signed distance fields
sult, although we have seen impressive results from these
to produce a surface reconstruction. It has been adopted
approaches on synthetic test sets, domain transfer and ap-
by many state-of-the-art real-time reconstruction methods,
plicationtoreal-world3Dscansremainsamajorlimitation.
from KinectFusion [22, 16] to VoxelHashing [23] and
Inordertoaddresstheshortcomingsofsupervisedlearn-
BundleFusion[10],aswellasstate-of-the-artofflinerecon-
ingtechniquesforscancompletion,weproposeanewself-
structionapproaches[5].
supervisedcompletionformulationthatcanbetrainedonly
These methods have produced impressive results in
on(partial)real-worlddata.Ourmainideaistolearntogen-
tracking and scalability of 3D reconstruction from com-
erate more complete 3D models from less complete data,
modityrangesensors. However,asignificantlimitationthat
while masking out any unknown regions; that is, from an
stillremainsisthepartialnatureof3Dscanning;i.e.,aper-
existingRGB-Dscan,weusethescanasthetargetandre-
fect scan is usually not possible due to occlusions and un-
moveframestoobtainamoreincompleteinput. Intheloss
observed regions and thus, the resulting 3D representation
function,wecannowcorrelatethedifferenceinpartialness
cannotreachthequalityofmanuallycreated3Dassets.
betweenthetwoscans,andconstrainthenetworktopredict
the delta while masking out unobserved areas. Although
there is no single training sample which contains a fully- Deep Learning on 3D Scans With recent advances in
complete3Dreconstruction,weshowthatournetworkcan deep learning and the improved availability of large-scale
nonetheless generalize to predict high levels of complete- 3D scan datasets such as ScanNet [9] or Matterport [3],
nessthroughacombinedaggregationofpatternsacrossthe learned approaches on 3D data can be used for a variety
entire training set. This way, our approach can be trained oftaskslikeclassification,segmentation,orcompletion.
withoutrequiringanyfully-completegroundtruthcounter- Many current methods make use of convolutional op-
parts that would make generalization through a synthetic- erators that have been shown to work well on 2D data.
to-realdomaingapchallenging. When extended into 3D, they operate on regular grid rep-
Furthermore,weproposeanewsparsegenerativeneural resentationssuchasdistancefields[11]oroccupancygrids
networkarchitecturethatcanpredicthigh-resolutiongeom- [20]. Since dense volumetric grids can come with high
etryinafully-convolutionalfashion. Fortraining, wepro- computationalandmemorycosts,severalrecentapproaches
pose a progressively growing network architecture trained have leveraged the sparsity of the 3D data for discrim-
incoarse-to-finefashion; i.e.,wefirstpredictthe3Dscene inative 3D tasks. PointNet [26, 27] introduced a deep
atalowresolution,andthencontinueincreasingthesurface network architecture for learning on point cloud data for
resolution through the training process. We show that our semantic segmentation and classification tasks. Octree-
self-supervised,sparsegenerativeapproachcanoutperform based approaches have also been developed [29, 33, 34]
state-of-the-art fully-supervised methods, despite their ac- that have been shown to be very memory efficient; how-
cesstomuchlargerquantitiesofsynthetic3Ddata. ever, generative tasks involving large, varying-sized envi-
Wepresentthefollowingmaincontributions: ronmentsseemschallengingandoctreegenerationhasonly
beenshownforsingleShapeNet-styleobjects[28,32]. An-
other option leveraging the sparsity of 3D geometric data
• A self-supervised approach for scene completion, en-
isthroughsparseconvolutions[14,13,6],whichhaveseen
abling training solely on incomplete, real-world scan
successindiscriminativetaskssuchassemanticsegmenta-
data while predicting geometry more complete than
tion,butnotinthecontextofgenerative3Dmodelingtasks,
any seen during training, by leveraging common pat-
wheretheoverallstructureofthesceneisunknown.
ternsinthedeltasofincompleteness.
• A generative formulation for sparse convolutions to ShapeandSceneCompletion Completing3Dscanshas
produce a sparse truncated signed distance function been well-studied in geometry processing. Traditional
representation at high resolution: we formulate this methods,suchasPoissonSurfaceReconstruction[17,18],
hierarchicallytoprogressivelygeneratea3Dscenein locallyoptimizeforasurfacetofittoobservedpoints,and
end-to-endfashion work well for small missing regions. Recently, variousdeep learning-based approaches have been developed with formulatethetrainingfromincompletescandatatolessin-
greatercapacityforlearningglobalstructuresofshapes,en- complete scan data; that is, from an existing RGB-D scan
abling compelling completion of larger missing regions in wecanremoveframesinordertocreateamorepartialob-
scans of objects [37, 11, 15, 35, 24]. Larger-scale com- servationofthescene. Thisenableslearningtocompletein
pletion of scans has been seen with SSCNet [31], operat- regions where scan geometry is known while ignoring re-
ing on a depth image of a scene, and ScanComplete [12], gionsofunobservedspace. Crucially,ourgenerativemodel
whichdemonstratedscenecompletiononroom-andbuild- canthenlearntogeneratemorecompletemodelsthanseen
ing floor-scale scans. However, both these approaches op- inaspecificsampleofthetargetdata.
erateondensevolumetricgrids,significantlylimitingtheir Toobtainanoutputhigh-resolution3Dmodelofascene,
output resolutions. Moreover, these approaches are fully weproposeSparseGenerativeNeuralNetworks(SG-NN),a
supervisedwithcomplete3Dscenedata,requiringtraining generativemodeltoproduceasparsesurfacerepresentation
onsynthetic3Dscenedata(wherecompletegroundtruthis ofascene. Webuilduponsparseconvolutions[14,13,6],
known),inordertocompletereal-worldscans. which have been shown to produce compelling semantic
An alternative approach for shape completion could segmentation results on 3D scenes by operating only on
through leveraging a single implicit latent space, as in surface geometry. In contrast to these discriminative tasks
DeepSDF [24] or Occupancy Networks [21]; however, it where the geometric structure is given as input, we de-
still remains a challenge as to how to scale a single latent velop our SG-NN to generate new, unseen 3D geometry
spacetorepresentlarge,varying-sizedenvironments. suitableforgenerative3Dmodelingtasks. Thisisdesigned
incoarse-to-finefashion,withaprogressivelygrowingnet-
3.MethodOverview work architecture which predicts each next higher resolu-
tion,finallypredictingahigh-resolutionsurfaceasasparse
FromanRGB-Dscanofa3Dscene,ourmethodlearns
TSDF. Since our sparse generative network operates in a
togenerateahigh-qualityreconstructionofthecomplete3D
fully-convolutionalfashion,wecanoperateon3Dscansof
scene,inaself-supervisedfashion. TheinputRGB-Dscan
varyingspatialsizes.
is representedas atruncated signed distancefield (TSDF),
asasparsesetofvoxellocationswithintruncationandtheir
4.Self-SupervisedCompletion
corresponding distance values. The output complete 3D
modelofthesceneisalsogeneratedasasparseTSDF(simi- Ourapproachforself-supervisionofscenecompletionof
larly,asetoflocationsandper-voxeldistances),fromwhich RGB-Dscansisbasedonlearninghowtocompletescange-
ameshcanbeextractedbyMarchingCubes[19]. ometryinregionsthathavebeenseen,whileignoringunob-
Wedesignthe3Dscenecompletionasaself-supervised servedregions.Tothisend,wecangenerateinputandtarget
process, enabling training purely on real-world scan data TSDFs with similar scanning patterns as real-world scans;
without requiring any fully-complete ground truth scenes. fromaninputscancomposedofRGB-Dframes{f ,...f },
0 n
Sincereal-worldscansarealwaysincompleteduetoocclu- wecangeneratethetargetTSDFS throughvolumet-
target
sions and physical sensor limitations, this is essential for ric fusion [7] of {f ,...f }, and the input TSDF S
0 n input
generating high-quality, complete models from real-world throughvolumetricfusionofasubsetoftheoriginalframes
scan data. Toachieve self-supervision, our main idea is to {f }⊂{f ,...f }.
k 0 n
Figure2: Ourself-supervisionapproachforscancompletionlearnsthroughdeltasinpartialnessofRGB-Dscans. Froma
given(incomplete)RGB-Dscan,ontheleft,weproduceamoreincompleteversionofthescanbyremovingsomeofitsdepth
frames(middle). Wecanthentraintocompletethemoreincompletescan(middle)usingtheoriginalscanasatarget(left),
whilemaskingoutunobservedregionsinthetargetscene(inorange). Thisenablesourpredictiontoproducescenesthatare
morecompletethanthetargetscenesseenduringtraining,asthetrainingprocesseffectivelymasksoutincompleteness.This produces input incomplete scans that maintain generateamoreincompleteversionofthescanS .
input
scanned data characteristics, as well as a correspondence Attraintime,weconsidercroppedviewsofthesescans
between S input and S target going from a more incomplete forefficiency,usingrandomcropsofsize64×64×128vox-
scan to a less incomplete scan. Since S target remains els. Thefully-convolutionalnatureofourapproachenables
nonethelessincomplete,wedonotwishtouseallofitsdata testingonfullscenesofvaryingsizesatinferencetime.
as the complete target for supervision, as this could result
in contradictory signals in the training set (e.g., table legs
5. Generating a Sparse 3D Scene Representa-
have been seen in one scan but not in another, then it be-
tion
comesunclearwhethertogeneratetablelegs).
Thus, to effectively learn to generate a complete 3D The geometry of a 3D scene occupies a very sparse set
modelbeyondeventhecompletenessofthetargettraining ofthetotal3Dextentofthescene,soweaimtogeneratea
data, we formulate the completion loss only on observed 3D representation of a scene in a similarly sparse fashion.
regions in the target scan. That is, the loss is only con- ThusweproposeSparseGenerativeNeuralNetworks(SG-
sidered in regions where S target(v) > −τ, for a voxel v NN) to hierarchically generate a sparse, truncated signed
withτ indicatingthevoxelsize. Figure2showsanexam- distancefieldrepresentationofa3Dscene,fromwhichwe
pleS input,S target,andprediction,withthisself-supervision canextracttheisosurfaceasthefinaloutputmesh.
setup,wecanlearntopredictgeometrythatwasunobserved
An overview of our network architecture for the scene
inS ,e.g.,occludedregionsbehindobjects.
target completion task is shown in Figure 3. The model is de-
signed in encoder-decoder fashion, with an input partial
4.1.DataGeneration
scan first encoded to representative features at low spatial
As input we consider an RGB-D scan comprising a set resolution,beforegeneratingthefinalTSDFoutput.
of depth images and their 6-DoF camera poses. For real- A partial scan, represented as a TSDF, is encoded with
worldscandataweusetheMatterport3D[3]dataset,which a series of 3D sparse convolutions [14, 13] which operate
containsavarietyofRGB-DscanstakenwithaMatterport only on the locations where the TSDF is within trunca-
tripodsetup. NotethatforMatterport3D,wetrainandeval- tiondistanceandusingthedistancevaluesasinputfeatures.
uateontheannotatedroomregions,whereastherawRGB- Eachsetofconvolutionsspatiallycompressesthesceneby
Ddataisasequencecoveringmanydifferentrooms,sowe afactoroftwo. Ourgenerativemodeltakestheencodingof
performanapproximateframe-to-roomassociationbytak- the scene and converts the features into a (low-resolution)
ingframeswhosecameralocationsliewithintheroom. dense3Dgrid. Thedenserepresentationenablesprediction
FromagivenRGB-Dscan,weconstructthetargetscan of the full scene geometry at very coarse resolution; here,
S using volumetric fusion [7] with 2cm voxels and weuseaseriesofdense3Dconvolutionstoproduceafea-
target
truncation of 3 voxels. A subset of the frames is taken by turemapF fromwhichwealsopredictcoarseoccupancy
0
randomly removing ≈ 50% of the frames (see Section 6 O and TSDF S representations of the complete scene.
0 0
for more analysis of varying degrees of incompleteness in We then construct a sparse representation of the predicted
S ,S ). Wecanthenagainusevolumetricfusionto scene based on O : the features input to the next level are
input target 0
Figure 3: Our Sparse Generative Neural Network architecture for the task of scan completion. An input scan is encoded
usingaseriesofsparseconvolutions,eachsetreducingthespatialdimensionsbyafactoroftwo. Togeneratehigh-resolution
scenegeometry,thecoarseencodingisconvertedtoadenserepresentationforacoarsepredictionofthecompletegeometry.
Thepredictedcoarsegeometryisconvertedtoasparserepresentationandinputtooursparse,coarse-to-finehierarchy,where
each level of the hierarchy predicts the geometry of the next resolution (losses indicated in orange). The final output is a
TSDFrepresentedbysparsesetofvoxellocationsandtheircorrespondingdistancevalues.composedasconcat(F ,O ,S )∀sigmoid(O (v))>0.5. 5.1.Training
k k k k
This can then be processed with sparse convolutions, then
WetrainourSG-NNonasingleNVIDIAGeForceRTX
upsampled by a factor of two to predict the scene geome-
2080, using the Adam optimizer with a learning rate of
try at the next higher resolution. This enables generative,
0.001andbatchsizeof8. WeuseN = 2000iterations
sparsepredictionsinahierarchicalfashion. Togeneratethe level
for progressive introduction of each higher resolution out-
final surface geometry, the last hierarchy level of our SG-
put,andtrainourmodelfor≈40hoursuntilconvergence.
NNoutputssparseO ,S ,andF ,whicharetheninputto
n n n
a final set of sparse convolutions to refine and predict the
6.ResultsandEvaluation
outputsigneddistancefieldvalues.
We evaluate our sparse generative neural network on
Sparseskipconnections. Forscenecompletion,wealso scene completion for RGB-D scans on both real-world
leverageskipconnectionsbetweentheencoderanddecoder scanswherenofullycompletegroundtruthisavailable[3],
parts of the network architecture, connecting feature maps as well as in a supervised setting on synthetic scans
of same spatial resolution. This is in the same spirit as U- which have complete ground truth information [31]. We
Net [30], but in our case the encoder and decoder features use the train/test splits provided by both datasets: 72/18
mapsarebothsparseandtypicallydonotcontainthesame and5519/155trainval/testscenescomprising1788/394and
setofsparselocations. Thusweconcatenatefeaturesfrom 39600/1000 rooms, respectively. To measure completion
thesetofsourcelocationswhicharesharedwiththedesti- quality,wefollow[12]andusean(cid:96) errormetricbetween
1
nationlocations,andusezerofeaturevectorsforthedesti- predicted and target TSDFs, where unobserved regions in
nationlocationswhichdonotexistinthesource. thetargetaremaskedout. Notethatunsigneddistancesare
usedintheerrorcomputationtoavoidsignambiguities.We
ProgressiveGeneration. Inordertoencouragemoreeffi- measurethe(cid:96) 1 distanceinvoxelunitsoftheentirevolume
cientandstabletraining,wetrainourgenerativemodelpro- (entirevolume),theunobservedregionofthevolume(unob-
gressively,startingwiththelowestresolution,andintroduc- servedspace),nearthetargetsurface(target),andnearthe
ing each successive hierarchy level after N iterations. predicted surface (predicted), using a threshold of ≤ 1 to
level
Each hierarchy level predicts the occupancy and TSDF of determinenearbyregions,andaglobaltruncationof3. For
the next level, enabling successive refinement from coarse all metrics, unobserved regions in the targets are ignored;
predictions,asshowninFigure4. note that on syntheticdata where complete ground truth is
available,wedonothaveanyunobservedregionstoignore.
Loss. We formulate the loss for the generated scene ge-
ometry on the final predicted TSDF locations and values, Comparisontostateoftheart. InTable1,wecompare
usingan(cid:96) losswiththetargetTSDFvaluesatthoseloca- to several state-of-the-art approaches for scan completion
1
tions.Following[11],welog-transformtheTSDFvaluesof onreal-worldscansfromtheMatterport3Ddataset[3]: the
the predictions and the targets before applying the (cid:96) loss, shape completion approach 3D-EPN [11], and the scene
1
inordertoencouragemoreaccuratepredictionnearthesur- completion approach ScanComplete [12]. These methods
facegeometry.Weadditionallyemployproxylossesateach both require fully-complete ground truth data for supervi-
hierarchy level for outputs O and S , using binary cross sion, which is not available for the real-world scenes, so
k k
entropy with target occupancies and (cid:96) with target TSDF we train them on synthetic scans [31]. Since 3D-EPN and
1
values, respectively. This helps avoid a trivial solution of ScanComplete use dense 3D convolutions, limiting voxel
zero loss for the final surface with no predicted geometry. resolution, we use 5cm resolution for training and evalua-
Note that for our self-supervised completion, we compute tion of all methods. Our self-supervised approach enables
these losses only in regions of observed target values, as training on incomplete real-world scan data, avoiding do-
describedinSection4. maintransferwhileoutperformingpreviousapproachesthat
leverage large amounts of synthetic 3D data. Qualitative
comparisonsareshowninFigure5.
To evaluate our SG-NN separate from its self-
supervision, we also evaluate synthetic scan completion
with full ground truth [31], in comparison to Poisson Sur-
face Reconstruction [17, 18], SSCNet[31], 3D-EPN [11],
and ScanComplete [12]. All data-driven approaches are
Figure 4: Progressive generation of a 3D scene using our fullysupervised,usinginputscansfrom[12].Similartothe
SG-NN which formulates a generative model to predict a realscanscenario, wetrainandevaluateat5cmresolution
sparseTSDFasoutput. duetoresolutionlimitationsofthepriorlearnedapproaches.Figure5: Comparisontostate-of-the-artscancompletionapproachesonMatterport3D[3]data(5cmresolution),withinput
scans generated from a subset of frames. In contrast to the fully-supervised 3D-EPN [11] and ScanComplete [12], our
self-supervisedapproachproducesmoreaccurate,completescenegeometry.
Method (cid:96) error (cid:96) error (cid:96) error (cid:96) error
1 1 1 1
entirevolume unobservedspace target predicted
3D-EPN(unet)[11] 0.31 0.28 0.45 1.12
ScanComplete[12] 0.20 0.15 0.51 0.74
Ours 0.17 0.14 0.35 0.67
Table1: Quantitativescancompletionresultsonreal-worldscandata[3],with(cid:96) distancemeasuredinvoxelunitsfor5cm
1
voxels. Since target scans are incomplete, unobserved space in the target is masked out for all metrics. 3D-EPN [11] and
ScanComplete[12]requirefullsupervision,andsoaretrainedonsyntheticdata[31]. Despitetheiraccesstolargequantities
ofsynthetic3Ddata,ourself-supervisedapproachoutperformsthesemethodswhiletrainingsolelyonreal-worlddata.
InTable2,weseethatoursparsegenerativeapproachout- leveragingknowledgeofobservedandunobservedspacein
performsstateoftheartinafully-supervisedscenario. RGB-D scans. To evaluate the completion quality of our
method against the completeness of the target scene data,
weperform aqualitative evaluation, aswe lackfully com-
Can self-supervision predict more complete geome- pletegroundtruthtoforquantitativeevaluation.InFigure7,
try than seen during training? Our approach to self- we see that our completion quality can exceed the com-
supervision is designed to enable prediction of scene ge- pletenessoftargetscenedata. Weadditionallyevaluateour
ometrybeyondthecompletenessofthetargetscandata,byMethod (cid:96) error (cid:96) error (cid:96) error (cid:96) error
1 1 1 1
entirevolume unobservedspace target predicted
PoissonSurfaceReconstruction[17,18] 0.53 0.51 1.70 1.18
SSCNet[31] 0.54 0.53 0.93 1.11
3D-EPN(unet)[11] 0.25 0.30 0.65 0.47
ScanComplete[12] 0.18 0.23 0.53 0.42
Ours 0.15 0.16 0.50 0.28
Table2: Quantitativescancompletionresultsonsyntheticscandata[31],wherecompletegroundtruthisavailabletosuper-
visealldata-drivenapproaches. (cid:96) distanceismeasuredinvoxelunitsfor5cmvoxels.
1
approachwithandwithoutourself-supervisionmaskingin
Figure7,wherew/oself-supervisionmaskingistrainedus-
ingthesamesetofless-incomplete/more-incompletescans
but without the loss masking. This can perform effective
completion in regions commonly observed in target scans,
but often fails to complete regions that are commonly oc-
cluded. Incontrast,ourformulationforself-supervisionus-
ingmaskingofunobservedregionsenablespredictingscene
geometryevenwherethetargetscanremainsincomplete.
Figure 6: Evaluating varying target data completeness
available for training. We generate various incom-
Comparison of our self-supervision approach to mask-
plete versions of the Matterport3D [3] scans using ≈
ingoutbyrandomcrops. InTable3,weevaluateagainst
30%,40%,50%,60%,and100%(all)oftheframesassoci-
anotherpossibleself-supervisionapproach:randomlycrop-
atedwitheachroomscene,andevaluateonthe50%incom-
ping out target geometry to be used as incomplete inputs
pletescans. Ourself-supervisedapproachremainsrobustto
(usingcropsforself-supervision),similarto[25]. Thissce-
thelevelofcompletenessofthetargettrainingdata.
nario does not reflect the data characteristics of real-world
scan partialness (e.g., from occlusions and lack of visibil-
ity),resultinginpoorcompletionperformance. the Matterport3D [3] scans using varying amounts of the
frames available: ≈ 30%,40%,50%,60%, and 100% (all)
of the frames associated with each room scene. We train
What’s the impact of the input/output representation?
our approach using three different versions of input-target
In Table 3, we evaluate the effect of a point cloud input
completeness: 50%−all (our default), 40%−60%, and
(vs. TSDFinput), aswellasoccupancyoutput(vs. TSDF
30%−50%. Even as the completeness of the target data
output). We find that the TSDF representation has more
decreases,ourapproachmaintainsrobustnessinpredicting
potentialdescriptivenessincharacterizingasurface(andits
completescenegeometry.
neighboringregions),resultinginimprovedperformancein
bothinputandoutputrepresentation.
Limitations Our SG-NN approach for self-supervised
What’s the impact of the degree of completeness of the scancompletionenableshigh-resolutiongeometricpredic-
targetdataduringtraining? InFigure6,weevaluatethe tion of complete geometry from real-world scans. How-
effectoftheamountofcompletenessofthetargetdataavail- ever, to generate the full appearance of a 3D scene, gen-
ablefortraining. Wecreateseveralincompleteversionsof eration and inpainting of color is also required. Currently,
Method (cid:96) error (cid:96) error (cid:96) error (cid:96) error
1 1 1 1
entirevolume unobservedspace target predicted
Usingcropsforself-supervision 0.13 0.09 1.25 0.68
Pointcloudinput 0.15 0.09 1.82 0.92
Occupancyoutput 0.13 0.10 0.89 0.86
2hierarchylevels 0.10 0.08 0.74 0.68
Ours 0.09 0.07 0.71 0.60
Table 3: Ablation study of our self-supervision and generative model design choices on real-world scan data [3], with (cid:96)
1
distancemeasuredinvoxelunitsfor2cmvoxels.our method also does not consider or predict the semantic faceenablesmuchhigheroutputgeometricresolutionthan
object decomposition of a scene; however, we believe this previousonlarge-scale3Dscenes. Self-supervisionallow-
wouldbeaninterestingdirection,specificallyinthecontext ing training only on real-world scan data for scan comple-
for enabling interaction with a 3D environment (e.g., inte- tion opens up new possibilities for various generative 3D
riorredesignorroboticunderstanding). modeling based only on real-world observations, perhaps
mitigatingthe need forextensive syntheticdata generation
7.Conclusion or domain transfer, and we believe this is a promising av-
enueforfutureresearch.
In this paper, we presented a self-supervised approach
forcompletionofRGB-Dscangeometrythatenablestrain-
Acknowledgments
ing solely on incomplete, real-world scans while learning
a generative geometric completion process capable of pre- This work was supported by the ZD.B, a Google Re-
dicting 3D scene geometry more complete than any single searchGrant,aTUM-IASRudolfMo¨ßbauerFellowship,an
targetsceneseenduringtraining. Oursparsegenerativeap- NVidia Professorship Award, and the ERC Starting Grant
proachtogeneratingasparseTSDFrepresentationofasur- Scan2CAD(804724).
Figure 7: Scan completion results on Matterport3D [3] data (2cm resolution), with input scans generated from a subset of
frames. Our self-supervision approach using lossmasking enables more complete sceneprediction than direct supervision
usingthetargetRGB-Dscan,particularlyinregionswhereocclusionscommonlyoccur.References scale scene completion and semantic segmentation for 3d
scans. In 2018 IEEE Conference on Computer Vision and
[1] Armen Avetisyan, Manuel Dahnert, Angela Dai, Manolis
PatternRecognition,CVPR2018,SaltLakeCity,UT,USA,
Savva, Angel X. Chang, and Matthias Nießner. Scan2cad:
June18-22,2018,pages4578–4587,2018. 2,3,5,6,7
LearningCADmodelalignmentinRGB-Dscans. InIEEE
[13] Benjamin Graham, Martin Engelcke, and Laurens van der
Conference on Computer Vision and Pattern Recognition,
Maaten. 3dsemanticsegmentationwithsubmanifoldsparse
CVPR2019,LongBeach,CA,USA,June16-20,2019,pages
convolutionalnetworks. In2018IEEEConferenceonCom-
2614–2623,2019. 1
puterVisionandPatternRecognition,CVPR2018,SaltLake
[2] ArmenAvetisyan,AngelaDai,andMatthiasNiessner. End-
City, UT, USA, June 18-22, 2018, pages 9224–9232, 2018.
to-endcadmodelretrievaland9dofalignmentin3dscans.
2,3,4
InTheIEEEInternationalConferenceonComputerVision
[14] Benjamin Graham and Laurens van der Maaten. Sub-
(ICCV),October2019. 1
manifold sparse convolutional networks. arXiv preprint
[3] AngelX.Chang,AngelaDai,ThomasA.Funkhouser,Ma-
arXiv:1706.01307,2017. 2,3,4
ciejHalber,MatthiasNießner,ManolisSavva,ShuranSong,
[15] Xiaoguang Han, Zhen Li, Haibin Huang, Evangelos
AndyZeng,andYindaZhang. Matterport3d:Learningfrom
Kalogerakis, and Yizhou Yu. High-resolution shape com-
RGB-Ddatainindoorenvironments. In2017International
pletionusingdeepneuralnetworksforglobalstructureand
Conferenceon3DVision,3DV2017,Qingdao,China,Octo-
localgeometryinference. InIEEEInternationalConference
ber10-12,2017,pages667–676,2017. 2,4,5,6,7,8,11,
onComputerVision,ICCV2017,Venice,Italy,October22-
12
29,2017,pages85–93,2017. 3
[4] Angel X Chang, Thomas Funkhouser, Leonidas Guibas,
[16] Shahram Izadi, David Kim, Otmar Hilliges, David
Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese,
Molyneaux,RichardA.Newcombe,PushmeetKohli,Jamie
Manolis Savva, Shuran Song, Hao Su, et al. Shapenet:
Shotton,SteveHodges,DustinFreeman,AndrewJ.Davison,
An information-rich 3d model repository. arXiv preprint
andAndrewW.Fitzgibbon. Kinectfusion: real-time3dre-
arXiv:1512.03012,2015. 2
constructionandinteractionusingamovingdepthcamera.In
[5] SungjoonChoi,Qian-YiZhou,andVladlenKoltun. Robust Proceedingsofthe24thAnnualACMSymposiumonUserIn-
reconstructionofindoorscenes. In2015IEEEConference terfaceSoftwareandTechnology,SantaBarbara,CA,USA,
onComputerVisionandPatternRecognition(CVPR),pages October16-19,2011,pages559–568,2011. 1,2
5556–5565.IEEE,2015. 1,2
[17] MichaelM.Kazhdan,MatthewBolitho,andHuguesHoppe.
[6] ChristopherB.Choy,JunYoungGwak,andSilvioSavarese. Poisson surface reconstruction. In Proceedings of the
4dspatio-temporalconvnets:Minkowskiconvolutionalneu- Fourth Eurographics Symposium on Geometry Processing,
ralnetworks. InIEEEConferenceonComputerVisionand Cagliari, Sardinia, Italy, June 26-28, 2006, pages 61–70,
Pattern Recognition, CVPR 2019, Long Beach, CA, USA, 2006. 1,2,5,7
June16-20,2019,pages3075–3084,2019. 2,3 [18] Michael M. Kazhdan and Hugues Hoppe. Screened pois-
[7] Brian Curless and Marc Levoy. A volumetric method for sonsurfacereconstruction.ACMTrans.Graph.,32(3):29:1–
buildingcomplexmodelsfromrangeimages.InProceedings 29:13,2013. 1,2,5,7
of the 23rd annual conference on Computer graphics and [19] WilliamE.LorensenandHarveyE.Cline. Marchingcubes:
interactivetechniques,pages303–312.ACM,1996. 2,3,4 A high resolution 3d surface construction algorithm. In
[8] Manuel Dahnert, Angela Dai, Leonidas J. Guibas, and Proceedings of the 14th Annual Conference on Computer
MatthiasNiessner. Jointembeddingof3dscanandcadob- Graphics and Interactive Techniques, SIGGRAPH 1987,
jects. In The IEEE International Conference on Computer Anaheim, California, USA, July 27-31, 1987, pages 163–
Vision(ICCV),October2019. 1 169,1987. 3
[9] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Hal- [20] Daniel Maturana and Sebastian Scherer. Voxnet: A 3d
ber, Thomas Funkhouser, and Matthias Niessner. Scannet: convolutional neural network for real-time object recogni-
Richly-annotated3dreconstructionsofindoorscenes.InThe tion. In2015IEEE/RSJInternationalConferenceonIntel-
IEEEConferenceonComputerVisionandPatternRecogni- ligentRobotsandSystems,IROS2015,Hamburg,Germany,
tion(CVPR),July2017. 2 September28-October2,2015,pages922–928,2015. 2
[10] AngelaDai,MatthiasNießner,MichaelZollho¨fer,Shahram [21] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Se-
Izadi, and Christian Theobalt. Bundlefusion: Real-time bastianNowozin,andAndreasGeiger.Occupancynetworks:
globallyconsistent3dreconstructionusingon-the-flysurface Learning 3d reconstruction in function space. In Proceed-
reintegration. ACMTrans.Graph.,36(3):24:1–24:18,2017. ingsIEEEConf.onComputerVisionandPatternRecogni-
1,2 tion(CVPR),2019. 3
[11] AngelaDai,CharlesRuizhongtaiQi,andMatthiasNießner. [22] Richard A. Newcombe, Shahram Izadi, Otmar Hilliges,
Shape completion using 3d-encoder-predictor cnns and David Molyneaux, David Kim, Andrew J. Davison, Push-
shapesynthesis. In2017IEEEConferenceonComputerVi- meet Kohli, Jamie Shotton, Steve Hodges, and Andrew W.
sion and Pattern Recognition, CVPR 2017, Honolulu, HI, Fitzgibbon. Kinectfusion: Real-time dense surface map-
USA,July21-26,2017,pages6545–6554,2017. 2,3,5,6,7 pingandtracking.In10thIEEEInternationalSymposiumon
[12] Angela Dai, Daniel Ritchie, Martin Bokeloh, Scott Reed, MixedandAugmentedReality,ISMAR2011,Basel,Switzer-
Ju¨rgenSturm,andMatthiasNießner. Scancomplete: Large- land,October26-29,2011,pages127–136,2011. 1,2[23] M. Nießner, M. Zollho¨fer, S. Izadi, and M. Stamminger. [34] Peng-ShuaiWang, Chun-YuSun, YangLiu, andXinTong.
Real-time 3d reconstruction at scale using voxel hashing. Adaptive O-CNN: a patch-based deep representation of 3d
ACMTransactionsonGraphics(TOG),2013. 1,2 shapes. ACMTrans.Graph.,37(6):217:1–217:11,2018. 2
[24] JeongJoonPark, PeterFlorence, JulianStraub, RichardA. [35] WeiyueWang, QianguiHuang, SuyaYou, ChaoYang, and
Newcombe,andStevenLovegrove. Deepsdf:Learningcon- UlrichNeumann. Shapeinpaintingusing3dgenerativead-
tinuoussigneddistancefunctionsforshaperepresentation.In versarialnetworkandrecurrentconvolutionalnetworks. In
IEEEConferenceonComputerVisionandPatternRecogni- IEEE International Conference on Computer Vision, ICCV
tion,CVPR2019,LongBeach,CA,USA,June16-20,2019, 2017,Venice,Italy,October22-29,2017,pages2317–2325,
pages165–174,2019. 3 2017. 3
[25] Deepak Pathak, Philipp Kra¨henbu¨hl, Jeff Donahue, Trevor [36] Thomas Whelan, Stefan Leutenegger, Renato F. Salas-
Darrell, and Alexei A. Efros. Context encoders: Feature Moreno, Ben Glocker, and Andrew J. Davison. Elasticfu-
learningbyinpainting. In2016IEEEConferenceonCom- sion:DenseSLAMwithoutAposegraph. InRobotics:Sci-
puterVisionandPatternRecognition,CVPR2016,LasVe- ence and Systems XI, Sapienza University of Rome, Rome,
gas, NV, USA, June 27-30, 2016, pages 2536–2544. IEEE Italy,July13-17,2015,2015. 1
ComputerSociety,2016. 7 [37] ZhirongWu, ShuranSong, AdityaKhosla, FisherYu, Lin-
[26] Charles Ruizhongtai Qi, Hao Su, Kaichun Mo, and guang Zhang, Xiaoou Tang, and Jianxiong Xiao. 3d
Leonidas J. Guibas. Pointnet: Deep learning on point sets shapenets: Adeeprepresentationforvolumetricshapes. In
for3dclassificationandsegmentation. In2017IEEECon- IEEEConferenceonComputerVisionandPatternRecogni-
ferenceonComputerVisionandPatternRecognition,CVPR tion,CVPR2015,Boston,MA,USA,June7-12,2015,pages
2017, Honolulu, HI, USA, July 21-26, 2017, pages 77–85, 1912–1920,2015. 3
2017. 2
[27] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J.
Guibas. Pointnet++: Deephierarchicalfeaturelearningon
pointsetsinametricspace. InAdvancesinNeuralInforma-
tionProcessingSystems30: AnnualConferenceonNeural
InformationProcessingSystems2017,4-9December2017,
LongBeach,CA,USA,pages5099–5108,2017. 2
[28] GernotRiegler,AliOsmanUlusoy,HorstBischof,andAn-
dreasGeiger.Octnetfusion:Learningdepthfusionfromdata.
In2017InternationalConferenceon3DVision,3DV2017,
Qingdao,China,October10-12,2017,pages57–66,2017.
2
[29] Gernot Riegler, Ali Osman Ulusoy, and Andreas Geiger.
Octnet:Learningdeep3drepresentationsathighresolutions.
In2017IEEEConferenceonComputerVisionandPattern
Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26,
2017,pages6620–6629,2017. 2
[30] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-
net: Convolutionalnetworksforbiomedicalimagesegmen-
tation. InInternationalConferenceonMedicalimagecom-
puting and computer-assisted intervention, pages 234–241.
Springer,2015. 5
[31] Shuran Song, Fisher Yu, Andy Zeng, Angel X. Chang,
ManolisSavva,andThomasA.Funkhouser.Semanticscene
completionfromasingledepthimage. In2017IEEECon-
ferenceonComputerVisionandPatternRecognition,CVPR
2017,Honolulu,HI,USA,July21-26,2017,pages190–198,
2017. 2,3,5,6,7
[32] MaximTatarchenko,AlexeyDosovitskiy,andThomasBrox.
Octree generating networks: Efficient convolutional archi-
tectures for high-resolution 3d outputs. In IEEE Interna-
tionalConferenceonComputerVision,ICCV2017,Venice,
Italy,October22-29,2017,pages2107–2115,2017. 2
[33] Peng-Shuai Wang, Yang Liu, Yu-Xiao Guo, Chun-Yu Sun,
and Xin Tong. O-CNN: octree-based convolutional neu-
ral networks for 3d shape analysis. ACM Trans. Graph.,
36(4):72:1–72:11,2017. 2A.SG-NNArchitectureDetails roomsceneswithvaryingdegreesofcompleteness;thatis,
we use ≈ 50%,60%, and 100% of the frames associated
Figure 8 details our Sparse Generative Neural Network
with each room scene to generate three different levels of
specificationforscancompletion. Convolutionparameters
completeness in the target scans, using ≈ 30%,40%, and
are given as (nf in, nf out, kernel size, stride, padding),
50%fortherespectiveinputscans. Weprovideaquantita-
with stride and padding default to 1 and 0 respectively.
tive evaluation in the main paper, and a qualitative evalua-
(cid:76)
Arrows indicate concatenation, and indicates addition.
tioninFigure9.Evenasthelevelofcompletenessinthetar-
Eachconvolution(exceptthelast)isfollowedbybatchnor-
getdatauseddecreases,ourapproachmaintainsrobustness
malizationandaReLU.
itscompletion,informedbythedeltasinincompletenessas
tothepatternsofgeneratingcompletegeometry.
B.VaryingTargetDataIncompleteness
Here, we aim to evaluate how well our self-supervision
approach performs as the completeness of the target data
seenduringtrainingdecreases. Aslongasthereisenough
varietyinthecompletionpatternsseenduringtraining,our
approach can learn to generate scene geometry with high
levels of completeness. To evaluate this, we generate sev-
eral versions of target scans from the Matterport3D [3]
Figure8: SG-NNarchitectureindetail. ThefinalTSDFvaluesarehighlightedinorange,andintermediateoutputsinyellow.
Convolutionparametersaregivenas(nf in,nf out,kernel size,stride,padding),withstrideandpaddingdefaultto1and0.
(cid:76)
Arrowsdenoteconcatenation,and denotesaddition.Figure9: Qualitativeevaluationofvaryingtargetdatacompletenessavailablefortraining. Wegeneratevariousincomplete
versions of the Matterport3D [3] scans using ≈ 30%,40%,50%,60%, and 100% of the frames associated with each room
scene, and evaluate on the 50% incomplete scans. Even as the level of completeness of the target data used during train-
ing decreases significantly, our self-supervised approach effectively learns the geometric completion process, maintaining
robustnessingeneratingcompletegeometry.